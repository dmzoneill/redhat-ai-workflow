# Skill: Review PR/MR
# Structured code review with local testing

name: review_pr
description: |
  Review a colleague's PR/MR.
  
  Resolves repository from issue_key, repo_name, or current directory.
  
  Accepts EITHER:
  - mr_id: GitLab MR number (e.g., 123)
  - issue_key: Jira issue key (e.g., AAP-61214) - will find the MR
  
  Checks MR description, commit format, pipelines, Jira context,
  runs local tests, and provides brief focused feedback.
  
  Automatically approves or posts feedback based on findings.
version: "1.2"

inputs:
  - name: mr_id
    type: integer
    required: false
    description: "GitLab MR ID (e.g., 1234)"
  
  - name: url
    type: string
    required: false
    description: "Full GitLab MR URL (e.g., https://gitlab.cee.redhat.com/org/repo/-/merge_requests/123)"
  
  - name: issue_key
    type: string
    required: false
    description: "Jira issue key (e.g., AAP-61214) - will search for associated MR"
  
  - name: repo_name
    type: string
    required: false
    description: "Repository name from config (e.g., 'automation-analytics-backend')"
  
  - name: run_tests
    type: boolean
    required: false
    default: false
    description: "Checkout branch and run local tests (default: false, static analysis only)"

# No hardcoded constants - resolved dynamically

steps:
  # ==================== RESOLVE REPOSITORY ====================
  
  - name: resolve_repo
    description: "Determine which repo, GitLab project, and settings to use"
    compute: |
      import json
      import os
      import re
      from pathlib import Path

      repo_path = None
      gitlab_project = None
      konflux_namespace = "aap-aa-tenant"
      fastapi_container = "automation-analytics-backend-api-fastapi-1"
      mr_id_from_url = None

      # Parse URL if provided
      if inputs.url:
          url_match = re.match(r'https?://[^/]+/(.+?)/-/merge_requests/(\d+)', inputs.url)
          if url_match:
              gitlab_project = url_match.group(1)
              mr_id_from_url = int(url_match.group(2))

      # Load config using shared loader
      from scripts.common.config_loader import load_config
      config = load_config()
      repos = config.get("repositories", {})

      # Repo name from config
      if inputs.repo_name and inputs.repo_name in repos:
          cfg = repos[inputs.repo_name]
          repo_path = cfg.get("path")
          gitlab_project = cfg.get("gitlab")
      # Resolve from issue key
      elif inputs.issue_key:
          project_prefix = inputs.issue_key.split("-")[0].upper()
          for name, cfg in repos.items():
              if cfg.get("jira_project") == project_prefix:
                  repo_path = cfg.get("path")
                  gitlab_project = cfg.get("gitlab")
                  break
      # Fall back to cwd
      else:
          cwd = os.getcwd()
          for name, cfg in repos.items():
              if cfg.get("path") == cwd:
                  repo_path = cfg.get("path")
                  gitlab_project = cfg.get("gitlab")
                  break

      if not gitlab_project:
          gitlab_project = "automation-analytics/automation-analytics-backend"
      if not repo_path:
          repo_path = os.getcwd()

      result = {
          "path": repo_path,
          "gitlab_project": gitlab_project,
          "konflux_namespace": konflux_namespace,
          "fastapi_container": fastapi_container,
          "mr_id_from_url": mr_id_from_url,
          "jira_url": config.get("jira", {}).get("url", "https://issues.redhat.com"),
      }
    output: resolved
  # ==================== RESOLVE MR ID ====================
  
  # Step 0a: Validate we have either mr_id, url, or issue_key
  - name: validate_input
    description: "Ensure we have mr_id, url, or issue_key"
    compute: |
      has_mr_id = inputs.get('mr_id') or resolved.get('mr_id_from_url')
      has_issue_key = inputs.get('issue_key')
      
      if not has_mr_id and not has_issue_key:
        raise ValueError("Must provide 'mr_id', 'url', or 'issue_key'")
      result = "OK"
    output: input_valid

  # Step 0b: Search for MR by Jira key if no mr_id provided
  - name: find_mr_by_jira
    description: "Find MR associated with Jira issue"
    condition: "not inputs.get('mr_id') and not resolved.get('mr_id_from_url') and inputs.get('issue_key')"
    tool: gitlab_mr_list
    args:
      project: "{{ resolved.gitlab_project }}"
      state: opened
      search: "{{ inputs.issue_key }}"
    output: mr_search_results
    on_error: continue

  # Step 0c: Extract MR ID from search results or URL
  - name: extract_mr_id
    description: "Extract MR ID from inputs, URL, or search results"
    compute: |
      import re
      
      resolved_mr_id = None
      
      # Priority 1: Explicit mr_id input
      if inputs.get('mr_id'):
        resolved_mr_id = inputs.mr_id
      # Priority 2: Extracted from URL
      elif resolved.get('mr_id_from_url'):
        resolved_mr_id = resolved.mr_id_from_url
      else:
        # Parse search results to find MR ID
        search_text = mr_search_results or ""
        
        # Look for MR ID patterns: !123, IID: 123, mr_id: 123
        matches = re.findall(r'!(\d+)|IID[:\s]+(\d+)|mr_id[:\s]+(\d+)', search_text, re.IGNORECASE)
        
        if matches:
          # Get first match
          for match in matches:
            for group in match:
              if group:
                resolved_mr_id = int(group)
                break
            if resolved_mr_id:
              break
        else:
          # Try to find any number that looks like an MR ID (3-5 digits)
          nums = re.findall(r'\b(\d{2,5})\b', search_text)
          if nums:
            resolved_mr_id = int(nums[0])
          else:
            raise ValueError(f"No MR found for issue {inputs.issue_key}. Please provide mr_id directly.")
      
      result = resolved_mr_id
    output: resolved_mr_id

  # Step 0d: Set mr_id for rest of workflow
  - name: set_mr_id
    description: "Set the MR ID for remaining steps"
    compute: |
      if inputs.get('mr_id'):
        result = inputs.mr_id
      else:
        result = resolved_mr_id
    output: mr_id

  # ==================== GATHER CONTEXT ====================

  # Step 1: Get MR details
  - name: get_mr
    description: "Fetch MR details from GitLab"
    tool: gitlab_mr_view
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ mr_id }}"
    output: mr_details

  # Step 2: Extract Jira issue key from title
  - name: extract_jira_key
    description: "Extract AAP-XXXXX from MR title"
    compute: |
      import re
      # MR title should be: AAP-12345 - type(scope): description
      match = re.match(r'^(AAP-\d+)', mr_details)
      if match:
        jira_key = match.group(1)
      else:
        # Try to find it anywhere in the title
        match = re.search(r'(AAP-\d+)', mr_details)
        jira_key = match.group(1) if match else None
      
      if not jira_key:
        jira_key = "NOT_FOUND"
      
      result = jira_key
    output: jira_key

  # Step 3: Get Jira issue details (if found)
  - name: get_jira_issue
    description: "Get Jira issue context"
    condition: "{{ jira_key != 'NOT_FOUND' }}"
    tool: jira_view_issue
    args:
      issue_key: "{{ jira_key }}"
    output: jira_details
    on_error: continue

  # Step 4: Get MR diff
  - name: get_diff
    description: "Get code changes"
    tool: gitlab_mr_diff
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ mr_id }}"
    output: mr_diff_raw

  # Step 4b: Check diff size and truncate if needed
  - name: check_diff_size
    description: "Validate diff size to avoid context overflow"
    compute: |
      diff_text = str(mr_diff_raw) if mr_diff_raw else ""
      lines = diff_text.split("\n")
      
      MAX_LINES = 3000
      MAX_CHARS = 100000
      
      is_large = len(lines) > MAX_LINES or len(diff_text) > MAX_CHARS
      
      if is_large:
          # Truncate and add warning
          if len(lines) > MAX_LINES:
              truncated = "\n".join(lines[:MAX_LINES])
              truncated += f"\n\n... [TRUNCATED: {len(lines) - MAX_LINES} more lines] ..."
          else:
              truncated = diff_text[:MAX_CHARS]
              truncated += f"\n\n... [TRUNCATED: {len(diff_text) - MAX_CHARS} more chars] ..."
          
          diff_info = {
              "text": truncated,
              "is_large": True,
              "total_lines": len(lines),
              "total_chars": len(diff_text),
              "warning": f"‚ö†Ô∏è Large diff ({len(lines)} lines, {len(diff_text)} chars) - truncated for analysis",
          }
      else:
          diff_info = {
              "text": diff_text,
              "is_large": False,
              "total_lines": len(lines),
              "total_chars": len(diff_text),
              "warning": None,
          }
      
      result = diff_info
    output: diff_size_check

  # Use truncated diff for analysis
  - name: set_mr_diff
    compute: |
      result = diff_size_check.get("text", "")
    output: mr_diff

  # Step 5: Check GitLab pipeline status
  - name: check_gitlab_pipeline
    description: "Check GitLab CI status"
    tool: gitlab_ci_status
    args:
      project: "{{ resolved.gitlab_project }}"
    output: gitlab_pipeline
    on_error: continue

  # Step 6: Check Konflux pipelines
  - name: check_konflux
    description: "Check Konflux integration tests"
    tool: konflux_list_pipelines
    args:
      namespace: "{{ resolved.konflux_namespace }}"
      limit: 5
    output: konflux_pipelines
    on_error: continue

  # ==================== VALIDATE FORMAT ====================

  # Step 7: Validate commit title format
  - name: validate_commit_format
    description: "Check commit title matches commitlint config from config.json"
    compute: |
      import re
      from scripts.common.config_loader import load_config
      
      issues = []
      
      # Load commit_format from config.json
      config = load_config()
      commit_cfg = config.get("commit_format", {})
      
      # Get valid types from config, with fallback
      valid_types = commit_cfg.get("types", ['feat', 'fix', 'docs', 'style', 'refactor', 'test', 'chore', 'perf', 'ci', 'build', 'revert'])
      
      # Extract title from MR details (first line usually)
      title_line = mr_details.split('\n')[0] if mr_details else ""
      
      # Check for Jira key at start
      if not re.match(r'^AAP-\d+', title_line):
        issues.append("Missing Jira key at start of title")
      
      # Check for type(scope): format
      type_match = re.search(r'- (\w+)(?:\([^)]+\))?\s*:', title_line)
      if type_match:
        commit_type = type_match.group(1).lower()
        if commit_type not in valid_types:
          issues.append(f"Invalid commit type '{commit_type}' - expected one of: {', '.join(valid_types)}")
      else:
        issues.append("Missing conventional commit format: type(scope): description")
      
      result = issues if issues else ["Format OK"]
    output: format_issues

  # Step 8: Check MR description
  - name: check_description
    description: "Verify MR has adequate description"
    compute: |
      issues = []
      
      # Look for empty or minimal descriptions
      desc_lines = [l for l in mr_details.split('\n') if l.strip() and not l.startswith('#')]
      
      if len(desc_lines) < 3:
        issues.append("MR description is minimal - consider adding context")
      
      # Check for Jira link
      jira_url = resolved.get("jira_url", "https://issues.redhat.com")
      jira_domain = jira_url.replace("https://", "").replace("http://", "").split("/")[0]
      if jira_domain not in mr_details and 'jira' not in mr_details.lower():
        if jira_key != 'NOT_FOUND':
          issues.append(f"Missing Jira link - add: {jira_url}/browse/{jira_key}")
      
      result = issues if issues else ["Description OK"]
    output: description_issues

  # ==================== LOCAL TESTING (optional) ====================

  # Step 9a: Extract branch name
  - name: extract_branch_name
    description: "Extract source branch from MR details"
    condition: "{{ inputs.run_tests }}"
    compute: |
      import re
      
      branch_match = re.search(r'Source branch:\s*(\S+)', mr_details)
      if not branch_match:
        branch_match = re.search(r'source_branch.*?[:\s]+(\S+)', mr_details, re.IGNORECASE)
      
      if branch_match:
        branch = branch_match.group(1)
      else:
        branch = f"mr-{mr_id}"
      
      result = branch
    output: source_branch

  # Step 9b: Fetch from origin
  - name: fetch_origin
    description: "Fetch latest from origin"
    condition: "{{ inputs.run_tests }}"
    tool: git_fetch
    args:
      repo: "{{ resolved.path }}"
    output: fetch_result
    on_error: continue

  # Step 9c: Checkout branch
  - name: checkout_branch
    description: "Checkout MR branch locally"
    condition: "{{ inputs.run_tests }}"
    tool: git_checkout
    args:
      repo: "{{ resolved.path }}"
      target: "{{ source_branch }}"
      force_create: true
      start_point: "origin/{{ source_branch }}"
    output: checkout_result
    on_error: continue

  # Step 9d: Fallback - try MR ref
  - name: fetch_mr_ref
    description: "Fallback: fetch MR directly"
    condition: "{{ inputs.run_tests and checkout_result and 'error' in str(checkout_result).lower() }}"
    tool: git_fetch
    args:
      repo: "{{ resolved.path }}"
      refspec: "merge-requests/{{ mr_id }}/head:{{ source_branch }}"
    output: mr_fetch_result
    on_error: continue

  # Step 9e: Checkout after MR fetch
  - name: checkout_mr_branch
    description: "Checkout after MR fetch"
    condition: "{{ inputs.run_tests and mr_fetch_result }}"
    tool: git_checkout
    args:
      repo: "{{ resolved.path }}"
      target: "{{ source_branch }}"
    output: checkout_result_final
    on_error: continue

  # Step 10: Check if docker-compose is running
  - name: check_docker
    description: "Verify docker-compose is running"
    condition: "{{ inputs.run_tests }}"
    tool: docker_compose_status
    args:
      repo: "{{ resolved.path }}"
      filter_name: "automation-analytics"
    output: docker_status_raw
    on_error: continue

  - name: start_docker_if_needed
    description: "Start docker-compose if containers not running"
    condition: "{{ inputs.run_tests and ('No containers' in str(docker_status_raw) or not docker_status_raw) }}"
    tool: docker_compose_up
    args:
      repo: "{{ resolved.path }}"
      detach: true
    output: docker_start_result
    on_error: continue

  - name: set_docker_status
    description: "Set docker status"
    condition: "{{ inputs.run_tests }}"
    compute: |
      if docker_start_result:
        result = str(docker_start_result)
      else:
        result = str(docker_status_raw) if docker_status_raw else "Docker status unknown"
    output: docker_status

  # Step 11: Run migrations
  - name: run_migrations
    description: "Run make migrations"
    condition: "{{ inputs.run_tests }}"
    tool: make_target
    args:
      repo: "{{ resolved.path }}"
      target: "migrations"
      timeout: 60
    output: migrations_result
    on_error: continue

  # Step 11b: Run data setup
  - name: run_data
    description: "Run make data"
    condition: "{{ inputs.run_tests }}"
    tool: make_target
    args:
      repo: "{{ resolved.path }}"
      target: "data"
      timeout: 60
    output: data_result
    on_error: continue

  - name: summarize_setup
    description: "Summarize setup results"
    condition: "{{ inputs.run_tests }}"
    compute: |
      mig_ok = "‚úÖ" in str(migrations_result) if migrations_result else False
      data_ok = "‚úÖ" in str(data_result) if data_result else False
      result = f"migrations: {'OK' if mig_ok else 'FAILED'}, data: {'OK' if data_ok else 'FAILED'}"
    output: setup_result

  # Step 12: Run pytest in container
  - name: run_tests
    description: "Run pytest in FastAPI container"
    condition: "{{ inputs.run_tests }}"
    compute: |
      import tempfile
      import os
      
      # Create test script
      test_script = (
          "#!/bin/bash\n"
          "set -e\n"
          "pipenv sync --dev\n"
          "export POSTGRESQL_USER=\"debug\"\n"
          "export POSTGRESQL_PASSWORD=\"debug\"\n"
          "export SECRET_KEY=\"1234\"\n"
          "export DB_SSLMODE=disable\n"
          "export DATABASE_PREFIX=\"postgresql://${POSTGRESQL_USER}:${POSTGRESQL_PASSWORD}@localhost\"\n"
          "pytest -vv --tb=short -q 2>&1 | tail -50\n"
      )
      
      # Write to temp file
      script_path = "/tmp/pr_review_test.sh"
      with open(script_path, 'w') as f:
        f.write(test_script)
      os.chmod(script_path, 0o755)
      
      # Store container name and script path for tool calls
      result = {
        "container": resolved["fastapi_container"],
        "script_path": script_path
      }
    output: test_prep

  # Copy script to container
  - name: copy_test_script
    description: "Copy test script to container"
    condition: "{{ test_prep }}"
    tool: docker_cp
    args:
      source: "{{ test_prep.script_path }}"
      destination: "{{ test_prep.container }}:/tmp/test.sh"
      to_container: true
    output: copy_result
    on_error: continue

  # Execute tests in container
  - name: execute_tests
    description: "Run tests in container"
    condition: "{{ test_prep and copy_result }}"
    tool: docker_exec
    args:
      container: "{{ test_prep.container }}"
      command: "bash /tmp/test.sh"
      timeout: 300
    output: test_exec_raw
    on_error: continue

  # Parse test results
  - name: parse_test_results
    description: "Parse test execution results"
    compute: |
      output = str(test_exec_raw) if 'test_exec_raw' in dir() and test_exec_raw else ""
      
      if len(output) > 2000:
        output = output[-2000:]
      
      if "passed" in output.lower() and "failed" not in output.lower():
        test_result = f"‚úÖ Tests passed\n{output}"
      elif "‚ùå" in output:
        test_result = f"‚ùå Tests failed\n{output}"
      else:
        test_result = f"‚ö†Ô∏è Test status unclear\n{output}"
      
      result = test_result
    output: test_result

  # ==================== CODE ANALYSIS ====================

  # Step 13: Analyze code for issues
  - name: analyze_code
    description: "Static analysis for security, memory, race conditions"
    compute: |
      import re
      
      diff_text = mr_diff if mr_diff else ""
      issues = []
      
      # Security checks
      security_patterns = [
        (r'eval\s*\(', "Potential code injection via eval()"),
        (r'exec\s*\(', "Potential code injection via exec()"),
        (r'password\s*=\s*["\'][^"\']+["\']', "Hardcoded password detected"),
        (r'secret\s*=\s*["\'][^"\']+["\']', "Hardcoded secret detected"),
        (r'\.format\s*\(.*user', "Potential SQL injection via string format"),
        (r'%s.*user|%s.*query', "Potential SQL injection via % formatting"),
        (r'shell\s*=\s*True', "Shell injection risk with shell=True"),
        (r'pickle\.loads?\s*\(', "Unsafe deserialization with pickle"),
      ]
      
      for pattern, msg in security_patterns:
        if re.search(pattern, diff_text, re.IGNORECASE):
          issues.append(f"üîê Security: {msg}")
      
      # Memory leak patterns
      memory_patterns = [
        (r'global\s+\w+\s*=\s*\[\]', "Global mutable default - potential memory leak"),
        (r'\.append\([^)]+\)\s*$(?!.*\.clear\(\))', "List append without clear - check for leak"),
        (r'cache\s*=\s*\{\}', "Unbounded cache dict - consider LRU"),
      ]
      
      for pattern, msg in memory_patterns:
        if re.search(pattern, diff_text):
          issues.append(f"üíæ Memory: {msg}")
      
      # Race condition patterns
      race_patterns = [
        (r'threading\.Thread\(', "Threading used - verify thread safety"),
        (r'asyncio\.create_task\((?!.*await)', "Task created without await - potential race"),
        (r'shared_state|global_state', "Shared state - verify synchronization"),
      ]
      
      for pattern, msg in race_patterns:
        if re.search(pattern, diff_text):
          issues.append(f"üèÉ Race: {msg}")
      
      # Check for test coverage
      new_files = re.findall(r'\+\+\+ b/(\S+\.py)', diff_text)
      new_files = [f for f in new_files if not f.startswith('tests/') and not f.endswith('test.py')]
      test_files = re.findall(r'\+\+\+ b/(tests/\S+\.py)', diff_text)
      
      if new_files and not test_files:
        issues.append(f"üìù Tests: New code in {len(new_files)} files but no test updates")
      
      result = issues if issues else []
    output: code_issues

  # Step 14: Prepare review summary
  - name: prepare_summary
    description: "Compile review findings"
    compute: |
      lines = []
      lines.append("## PR Review Summary")
      lines.append(f"**MR:** !{mr_id}")
      lines.append(f"**Jira:** {jira_key}")
      lines.append("")
      
      # Format validation
      if format_issues and format_issues != ["Format OK"]:
        lines.append("### ‚ùå Commit Format")
        for issue in format_issues[:3]:
          lines.append(f"- {issue}")
        lines.append("")
      
      # Description validation
      if description_issues and description_issues != ["Description OK"]:
        lines.append("### ‚ö†Ô∏è Description")
        for issue in description_issues[:2]:
          lines.append(f"- {issue}")
        lines.append("")
      
      # Pipeline status
      if 'Failed' in str(gitlab_pipeline) or 'failed' in str(konflux_pipelines):
        lines.append("### ‚ùå Pipelines")
        lines.append("- CI/CD pipelines have failures - check before approving")
        lines.append("")
      
      # Code issues
      if code_issues:
        lines.append("### üîç Code Review")
        for issue in code_issues[:5]:
          lines.append(f"- {issue}")
        lines.append("")
      
      # Test results
      if inputs.run_tests:
        lines.append("### üß™ Tests")
        if test_result:
          if '‚úÖ' in str(test_result):
            lines.append("- Tests passed")
          else:
            lines.append("- Tests failed - see details above")
        else:
          lines.append("- Could not run tests")
        lines.append("")
      
      # Determine overall status
      has_blockers = (
        (format_issues and format_issues != ["Format OK"]) or
        ('Failed' in str(gitlab_pipeline)) or
        (code_issues and any('Security' in str(i) for i in code_issues)) or
        (inputs.run_tests and test_result and '‚ùå' in str(test_result))
      )
      
      if has_blockers:
        lines.append("### üö´ Recommendation: Request Changes")
      else:
        lines.append("### ‚úÖ Recommendation: Approve")
      
      result = '\n'.join(lines)
    output: review_summary

# ==================== AUTO ACTIONS ====================

  # Step 15: Determine action based on review
  - name: determine_action
    description: "Decide whether to approve or request changes"
    compute: |
      # Check for blockers
      has_security_issues = code_issues and any('Security' in str(i) for i in code_issues)
      has_format_issues = format_issues and format_issues != ["Format OK"]
      has_test_failures = inputs.run_tests and test_result and '‚ùå' in str(test_result)
      has_pipeline_failures = 'Failed' in str(gitlab_pipeline) or 'failed' in str(konflux_pipelines).lower()
      
      # Determine action
      if has_security_issues:
        action = "request_changes"
        reason = "Security issues detected"
      elif has_test_failures:
        action = "request_changes"
        reason = "Tests are failing"
      elif has_pipeline_failures:
        action = "request_changes"
        reason = "CI/CD pipelines failing"
      elif has_format_issues:
        action = "request_changes"
        reason = "Commit format issues"
      elif code_issues and len(code_issues) > 3:
        action = "request_changes"
        reason = f"{len(code_issues)} code issues found"
      else:
        action = "approve"
        reason = "No blocking issues found"
      
      result = {"action": action, "reason": reason}
    output: review_action

  # Step 16: Build feedback message (for request_changes)
  - name: build_feedback
    description: "Build feedback message for GitLab"
    condition: "review_action.get('action') == 'request_changes'"
    compute: |
      lines = [f"## Code Review Feedback for {jira_key}"]
      lines.append("")
      
      if format_issues and format_issues != ["Format OK"]:
        lines.append("### Commit Format")
        for issue in format_issues[:3]:
          lines.append(f"- {issue}")
        lines.append("")
      
      if code_issues:
        lines.append("### Code Issues")
        for issue in code_issues[:5]:
          lines.append(f"- {issue}")
        lines.append("")
      
      if inputs.run_tests and test_result and '‚ùå' in str(test_result):
        lines.append("### Tests")
        lines.append("- Local tests are failing. Please investigate.")
        lines.append("")
      
      lines.append("---")
      lines.append("*Automated review by AI Workflow*")
      
      result = '\n'.join(lines)
    output: feedback_message

  # Step 17: Post feedback comment (if requesting changes)
  - name: post_feedback
    description: "Post review feedback to MR"
    condition: "review_action.get('action') == 'request_changes'"
    tool: gitlab_mr_comment
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ mr_id }}"
      comment: "{{ feedback_message }}"
    output: feedback_posted
    on_error: continue

  # Step 18: Approve MR (if no issues)
  - name: approve_mr
    description: "Approve the MR on GitLab"
    condition: "review_action.get('action') == 'approve'"
    tool: gitlab_mr_approve
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ mr_id }}"
    output: approval_result
    on_error: continue

  # Step 18b: Emit approval hook
  - name: emit_approval_hook
    description: "Notify author that MR was approved"
    condition: "review_action.get('action') == 'approve' and approval_result"
    compute: |
      import asyncio
      import sys
      from pathlib import Path
      sys.path.insert(0, str(Path.home() / "src/redhat-ai-workflow"))
      
      try:
          from scripts.skill_hooks import emit_event
          
          # Extract author from MR details
          import re
          author_match = re.search(r'Author[:\s]+@?(\w+)', mr_details or "", re.IGNORECASE)
          author = author_match.group(1) if author_match else ""
          
          asyncio.run(emit_event("review_approved", {
              "mr_id": str(mr_id),
              "author": author,
              "project": resolved.get("gitlab_project", ""),
          }))
          result = "hook sent"
      except Exception as e:
          result = f"hook skipped: {e}"
    output: approval_hook_result
    on_error: continue

  # Step 18c: Emit feedback hook
  - name: emit_feedback_hook
    description: "Notify author that feedback was posted"
    condition: "review_action.get('action') == 'request_changes' and feedback_posted"
    compute: |
      import asyncio
      import sys
      from pathlib import Path
      sys.path.insert(0, str(Path.home() / "src/redhat-ai-workflow"))
      
      try:
          from scripts.skill_hooks import emit_event
          
          # Extract author from MR details
          import re
          author_match = re.search(r'Author[:\s]+@?(\w+)', mr_details or "", re.IGNORECASE)
          author = author_match.group(1) if author_match else ""
          
          issue_count = len(code_issues) if code_issues else 0
          if format_issues and format_issues != ["Format OK"]:
              issue_count += len(format_issues)
          
          asyncio.run(emit_event("review_comment", {
              "mr_id": str(mr_id),
              "author": author,
              "project": resolved.get("gitlab_project", ""),
              "issue_count": str(issue_count),
          }))
          result = "hook sent"
      except Exception as e:
          result = f"hook skipped: {e}"
    output: feedback_hook_result
    on_error: continue

  # Step 19: Add Jira comment about review
  - name: jira_comment
    description: "Add review note to Jira issue"
    condition: "{{ jira_key != 'NOT_FOUND' }}"
    tool: jira_add_comment
    args:
      issue_key: "{{ jira_key }}"
      comment: |
        MR !{{ mr_id }} reviewed.
        Action: {{ review_action.get('action', 'unknown') }}
        Reason: {{ review_action.get('reason', '') }}
    output: jira_updated
    on_error: continue

# ==================== OUTPUT ====================

outputs:
  - name: summary
    value: |
      {{ review_summary }}
      
      ---
      
      ## Action Taken
      
      **Decision:** {{ review_action.get('action', 'unknown') | upper }}
      **Reason:** {{ review_action.get('reason', '') }}
      
      {% if review_action.get('action') == 'approve' %}
      ‚úÖ MR has been approved on GitLab.
      {% else %}
      üìù Feedback has been posted to the MR.
      {% endif %}
      
      {% if jira_key != 'NOT_FOUND' %}
      üìã Jira issue {{ jira_key }} updated with review note.
      {% endif %}
      
      {% if not inputs.run_tests %}
      ---
      
      ## üß™ Want to Run Local Tests?
      
      This review was **static analysis only**. To also checkout the branch and run tests:
      
      ```
      skill_run("review_pr", '{"mr_id": {{ mr_id }}, "run_tests": true}')
      ```
      
      Or just say: "Also run the tests for this MR"
      {% endif %}
  
  - name: context
    value:
      mr_id: "{{ mr_id }}"
      jira_key: "{{ jira_key }}"
      action: "{{ review_action.get('action') }}"
      has_issues: "{{ code_issues | length > 0 }}"
      format_ok: "{{ format_issues == ['Format OK'] }}"
      tests_ran: "{{ inputs.run_tests }}"
      tests_passed: "{{ '‚úÖ' in str(test_result) if test_result else 'not_run' }}"

