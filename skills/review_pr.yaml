# Skill: Review PR/MR
# Structured code review with local testing
# Enhanced with knowledge/memory/vector search integration

name: review_pr
description: |
  Review a colleague's PR/MR.

  Resolves repository from issue_key, repo_name, or current directory.

  Accepts EITHER:
  - mr_id: GitLab MR number (e.g., 123)
  - issue_key: Jira issue key (e.g., AAP-61214) - will find the MR

  Checks MR description, commit format, pipelines, Jira context,
  runs local tests, and provides brief focused feedback.

  **NEW Features:**
  - Semantic search for similar code patterns in codebase
  - Architecture validation against project knowledge
  - Pattern checking against learned error patterns
  - Author history and coaching based on past reviews

  Automatically approves or posts feedback based on findings.
version: "2.0"

inputs:
  - name: mr_id
    type: integer
    required: false
    description: "GitLab MR ID (e.g., 1234)"

  - name: url
    type: string
    required: false
    description: "Full GitLab MR URL (e.g., https://gitlab.cee.redhat.com/org/repo/-/merge_requests/123)"

  - name: issue_key
    type: string
    required: false
    description: "Jira issue key (e.g., AAP-61214) - will search for associated MR"

  - name: repo_name
    type: string
    required: false
    description: "Repository name from config (e.g., 'automation-analytics-backend')"

  - name: run_tests
    type: boolean
    required: false
    default: false
    description: "Checkout branch and run local tests (default: false, static analysis only)"

  - name: auto_merge
    type: boolean
    required: false
    default: false
    description: "Automatically merge if approved (default: false)"

  - name: slack_format
    type: boolean
    required: false
    default: false
    description: "Use Slack link format in summary"

# No hardcoded constants - resolved dynamically

steps:
  # ==================== RESOLVE REPOSITORY ====================

  - name: resolve_repo
    description: "Determine which repo, GitLab project, and settings to use"
    compute: |
      import os
      from scripts.common.parsers import extract_mr_id_from_url

      repo_path = None
      gitlab_project = None
      konflux_namespace = "aap-aa-tenant"
      fastapi_container = "automation-analytics-backend-api-fastapi-1"
      mr_id_from_url = None

      # Parse URL if provided using shared parser
      if inputs.get('url'):
          url_info = extract_mr_id_from_url(inputs.get('url'))
          if url_info:
              gitlab_project = url_info["project"]
              mr_id_from_url = url_info["mr_id"]

      # Load config using shared loader
      from scripts.common.config_loader import load_config
      config = load_config()
      repos = config.get("repositories", {})

      # Repo name from config
      if inputs.get('repo_name') and inputs.get('repo_name') in repos:
          cfg = repos[inputs.get('repo_name')]
          repo_path = cfg.get("path")
          gitlab_project = cfg.get("gitlab")
      # Resolve from issue key
      elif inputs.get('issue_key'):
          project_prefix = inputs.get('issue_key').split("-")[0].upper()
          for name, cfg in repos.items():
              if cfg.get("jira_project") == project_prefix:
                  repo_path = cfg.get("path")
                  gitlab_project = cfg.get("gitlab")
                  break
      # Fall back to cwd
      else:
          cwd = os.getcwd()
          for name, cfg in repos.items():
              if cfg.get("path") == cwd:
                  repo_path = cfg.get("path")
                  gitlab_project = cfg.get("gitlab")
                  break

      if not gitlab_project:
          gitlab_project = "automation-analytics/automation-analytics-backend"
      if not repo_path:
          repo_path = os.getcwd()

      result = {
          "path": repo_path,
          "gitlab_project": gitlab_project,
          "konflux_namespace": konflux_namespace,
          "fastapi_container": fastapi_container,
          "mr_id_from_url": mr_id_from_url,
          "jira_url": config.get("jira", {}).get("url", "https://issues.redhat.com"),
      }
    output: resolved
  # ==================== RESOLVE MR ID ====================

  # Step 0a: Validate we have either mr_id, url, or issue_key
  - name: validate_input
    description: "Ensure we have mr_id, url, or issue_key"
    compute: |
      has_mr_id = inputs.get('mr_id') or resolved.get('mr_id_from_url')
      has_issue_key = inputs.get('issue_key')

      if not has_mr_id and not has_issue_key:
        raise ValueError("Must provide 'mr_id', 'url', or 'issue_key'")
      result = "OK"
    output: input_valid

  # Step 0b: Search for MR by Jira key if no mr_id provided
  - name: find_mr_by_jira
    description: "Find MR associated with Jira issue"
    condition: "not inputs.get('mr_id') and not resolved.get('mr_id_from_url') and inputs.get('issue_key')"
    tool: gitlab_mr_list
    args:
      project: "{{ resolved.gitlab_project }}"
      state: opened
      search: "{{ inputs.get('issue_key', '') }}"
    output: mr_search_results
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Step 0c: Extract MR ID from search results or URL
  - name: extract_mr_id
    description: "Extract MR ID from inputs, URL, or search results using shared parser"
    compute: |
      from scripts.common.parsers import extract_mr_id_from_text

      resolved_mr_id = None

      # Priority 1: Explicit mr_id input
      if inputs.get('mr_id'):
        resolved_mr_id = inputs.get('mr_id')
      # Priority 2: Extracted from URL
      elif resolved.get('mr_id_from_url'):
        resolved_mr_id = resolved.get('mr_id_from_url')
      else:
        # Parse search results to find MR ID using shared parser
        resolved_mr_id = extract_mr_id_from_text(mr_search_results or "")

        if not resolved_mr_id:
          raise ValueError(f"No MR found for issue {inputs.get('issue_key')}. Please provide mr_id directly.")

      result = resolved_mr_id
    output: resolved_mr_id

  # Step 0d: Set mr_id for rest of workflow
  - name: set_mr_id
    description: "Set the MR ID for remaining steps"
    compute: |
      if inputs.get('mr_id'):
        result = inputs.get('mr_id')
      else:
        result = resolved_mr_id
    output: mr_id

  # ==================== PRE-EMPTIVE MEMORY CHECKS ====================

  # Check for known GitLab API issues before fetching MR
  - name: check_gitlab_known_issues
    description: "Check memory for known GitLab API issues before fetching MR"
    tool: check_known_issues
    args:
      tool_name: "gitlab_mr_view"
      error_text: ""
    output: gitlab_known_issues
    on_error: continue

  - name: preempt_gitlab_errors
    description: "Apply pre-emptive fixes based on known patterns"
    compute: |
      known = str(gitlab_known_issues) if 'gitlab_known_issues' in dir() and gitlab_known_issues else ""

      preemptive_warnings = []
      if "401" in known or "unauthorized" in known.lower():
          preemptive_warnings.append("GitLab token may need refresh")
      if "timeout" in known.lower():
          preemptive_warnings.append("GitLab API may be slow")
      if "rate limit" in known.lower():
          preemptive_warnings.append("GitLab rate limiting detected")

      result = {
          "has_warnings": len(preemptive_warnings) > 0,
          "warnings": preemptive_warnings,
      }
    output: gitlab_preemption
    on_error: continue

  # Check if we've reviewed this MR before (load from cache)
  - name: check_previous_review
    description: "Load previous review if this is a re-review"
    compute: |
      mr_str = str(mr_id) if mr_id else ""

      data = memory.read_memory("learned/review_cache") if memory else {}
      recent = data.get("recent_approvals", []) if isinstance(data.get("recent_approvals"), list) else []

      previous = next((r for r in recent if str(r.get("mr_id", "")) == mr_str), None)

      if previous:
          result = {
              "is_rereview": True,
              "previous_approval": previous.get("approved_at"),
              "previous_author": previous.get("author"),
              "previous_files_changed": previous.get("files_changed", 0),
              "hint": "This MR was previously reviewed - checking for new commits",
          }
      else:
          result = {"is_rereview": False}
    output: previous_review
    on_error: continue

  # ==================== GATHER CONTEXT ====================

  # Step 1: Get MR details
  - name: get_mr
    description: "Fetch MR details from GitLab"
    tool: gitlab_mr_view
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ mr_id }}"
    output: mr_details
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Step 1b: Get commit history for the MR
  - name: get_mr_commits
    description: "Get commit history for the MR"
    tool: gitlab_commit_list
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ mr_id }}"
      limit: 20
    output: mr_commits_raw
    on_error: continue

  - name: parse_commits
    description: "Parse commit history"
    compute: |
      commits_text = str(mr_commits_raw) if 'mr_commits_raw' in dir() and mr_commits_raw else ""

      commits = []
      for line in commits_text.split("\n"):
          if line.strip():
              commits.append(line.strip()[:100])

      # Check commit message quality
      import re
      good_format = 0
      for commit in commits[:10]:
          if re.search(r'AAP-\d+', commit, re.I):
              good_format += 1

      result = {
          "commits": commits[:10],
          "count": len(commits),
          "good_format_ratio": good_format / max(len(commits[:10]), 1),
          "all_have_jira": good_format == len(commits[:10]) and len(commits) > 0,
      }
    output: commit_history
    on_error: continue

  # Step 1c: Get the actual diff for the MR
  - name: get_mr_diff
    description: "Get the code diff for the MR"
    tool: git_diff
    args:
      repo: "{{ resolved.path }}"
      ref1: "{{ resolved.default_branch }}"
      ref2: "HEAD"
      stat: true
    output: mr_diff_raw
    on_error: continue

  - name: parse_diff
    description: "Parse diff to identify changed files"
    compute: |
      diff_text = str(mr_diff_raw) if 'mr_diff_raw' in dir() and mr_diff_raw else ""

      files_changed = []
      additions = 0
      deletions = 0

      for line in diff_text.split("\n"):
          if "|" in line:
              parts = line.split("|")
              if parts:
                  filename = parts[0].strip()
                  if filename and not filename.startswith("---"):
                      files_changed.append(filename)
          # Count +/- lines
          if line.startswith("+") and not line.startswith("+++"):
              additions += 1
          elif line.startswith("-") and not line.startswith("---"):
              deletions += 1

      result = {
          "files_changed": files_changed[:15],
          "file_count": len(files_changed),
          "additions": additions,
          "deletions": deletions,
          "diff_preview": diff_text[:600] if diff_text else "",
      }
    output: diff_analysis
    on_error: continue

  # Step 1d: Get blame for key changed files
  - name: blame_key_file
    description: "Get blame info for main changed file"
    condition: "diff_analysis and diff_analysis.file_count > 0"
    tool: git_blame
    args:
      repo: "{{ resolved.path }}"
      file: "{{ diff_analysis.files_changed[0] }}"
    output: blame_raw
    on_error: continue

  - name: parse_blame_info
    description: "Extract authorship info from blame"
    compute: |
      blame_text = str(blame_raw) if 'blame_raw' in dir() and blame_raw else ""

      import re
      authors = {}
      for line in blame_text.split("\n"):
          author_match = re.search(r'\(([^)]+)\s+\d{4}-\d{2}-\d{2}', line)
          if author_match:
              author = author_match.group(1).strip()
              authors[author] = authors.get(author, 0) + 1

      # Sort by contribution
      sorted_authors = sorted(authors.items(), key=lambda x: x[1], reverse=True)

      result = {
          "authors": [a[0] for a in sorted_authors[:5]],
          "main_author": sorted_authors[0][0] if sorted_authors else None,
          "author_count": len(authors),
      }
    output: blame_analysis
    on_error: continue

  # Step 2: Extract Jira issue key from title
  - name: extract_jira_key
    description: "Extract AAP-XXXXX from MR title"
    compute: |
      # parsers is available from skill engine safe_globals
      # Use shared parser
      jira_key = parsers.extract_jira_key(mr_details) if parsers else "NOT_FOUND"
      result = jira_key or "NOT_FOUND"
    output: jira_key

  # ==================== KNOWLEDGE & VECTOR SEARCH ====================

  # Search for similar code patterns in the codebase
  - name: search_similar_patterns
    description: "Find similar code patterns to validate against conventions"
    condition: "diff_analysis and diff_analysis.file_count > 0"
    tool: code_search
    args:
      query: "{{ diff_analysis.files_changed[0] | default('') }} implementation pattern"
      project: "{{ resolved.gitlab_project | replace('automation-analytics/', '') }}"
      limit: 5
    output: similar_code_raw
    on_error: continue

  - name: parse_similar_code
    description: "Parse similar code search results"
    compute: |
      similar = []
      if 'similar_code_raw' in dir() and similar_code_raw:
          raw_text = str(similar_code_raw)
          for line in raw_text.split('\n'):
              if line.strip() and ':' in line:
                  similar.append(line.strip()[:100])

      result = {
          "found": len(similar) > 0,
          "results": similar[:5],
          "count": len(similar),
      }
    output: similar_code
    on_error: continue

  # Load architecture knowledge for validation
  - name: load_architecture_for_review
    description: "Load architecture knowledge to validate changes"
    tool: knowledge_query
    args:
      project: "{{ resolved.gitlab_project | replace('automation-analytics/', '') }}"
      section: "architecture.key_modules"
    output: architecture_modules_raw
    on_error: continue

  - name: parse_architecture_modules
    description: "Parse architecture modules for validation"
    compute: |
      modules = []
      if 'architecture_modules_raw' in dir() and architecture_modules_raw:
          raw_text = str(architecture_modules_raw)
          import re
          # Extract module paths
          for match in re.finditer(r'path[:\s]+([^\n,]+)', raw_text, re.I):
              modules.append(match.group(1).strip())

      result = {
          "found": len(modules) > 0,
          "modules": modules[:10],
      }
    output: architecture_modules
    on_error: continue

  # Check if changed files align with architecture
  - name: validate_architecture_alignment
    description: "Check if changes align with project architecture"
    compute: |
      alignment_issues = []

      if 'diff_analysis' in dir() and diff_analysis and 'architecture_modules' in dir() and architecture_modules:
          changed_files = diff_analysis.get("files_changed", [])
          known_modules = architecture_modules.get("modules", [])

          # Check if changes are in known module areas
          for f in changed_files[:10]:
              in_known_module = False
              for mod in known_modules:
                  if mod in f or f.startswith(mod.rstrip('/')):
                      in_known_module = True
                      break

              # Flag files outside known modules (potential architecture violation)
              if not in_known_module and not any(skip in f for skip in ['test', 'conftest', '__init__', 'migrations']):
                  if f.endswith('.py') and '/' in f:
                      alignment_issues.append(f"New code in `{f}` - verify it belongs in this location")

      result = {
          "has_issues": len(alignment_issues) > 0,
          "issues": alignment_issues[:3],
      }
    output: architecture_alignment
    on_error: continue

  # Load coding patterns from knowledge
  - name: load_coding_patterns
    description: "Load coding patterns for the project"
    tool: knowledge_query
    args:
      project: "{{ resolved.gitlab_project | replace('automation-analytics/', '') }}"
      section: "patterns.coding"
    output: coding_patterns_raw
    on_error: continue

  - name: parse_coding_patterns
    description: "Parse coding patterns"
    compute: |
      patterns = []
      if 'coding_patterns_raw' in dir() and coding_patterns_raw:
          raw_text = str(coding_patterns_raw)
          import re
          for match in re.finditer(r'-\s*(.+?)(?=\n-|\n\n|$)', raw_text, re.DOTALL):
              pattern = match.group(1).strip()[:100]
              if pattern and len(pattern) > 10:
                  patterns.append(pattern)

      result = {
          "found": len(patterns) > 0,
          "patterns": patterns[:5],
      }
    output: coding_patterns
    on_error: continue

  # Check for known problematic patterns in the diff
  - name: check_known_problematic_patterns
    description: "Check diff against known error patterns"
    tool: check_known_issues
    args:
      tool_name: ""
      error_text: "{{ mr_diff[:500] if mr_diff else '' }}"
    output: problematic_patterns_raw
    on_error: continue

  - name: parse_problematic_patterns
    description: "Parse problematic pattern matches"
    compute: |
      issues = []
      if 'problematic_patterns_raw' in dir() and problematic_patterns_raw:
          raw_text = str(problematic_patterns_raw)
          if "No known issues" not in raw_text and len(raw_text) > 20:
              import re
              for match in re.finditer(r'(?:pattern|fix)[:\s]+([^\n]+)', raw_text, re.I):
                  issues.append(match.group(1).strip()[:80])

      result = {
          "found": len(issues) > 0,
          "issues": issues[:3],
      }
    output: problematic_patterns
    on_error: continue

  # Step 3: Get Jira issue details (if found)
  - name: get_jira_issue
    description: "Get Jira issue context"
    condition: "{{ jira_key != 'NOT_FOUND' }}"
    tool: jira_view_issue
    args:
      issue_key: "{{ jira_key }}"
    output: jira_details
    on_error: auto_heal  # Jira API - may need auth refresh

  # Step 4: Get MR diff
  - name: get_diff
    description: "Get code changes"
    tool: gitlab_mr_diff
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ mr_id }}"
    output: mr_diff_raw

  # Step 4b: Check diff size and truncate if needed
  - name: check_diff_size
    description: "Validate diff size to avoid context overflow"
    compute: |
      diff_text = str(mr_diff_raw) if mr_diff_raw else ""
      lines = diff_text.split("\n")

      MAX_LINES = 3000
      MAX_CHARS = 100000

      is_large = len(lines) > MAX_LINES or len(diff_text) > MAX_CHARS

      if is_large:
          # Truncate and add warning
          if len(lines) > MAX_LINES:
              truncated = "\n".join(lines[:MAX_LINES])
              truncated += f"\n\n... [TRUNCATED: {len(lines) - MAX_LINES} more lines] ..."
          else:
              truncated = diff_text[:MAX_CHARS]
              truncated += f"\n\n... [TRUNCATED: {len(diff_text) - MAX_CHARS} more chars] ..."

          diff_info = {
              "text": truncated,
              "is_large": True,
              "total_lines": len(lines),
              "total_chars": len(diff_text),
              "warning": f"‚ö†Ô∏è Large diff ({len(lines)} lines, {len(diff_text)} chars) - truncated for analysis",
          }
      else:
          diff_info = {
              "text": diff_text,
              "is_large": False,
              "total_lines": len(lines),
              "total_chars": len(diff_text),
              "warning": None,
          }

      result = diff_info
    output: diff_size_check

  # Use truncated diff for analysis
  - name: set_mr_diff
    compute: |
      result = diff_size_check.get("text", "")
    output: mr_diff

  # Step 5: Check GitLab pipeline status
  - name: check_gitlab_pipeline
    description: "Check GitLab CI status"
    tool: gitlab_ci_status
    args:
      project: "{{ resolved.gitlab_project }}"
    output: gitlab_pipeline
    on_error: auto_heal  # GitLab API - may need auth refresh

  - name: get_pipeline_trace
    description: "Get pipeline trace if failed"
    condition: "gitlab_pipeline and 'failed' in str(gitlab_pipeline).lower()"
    tool: gitlab_ci_trace
    args:
      project: "{{ resolved.gitlab_project }}"
    output: pipeline_trace_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Learn from pipeline failure patterns for faster diagnosis
  - name: learn_pipeline_failure
    description: "Save pipeline failure patterns for faster diagnosis"
    condition: "pipeline_trace_raw and 'error' in str(pipeline_trace_raw).lower()"
    compute: |
      from datetime import datetime
      import re

      trace = str(pipeline_trace_raw) if pipeline_trace_raw else ""

      # Extract error signature
      error_lines = [l.strip() for l in trace.split("\n") if "error" in l.lower() or "failed" in l.lower()]
      error_signature = error_lines[0][:100] if error_lines else "unknown"

      # Check if this pattern exists
      data = memory.read_memory("learned/patterns") if memory else {}
      pipeline_patterns = data.get("pipeline_patterns", []) if isinstance(data.get("pipeline_patterns"), list) else []

      # Simple deduplication by first 50 chars
      signature_short = error_signature[:50]
      existing = next((p for p in pipeline_patterns if signature_short in str(p.get("pattern", ""))[:50]), None)

      if not existing and signature_short != "unknown":
          pipeline_patterns.append({
              "pattern": signature_short,
              "meaning": "Pipeline failure - needs investigation",
              "fix": "Check CI logs for details",
              "first_seen": datetime.now().isoformat(),
              "mr_ids": [str(mr_id)],
              "commands": ["gitlab_ci_trace"],
          })
          data["pipeline_patterns"] = pipeline_patterns[-30:]
          if memory:
              memory.write_memory("learned/patterns", data)
          result = {"learned": True, "pattern": signature_short}
      elif existing:
          # Update existing pattern with new MR
          mr_list = existing.get("mr_ids", [])
          if str(mr_id) not in mr_list:
              mr_list.append(str(mr_id))
              existing["mr_ids"] = mr_list[-10:]
              existing["last_seen"] = datetime.now().isoformat()
              if memory:
                  memory.write_memory("learned/patterns", data)
          result = {"learned": False, "pattern": signature_short, "recurrence": len(mr_list)}
      else:
          result = {"learned": False, "pattern": "none"}
    output: pipeline_learning_result
    on_error: continue

  - name: get_approvers
    description: "Get MR approvers list"
    tool: gitlab_mr_approvers
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ mr_id }}"
    output: approvers_raw
    on_error: continue

  - name: parse_approvers
    description: "Parse approvers list"
    compute: |
      approvers_text = str(approvers_raw) if 'approvers_raw' in dir() and approvers_raw else ""

      approvers = []
      for line in approvers_text.split("\n"):
        line = line.strip()
        if line and "@" in line:
          approvers.append(line[:40])

      result = {
        "approvers": approvers[:5],
        "count": len(approvers),
      }
    output: approvers_info
    on_error: continue

  # Step 6: Check Konflux pipelines
  - name: check_konflux
    description: "Check Konflux integration tests"
    tool: konflux_list_pipelines
    args:
      namespace: "{{ resolved.konflux_namespace }}"
      limit: 5
    output: konflux_pipelines
    on_error: continue

  # ==================== VALIDATE FORMAT ====================

  # Step 7: Validate commit title format using config.json pattern
  - name: validate_commit_format
    description: "Check commit title matches commit_format pattern from config.json"
    compute: |
      from scripts.common.config_loader import get_commit_format, validate_commit_message

      # Extract title from MR details (first line usually)
      title_line = mr_details.split('\n')[0] if mr_details else ""

      # Use the centralized validation function from config_loader
      is_valid, issues = validate_commit_message(title_line)

      if is_valid:
        result = ["Format OK"]
      else:
        # Add hint about the expected format
        commit_cfg = get_commit_format()
        result = issues + [f"Pattern: {commit_cfg['pattern']}"]
    output: format_issues

  # Learn from new format issues for future detection
  - name: learn_format_pattern
    description: "Save new format issues to patterns for future detection"
    condition: "format_issues and format_issues != ['Format OK'] and len(format_issues) > 1"
    compute: |
      from datetime import datetime

      data = memory.read_memory("learned/patterns") if memory else {}
      commit_patterns = data.get("commit_format_patterns", []) if isinstance(data.get("commit_format_patterns"), list) else []

      # Extract the first specific issue (not the generic pattern line)
      new_issue = format_issues[0] if format_issues else ""

      # Check if already known
      known = any(p.get("issue") == new_issue for p in commit_patterns)

      if not known and new_issue and not new_issue.startswith("Pattern:"):
          commit_patterns.append({
              "issue": new_issue,
              "mr_id": str(mr_id),
              "first_seen": datetime.now().isoformat(),
              "occurrences": 1,
          })
          data["commit_format_patterns"] = commit_patterns[-50:]
          if memory:
              memory.write_memory("learned/patterns", data)
          result = f"Learned new format pattern: {new_issue[:50]}"
      else:
          # Increment occurrence count for known pattern
          for p in commit_patterns:
              if p.get("issue") == new_issue:
                  p["occurrences"] = p.get("occurrences", 0) + 1
                  p["last_seen"] = datetime.now().isoformat()
                  if memory:
                      memory.write_memory("learned/patterns", data)
                  break
          result = "Pattern already known"
    output: format_learning_result
    on_error: continue

  # Step 8: Check MR description
  - name: check_description
    description: "Verify MR has adequate description"
    compute: |
      issues = []

      # Look for empty or minimal descriptions
      desc_lines = [l for l in mr_details.split('\n') if l.strip() and not l.startswith('#')]

      if len(desc_lines) < 3:
        issues.append("MR description is minimal - consider adding context")

      # Check for Jira link
      jira_url = resolved.get("jira_url", "https://issues.redhat.com")
      jira_domain = jira_url.replace("https://", "").replace("http://", "").split("/")[0]
      if jira_domain not in mr_details and 'jira' not in mr_details.lower():
        if jira_key != 'NOT_FOUND':
          issues.append(f"Missing Jira link - add: {jira_url}/browse/{jira_key}")

      result = issues if issues else ["Description OK"]
    output: description_issues

  # ==================== LOCAL TESTING (optional) ====================

  # Step 9a: Extract branch name
  - name: extract_branch_name
    description: "Extract source branch from MR details using shared parser"
    condition: "{{ inputs.get('run_tests', False) }}"
    compute: |
      from scripts.common.parsers import extract_branch_from_mr

      branch = extract_branch_from_mr(mr_details)
      if not branch:
        branch = f"mr-{mr_id}"

      result = branch
    output: source_branch

  # Step 9b: Fetch from origin
  - name: fetch_origin
    description: "Fetch latest from origin"
    condition: "{{ inputs.get('run_tests', False) }}"
    tool: git_fetch
    args:
      repo: "{{ resolved.path }}"
    output: fetch_result
    on_error: auto_heal  # Git remote - may need auth/network

  # Step 9c: Checkout branch
  - name: checkout_branch
    description: "Checkout MR branch locally"
    condition: "{{ inputs.get('run_tests', False) }}"
    tool: git_checkout
    args:
      repo: "{{ resolved.path }}"
      target: "{{ source_branch }}"
      force_create: true
      start_point: "origin/{{ source_branch }}"
    output: checkout_result
    on_error: continue

  # Step 9d: Fallback - try MR ref
  - name: fetch_mr_ref
    description: "Fallback: fetch MR directly"
    condition: "{{ inputs.get('run_tests', False) and checkout_result and 'error' in str(checkout_result).lower() }}"
    tool: git_fetch
    args:
      repo: "{{ resolved.path }}"
      refspec: "merge-requests/{{ mr_id }}/head:{{ source_branch }}"
    output: mr_fetch_result
    on_error: auto_heal  # Git remote - may need auth/network

  # Step 9e: Checkout after MR fetch
  - name: checkout_mr_branch
    description: "Checkout after MR fetch"
    condition: "{{ inputs.get('run_tests', False) and mr_fetch_result }}"
    tool: git_checkout
    args:
      repo: "{{ resolved.path }}"
      target: "{{ source_branch }}"
    output: checkout_result_final
    on_error: continue

  # Step 10: Check if docker-compose is running
  - name: check_docker
    description: "Verify docker-compose is running"
    condition: "{{ inputs.get('run_tests', False) }}"
    tool: docker_compose_status
    args:
      repo: "{{ resolved.path }}"
      filter_name: "automation-analytics"
    output: docker_status_raw
    on_error: continue

  - name: start_docker_if_needed
    description: "Start docker-compose if containers not running"
    condition: "{{ inputs.get('run_tests', False) and ('No containers' in str(docker_status_raw) or not docker_status_raw) }}"
    tool: docker_compose_up
    args:
      repo: "{{ resolved.path }}"
      detach: true
    output: docker_start_result
    on_error: continue

  - name: set_docker_status
    description: "Set docker status"
    condition: "{{ inputs.get('run_tests', False) }}"
    compute: |
      if docker_start_result:
        result = str(docker_start_result)
      else:
        result = str(docker_status_raw) if docker_status_raw else "Docker status unknown"
    output: docker_status

  # Step 11: Run migrations
  - name: run_migrations
    description: "Run make migrations"
    condition: "{{ inputs.get('run_tests', False) }}"
    tool: make_target
    args:
      repo: "{{ resolved.path }}"
      target: "migrations"
      timeout: 60
    output: migrations_result
    on_error: continue

  # Step 11b: Run data setup
  - name: run_data
    description: "Run make data"
    condition: "{{ inputs.get('run_tests', False) }}"
    tool: make_target
    args:
      repo: "{{ resolved.path }}"
      target: "data"
      timeout: 60
    output: data_result
    on_error: continue

  - name: summarize_setup
    description: "Summarize setup results"
    condition: "{{ inputs.get('run_tests', False) }}"
    compute: |
      mig_ok = "‚úÖ" in str(migrations_result) if migrations_result else False
      data_ok = "‚úÖ" in str(data_result) if data_result else False
      result = f"migrations: {'OK' if mig_ok else 'FAILED'}, data: {'OK' if data_ok else 'FAILED'}"
    output: setup_result

  # Step 12: Run pytest in container
  - name: run_tests
    description: "Run pytest in FastAPI container"
    condition: "{{ inputs.get('run_tests', False) }}"
    compute: |
      import tempfile
      import os

      # Create test script
      test_script = (
          "#!/bin/bash\n"
          "set -e\n"
          "pipenv sync --dev\n"
          "export POSTGRESQL_USER=\"debug\"\n"
          "export POSTGRESQL_PASSWORD=\"debug\"\n"
          "export SECRET_KEY=\"1234\"\n"
          "export DB_SSLMODE=disable\n"
          "export DATABASE_PREFIX=\"postgresql://${POSTGRESQL_USER}:${POSTGRESQL_PASSWORD}@localhost\"\n"
          "pytest -vv --tb=short -q 2>&1 | tail -50\n"
      )

      # Write to temp file
      script_path = "/tmp/pr_review_test.sh"
      with open(script_path, 'w') as f:
        f.write(test_script)
      os.chmod(script_path, 0o755)

      # Store container name and script path for tool calls
      result = {
        "container": resolved["fastapi_container"],
        "script_path": script_path
      }
    output: test_prep

  # Copy script to container
  - name: copy_test_script
    description: "Copy test script to container"
    condition: "{{ test_prep }}"
    tool: docker_cp
    args:
      source: "{{ test_prep.script_path }}"
      destination: "{{ test_prep.container }}:/tmp/test.sh"
      to_container: true
    output: copy_result
    on_error: continue

  # Execute tests in container
  - name: execute_tests
    description: "Run tests in container"
    condition: "{{ test_prep and copy_result }}"
    tool: docker_exec
    args:
      container: "{{ test_prep.container }}"
      command: "bash /tmp/test.sh"
      timeout: 300
    output: test_exec_raw
    on_error: continue

  # Parse test results
  - name: parse_test_results
    description: "Parse test execution results"
    compute: |
      output = str(test_exec_raw) if 'test_exec_raw' in dir() and test_exec_raw else ""

      if len(output) > 2000:
        output = output[-2000:]

      if "passed" in output.lower() and "failed" not in output.lower():
        test_result = f"‚úÖ Tests passed\n{output}"
      elif "‚ùå" in output:
        test_result = f"‚ùå Tests failed\n{output}"
      else:
        test_result = f"‚ö†Ô∏è Test status unclear\n{output}"

      result = test_result
    output: test_result

  # ==================== CODE ANALYSIS ====================

  # Step 13: Analyze code for issues
  - name: analyze_code
    description: "Static analysis for security, memory, race conditions"
    compute: |
      import re

      diff_text = mr_diff if mr_diff else ""
      issues = []

      # Security checks
      security_patterns = [
        (r'eval\s*\(', "Potential code injection via eval()"),
        (r'exec\s*\(', "Potential code injection via exec()"),
        (r'password\s*=\s*["\'][^"\']+["\']', "Hardcoded password detected"),
        (r'secret\s*=\s*["\'][^"\']+["\']', "Hardcoded secret detected"),
        (r'\.format\s*\(.*user', "Potential SQL injection via string format"),
        (r'%s.*user|%s.*query', "Potential SQL injection via % formatting"),
        (r'shell\s*=\s*True', "Shell injection risk with shell=True"),
        (r'pickle\.loads?\s*\(', "Unsafe deserialization with pickle"),
      ]

      for pattern, msg in security_patterns:
        if re.search(pattern, diff_text, re.IGNORECASE):
          issues.append(f"üîê Security: {msg}")

      # Memory leak patterns
      memory_patterns = [
        (r'global\s+\w+\s*=\s*\[\]', "Global mutable default - potential memory leak"),
        (r'\.append\([^)]+\)\s*$(?!.*\.clear\(\))', "List append without clear - check for leak"),
        (r'cache\s*=\s*\{\}', "Unbounded cache dict - consider LRU"),
      ]

      for pattern, msg in memory_patterns:
        if re.search(pattern, diff_text):
          issues.append(f"üíæ Memory: {msg}")

      # Race condition patterns
      race_patterns = [
        (r'threading\.Thread\(', "Threading used - verify thread safety"),
        (r'asyncio\.create_task\((?!.*await)', "Task created without await - potential race"),
        (r'shared_state|global_state', "Shared state - verify synchronization"),
      ]

      for pattern, msg in race_patterns:
        if re.search(pattern, diff_text):
          issues.append(f"üèÉ Race: {msg}")

      # Check for test coverage
      new_files = re.findall(r'\+\+\+ b/(\S+\.py)', diff_text)
      new_files = [f for f in new_files if not f.startswith('tests/') and not f.endswith('test.py')]
      test_files = re.findall(r'\+\+\+ b/(tests/\S+\.py)', diff_text)

      if new_files and not test_files:
        issues.append(f"üìù Tests: New code in {len(new_files)} files but no test updates")

      # Add architecture alignment issues from knowledge check
      if 'architecture_alignment' in dir() and architecture_alignment and architecture_alignment.get('has_issues'):
          for arch_issue in architecture_alignment.get('issues', []):
              issues.append(f"üèóÔ∏è Architecture: {arch_issue}")

      # Add problematic patterns from memory
      if 'problematic_patterns' in dir() and problematic_patterns and problematic_patterns.get('found'):
          for pattern_issue in problematic_patterns.get('issues', []):
              issues.append(f"‚ö†Ô∏è Known Pattern: {pattern_issue}")

      result = issues if issues else []
    output: code_issues

  # Track common code issues by author for coaching
  - name: track_author_code_issues
    description: "Track common code issues per author for coaching"
    condition: "code_issues and len(code_issues) > 0"
    compute: |
      from datetime import datetime

      # Get author from context (will be set after extract_mr_author runs)
      # For now, extract from MR details directly
      author = ""
      if parsers and mr_details:
          author = parsers.extract_author_from_mr(mr_details) or ""

      if author and memory:
          data = memory.read_memory("learned/teammate_preferences")
          teammates = data.get("teammates", {}) if isinstance(data.get("teammates"), dict) else {}

          if author not in teammates:
              teammates[author] = {}

          # Track issue types by category
          issue_types = teammates[author].get("common_issues", {})
          for issue in code_issues:
              # Extract category from emoji prefix
              if "Security" in issue:
                  issue_type = "security"
              elif "Memory" in issue:
                  issue_type = "memory"
              elif "Race" in issue:
                  issue_type = "race_condition"
              elif "Tests" in issue:
                  issue_type = "missing_tests"
              else:
                  issue_type = "other"
              issue_types[issue_type] = issue_types.get(issue_type, 0) + 1

          teammates[author]["common_issues"] = issue_types
          teammates[author]["last_code_review"] = datetime.now().isoformat()

          data["teammates"] = teammates
          memory.write_memory("learned/teammate_preferences", data)

          result = f"Tracked {len(code_issues)} issues for {author}"
      else:
          result = "No author or memory available"
    output: author_issues_tracking
    on_error: continue

  # Step 14: Prepare review summary
  - name: prepare_summary
    description: "Compile review findings"
    compute: |
      from scripts.common.parsers import linkify_jira_keys, linkify_mr_ids
      is_slack = inputs.get('slack_format', True)
      lines = []
      lines.append("## PR Review Summary")
      mr_str = linkify_mr_ids(f"!{mr_id}", slack_format=is_slack)
      lines.append(f"**MR:** {mr_str}")
      jira_str = linkify_jira_keys(jira_key, slack_format=is_slack)
      lines.append(f"**Jira:** {jira_str}")
      lines.append("")

      # Format validation
      if format_issues and format_issues != ["Format OK"]:
        lines.append("### ‚ùå Commit Format")
        for issue in format_issues[:3]:
          lines.append(f"- {issue}")
        lines.append("")

      # Description validation
      if description_issues and description_issues != ["Description OK"]:
        lines.append("### ‚ö†Ô∏è Description")
        for issue in description_issues[:2]:
          lines.append(f"- {issue}")
        lines.append("")

      # Pipeline status
      if 'Failed' in str(gitlab_pipeline) or 'failed' in str(konflux_pipelines):
        lines.append("### ‚ùå Pipelines")
        lines.append("- CI/CD pipelines have failures - check before approving")
        lines.append("")

      # Code issues
      if code_issues:
        lines.append("### üîç Code Review")
        for issue in code_issues[:5]:
          lines.append(f"- {issue}")
        lines.append("")

      # Similar code patterns found
      if 'similar_code' in dir() and similar_code and similar_code.get('found'):
        lines.append("### üîé Similar Code in Codebase")
        lines.append("Found similar patterns - verify consistency:")
        for result in similar_code.get('results', [])[:3]:
          lines.append(f"- `{result}`")
        lines.append("")

      # Coding patterns from knowledge
      if 'coding_patterns' in dir() and coding_patterns and coding_patterns.get('found'):
        lines.append("### üìö Project Coding Patterns")
        lines.append("Ensure changes follow these patterns:")
        for pattern in coding_patterns.get('patterns', [])[:3]:
          lines.append(f"- {pattern}")
        lines.append("")

      # Test results
      if inputs.get('run_tests'):
        lines.append("### üß™ Tests")
        if test_result:
          if '‚úÖ' in str(test_result):
            lines.append("- Tests passed")
          else:
            lines.append("- Tests failed - see details above")
        else:
          lines.append("- Could not run tests")
        lines.append("")

      # Determine overall status
      has_blockers = (
        (format_issues and format_issues != ["Format OK"]) or
        ('Failed' in str(gitlab_pipeline)) or
        (code_issues and any('Security' in str(i) for i in code_issues)) or
        (inputs.get('run_tests') and test_result and '‚ùå' in str(test_result))
      )

      if has_blockers:
        lines.append("### üö´ Recommendation: Request Changes")
      else:
        lines.append("### ‚úÖ Recommendation: Approve")

      result = '\n'.join(lines)
    output: review_summary

  # ==================== AUTO ACTIONS ====================

  # Step 15: Determine action based on review
  - name: determine_action
    description: "Decide whether to approve or request changes"
    compute: |
      # Check for blockers
      has_security_issues = code_issues and any('Security' in str(i) for i in code_issues)
      has_format_issues = format_issues and format_issues != ["Format OK"]
      has_test_failures = inputs.get('run_tests') and test_result and '‚ùå' in str(test_result)
      has_pipeline_failures = 'Failed' in str(gitlab_pipeline) or 'failed' in str(konflux_pipelines).lower()

      # Determine action
      if has_security_issues:
        action = "request_changes"
        reason = "Security issues detected"
      elif has_test_failures:
        action = "request_changes"
        reason = "Tests are failing"
      elif has_pipeline_failures:
        action = "request_changes"
        reason = "CI/CD pipelines failing"
      elif has_format_issues:
        action = "request_changes"
        reason = "Commit format issues"
      elif code_issues and len(code_issues) > 3:
        action = "request_changes"
        reason = f"{len(code_issues)} code issues found"
      else:
        action = "approve"
        reason = "No blocking issues found"

      result = {"action": action, "reason": reason}
    output: review_action

  # Step 16: Build feedback message (for request_changes)
  - name: build_feedback
    description: "Build feedback message for GitLab"
    condition: "review_action.get('action') == 'request_changes'"
    compute: |
      lines = [f"## Code Review Feedback for {jira_key}"]
      lines.append("")

      if format_issues and format_issues != ["Format OK"]:
        lines.append("### Commit Format")
        for issue in format_issues[:3]:
          lines.append(f"- {issue}")
        lines.append("")

      if code_issues:
        lines.append("### Code Issues")
        for issue in code_issues[:5]:
          lines.append(f"- {issue}")
        lines.append("")

      if inputs.get('run_tests') and test_result and '‚ùå' in str(test_result):
        lines.append("### Tests")
        lines.append("- Local tests are failing. Please investigate.")
        lines.append("")

      lines.append("---")
      lines.append("*Automated review by AI Workflow*")

      result = '\n'.join(lines)
    output: feedback_message

  # Step 17: Post feedback comment (if requesting changes)
  - name: post_feedback
    description: "Post review feedback to MR"
    condition: "review_action.get('action') == 'request_changes'"
    tool: gitlab_mr_comment
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ mr_id }}"
      message: "{{ feedback_message }}"
    output: feedback_posted
    on_error: continue

  # Step 18: Approve MR (if no issues)
  - name: approve_mr
    description: "Approve the MR on GitLab"
    condition: "review_action.get('action') == 'approve'"
    tool: gitlab_mr_approve
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ mr_id }}"
    output: approval_result
    on_error: continue

  # Cache successful review for quick reference on re-reviews
  - name: cache_successful_review
    description: "Save successful review patterns for quick reference"
    condition: "review_action.get('action') == 'approve' and approval_result"
    compute: |
      from datetime import datetime

      author = ""
      if parsers and mr_details:
          author = parsers.extract_author_from_mr(mr_details) or ""

      review_entry = {
          "mr_id": str(mr_id),
          "jira_key": jira_key if jira_key != 'NOT_FOUND' else None,
          "author": author,
          "approved_at": datetime.now().isoformat(),
          "commit_count": commit_history.get("count", 0) if 'commit_history' in dir() and commit_history else 0,
          "files_changed": diff_analysis.get("file_count", 0) if 'diff_analysis' in dir() and diff_analysis else 0,
          "gitlab_project": resolved.get("gitlab_project", ""),
      }

      if memory:
          data = memory.read_memory("learned/review_cache")
          approvals = data.get("recent_approvals", []) if isinstance(data.get("recent_approvals"), list) else []

          # Remove old entry for same MR if exists
          approvals = [a for a in approvals if str(a.get("mr_id", "")) != str(mr_id)]
          approvals.append(review_entry)

          # Keep last 100 reviews
          data["recent_approvals"] = approvals[-100:]
          data["last_updated"] = datetime.now().isoformat()
          memory.write_memory("learned/review_cache", data)

          result = "Review cached"
      else:
          result = "Cache skipped: memory not available"
    output: review_cache_result
    on_error: continue

  # Step 18b: Auto-merge if approved (optional)
  - name: emit_approval_hook
    description: "Notify author that MR was approved"
    condition: "review_action.get('action') == 'approve' and approval_result"
    compute: |
      # emit_event and parsers are available from skill engine safe_globals
      if emit_event and parsers:
          # Extract author from MR details using shared parser
          author = parsers.extract_author_from_mr(mr_details) or ""

          emit_event("review_approved", {
              "mr_id": str(mr_id),
              "author": author,
              "project": resolved.get("gitlab_project", ""),
          })
          result = "hook sent"
      else:
          result = "hook skipped: globals not available"
    output: approval_hook_result
    on_error: continue

  # Step 18c: Emit feedback hook
  - name: emit_feedback_hook
    description: "Notify author that feedback was posted"
    condition: "review_action.get('action') == 'request_changes' and feedback_posted"
    compute: |
      # emit_event and parsers are available from skill engine safe_globals
      if emit_event and parsers:
          # Extract author from MR details using shared parser
          author = parsers.extract_author_from_mr(mr_details) or ""

          issue_count = len(code_issues) if code_issues else 0
          if format_issues and format_issues != ["Format OK"]:
              issue_count += len(format_issues)

          emit_event("review_comment", {
              "mr_id": str(mr_id),
              "author": author,
              "project": resolved.get("gitlab_project", ""),
              "issue_count": str(issue_count),
          })
          result = "hook sent"
      else:
          result = "hook skipped: globals not available"
    output: feedback_hook_result
    on_error: continue

  # Step 18d: Extract author for Slack notification
  - name: extract_mr_author
    description: "Extract MR author GitLab username for Slack notification"
    compute: |
      from scripts.common.parsers import extract_author_from_mr, extract_mr_url

      author = extract_author_from_mr(mr_details) or ""

      # Get MR URL for the notification using shared parser
      mr_url = extract_mr_url(str(mr_details))

      if not mr_url:
          # Construct URL from project and mr_id
          mr_url = f"https://gitlab.cee.redhat.com/{resolved.get('gitlab_project', '')}/-/merge_requests/{mr_id}"

      result = {
          "author": author,
          "mr_url": mr_url
      }
    output: author_info

  # Load author's past review history from memory
  - name: load_author_history
    description: "Check if we've reviewed this author's MRs before"
    condition: "author_info and author_info.get('author')"
    compute: |
      author = author_info.get("author", "") if author_info else ""

      data = memory.read_memory("learned/teammate_preferences") if memory else {}
      teammates = data.get("teammates", {}) if isinstance(data.get("teammates"), dict) else {}

      author_data = teammates.get(author, {})

      reviews_given = author_data.get("reviews_given", 0)
      approvals = author_data.get("approvals", 0)

      result = {
          "author": author,
          "previous_reviews": reviews_given,
          "approval_rate": round(approvals / max(reviews_given, 1) * 100),
          "last_reviewed": author_data.get("last_reviewed"),
          "common_issues": author_data.get("common_issues", {}),
          "is_known_author": reviews_given > 0,
      }
    output: author_history
    on_error: continue

  # Step 18e: Slack DM author on approval
  - name: slack_notify_approval
    description: "Send Slack DM to author that MR was approved"
    condition: "review_action.get('action') == 'approve' and author_info.get('author')"
    tool: slack_dm_gitlab_user
    args:
      gitlab_username: "{{ author_info.get('author') }}"
      notification_type: "approval"
      text: |
        Your MR has been *approved*! üéâ

        üìã *MR:* <{{ author_info.get('mr_url', '') }}|!{{ mr_id }}>
        {% if jira_key != 'NOT_FOUND' %}
        üé´ *Jira:* {{ jira_key }}
        {% endif %}

        Ready to merge when you are.
    output: slack_approval_result
    on_error: continue

  # Step 18f: Slack DM author on feedback
  - name: slack_notify_feedback
    description: "Send Slack DM to author that feedback was posted"
    condition: "review_action.get('action') == 'request_changes' and author_info.get('author')"
    tool: slack_dm_gitlab_user
    args:
      gitlab_username: "{{ author_info.get('author') }}"
      notification_type: "feedback"
      text: |
        I've left some feedback on your MR.

        üìã *MR:* <{{ author_info.get('mr_url', '') }}|!{{ mr_id }}>
        {% if jira_key != 'NOT_FOUND' %}
        üé´ *Jira:* {{ jira_key }}
        {% endif %}
        üìù *Issues found:* {{ (code_issues|length if code_issues else 0) + (format_issues|length if format_issues and format_issues != ['Format OK'] else 0) }}

        Please take a look when you get a chance.
    output: slack_feedback_result
    on_error: continue

  # Step 19: Add Jira comment about review
  - name: jira_comment
    description: "Add review note to Jira issue"
    condition: "{{ jira_key != 'NOT_FOUND' }}"
    tool: jira_add_comment
    args:
      issue_key: "{{ jira_key }}"
      comment: |
        MR !{{ mr_id }} reviewed.
        Action: {{ review_action.get('action', 'unknown') }}
        Reason: {{ review_action.get('reason', '') }}
    output: jira_updated
    on_error: auto_heal  # Jira API - may need auth refresh

  # ==================== MEMORY INTEGRATION ====================

  - name: build_memory_context
    description: "Build context for memory updates"
    compute: |
      from datetime import datetime

      result = {
          "timestamp": datetime.now().isoformat(),
          "mr_id": str(mr_id) if mr_id else "",
          "action": review_action.get("action", "unknown") if review_action else "unknown",
          "author": author_info.get("author", "unknown") if author_info else "unknown",
      }
    output: memory_context

  - name: log_session_review
    description: "Log review to session"
    tool: memory_session_log
    args:
      action: "Reviewed MR !{{ memory_context.mr_id }} ({{ memory_context.action }})"
      details: "Author: {{ memory_context.author }}, Jira: {{ jira_key }}"
    on_error: continue

  - name: track_review_given
    description: "Track review in teammate preferences (learn reviewer patterns)"
    condition: "memory_context.action == 'approve' or memory_context.action == 'feedback'"
    compute: |
      # Use shared memory helpers
      data = memory.read_memory("learned/teammate_preferences")
      teammates = data.get("teammates", {}) if isinstance(data.get("teammates"), dict) else {}
      author = memory_context.get("author", "unknown")

      if author != "unknown":
          if author not in teammates:
              teammates[author] = {
                  "reviews_given": 0,
                  "approvals": 0,
                  "feedback_given": 0,
              }

          teammates[author]["reviews_given"] = teammates[author].get("reviews_given", 0) + 1
          if memory_context.get("action") == "approve":
              teammates[author]["approvals"] = teammates[author].get("approvals", 0) + 1
          else:
              teammates[author]["feedback_given"] = teammates[author].get("feedback_given", 0) + 1

          teammates[author]["last_reviewed"] = memory_context["timestamp"]
          data["teammates"] = teammates
          memory.write_memory("learned/teammate_preferences", data)

      result = f"tracked review for {author}"
    output: review_tracking_result
    on_error: continue

  # Share review context for other skills (investigate_alert, debug_prod)
  - name: save_shared_review_context
    description: "Share review findings for debug_prod, investigate_alert skills"
    condition: "review_action.get('action') == 'request_changes' or (code_issues and len(code_issues) > 0)"
    compute: |
      from scripts.common.memory import save_shared_context

      context = {
          "type": "pr_review",
          "mr_id": str(mr_id) if mr_id else "",
          "jira_key": jira_key if jira_key != 'NOT_FOUND' else None,
          "author": memory_context.get("author", "unknown"),
          "action": review_action.get("action") if review_action else "unknown",
          "issues_found": code_issues[:5] if code_issues else [],
          "format_issues": format_issues if format_issues and format_issues != ["Format OK"] else [],
          "pipeline_status": "failed" if "failed" in str(gitlab_pipeline).lower() else "ok",
          "files_changed": diff_analysis.get("files_changed", [])[:10] if 'diff_analysis' in dir() and diff_analysis else [],
          "has_security_issues": any("Security" in str(i) for i in code_issues) if code_issues else False,
      }

      save_shared_context("review_pr", context, ttl_hours=24)
      result = "Context shared for 24h"
    output: shared_context_result
    on_error: continue

  # Load any active investigation context at session end for awareness
  - name: check_investigation_context
    description: "Check if there's an active investigation that might relate to this review"
    compute: |
      from scripts.common.memory import load_shared_context

      ctx = load_shared_context()

      if ctx and ctx.get("type") in ["investigate_alert", "debug_prod"]:
          result = {
              "investigation_active": True,
              "investigation_type": ctx.get("type"),
              "namespace": ctx.get("namespace"),
              "hint": "‚ö†Ô∏è Active investigation ongoing - review findings may be relevant",
          }
      else:
          result = {"investigation_active": False}
    output: investigation_context
    on_error: continue

  # ==================== LEARNING FROM FAILURES ====================

  - name: detect_review_failures
    description: "Detect failure patterns from PR review operations"
    compute: |
      errors_detected = []

      # Check GitLab API failures
      mr_text = str(mr_details) if 'mr_details' in dir() and mr_details else ""
      diff_text = str(mr_diff_raw) if 'mr_diff_raw' in dir() and mr_diff_raw else ""
      combined = mr_text + diff_text

      if "no such host" in combined.lower() or "dial tcp" in combined.lower():
          errors_detected.append({
              "tool": "gitlab_mr_view",
              "pattern": "no such host",
              "cause": "VPN not connected - internal GitLab not reachable",
              "fix": "Run vpn_connect() to connect to Red Hat VPN"
          })
      if "unauthorized" in combined.lower() or "401" in combined:
          errors_detected.append({
              "tool": "gitlab_mr_view",
              "pattern": "unauthorized",
              "cause": "GitLab authentication failed or token expired",
              "fix": "Check GitLab token in config.json"
          })
      if "not found" in combined.lower() and "merge request" in combined.lower():
          errors_detected.append({
              "tool": "gitlab_mr_view",
              "pattern": "merge request not found",
              "cause": "MR ID is incorrect or MR has been closed/merged",
              "fix": "Verify MR ID with gitlab_mr_list()"
          })

      # Check Jira failures
      jira_text = str(jira_details) if 'jira_details' in dir() and jira_details else ""
      if "issue does not exist" in jira_text.lower():
          errors_detected.append({
              "tool": "jira_view_issue",
              "pattern": "issue does not exist",
              "cause": "Jira issue key is incorrect or issue was deleted",
              "fix": "Verify Jira issue key format (e.g., AAP-12345)"
          })

      result = errors_detected
    output: review_errors_detected
    on_error: continue

  - name: learn_review_vpn_failure
    description: "Learn from GitLab VPN failures"
    condition: "review_errors_detected and any(e.get('pattern') == 'no such host' for e in review_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "gitlab_mr_view"
      error_pattern: "no such host"
      root_cause: "VPN not connected - internal GitLab not reachable"
      fix_description: "Run vpn_connect() to connect to Red Hat VPN"
    output: review_vpn_fix_learned
    on_error: continue

  - name: learn_review_auth_failure
    description: "Learn from GitLab auth failures"
    condition: "review_errors_detected and any(e.get('pattern') == 'unauthorized' for e in review_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "gitlab_mr_view"
      error_pattern: "unauthorized"
      root_cause: "GitLab authentication failed or token expired"
      fix_description: "Check GitLab token in config.json"
    output: review_auth_fix_learned
    on_error: continue

# ==================== OUTPUT ====================

outputs:
  - name: summary
    value: |
      {{ review_summary }}

      ---

      ## Action Taken

      **Decision:** {{ review_action.get('action', 'unknown') | upper }}
      **Reason:** {{ review_action.get('reason', '') }}

      {% if review_action.get('action') == 'approve' %}
      ‚úÖ MR has been approved on GitLab.
      {% if merge_result %}
      üîÄ MR has been merged (or will merge when pipeline succeeds).
      {% elif inputs.get('auto_merge', False) %}
      ‚ö†Ô∏è Auto-merge was requested but failed - merge manually.
      {% endif %}
      {% else %}
      üìù Feedback has been posted to the MR.
      {% endif %}

      {% if jira_key != 'NOT_FOUND' %}
      üìã Jira issue {{ jira_key }} updated with review note.
      {% endif %}

      {% if not inputs.get('run_tests', False) %}
      ---

      ## üß™ Want to Run Local Tests?

      This review was **static analysis only**. To also checkout the branch and run tests:

      ```
      skill_run("review_pr", '{"mr_id": {{ mr_id }}, "run_tests": true}')
      ```

      Or just say: "Also run the tests for this MR"
      {% endif %}

  - name: context
    value:
      mr_id: "{{ mr_id }}"
      jira_key: "{{ jira_key }}"
      action: "{{ review_action.get('action') }}"
      has_issues: "{{ code_issues | length > 0 }}"
      format_ok: "{{ format_issues == ['Format OK'] }}"
      tests_ran: "{{ inputs.get('run_tests', False) }}"
      tests_passed: "{{ '‚úÖ' in str(test_result) if test_result else 'not_run' }}"
