# Skill: Scale Deployment
# Scale pods up/down and monitor rollout

name: scale_deployment
description: |
  Scale a Kubernetes deployment and monitor the rollout.

  Use for:
  - Scaling up for high traffic
  - Scaling down to save resources
  - Testing with different replica counts
  - Recovering from OOM by increasing replicas

  The skill will:
  1. Get current deployment state
  2. Scale to desired replicas
  3. Monitor rollout progress
  4. Report final state

version: "1.0"

inputs:
  - name: deployment
    type: string
    required: true
    description: "Deployment name to scale"

  - name: replicas
    type: integer
    required: true
    description: "Desired number of replicas"

  - name: namespace
    type: string
    required: false
    default: "tower-analytics-stage"
    description: "Kubernetes namespace"

  - name: environment
    type: string
    required: false
    default: "stage"
    description: "Environment: 'stage', 'production', 'ephemeral'"

  - name: wait
    type: boolean
    required: false
    default: true
    description: "Wait for rollout to complete"

steps:
  # ==================== MEMORY CONTEXT ====================

  - name: check_known_issues
    description: "Check for known scaling issues"
    compute: |
      # Check known issues for kubectl scale
      issues = memory.check_known_issues("kubectl_scale", "")

      result = {
          "has_known_issues": len(issues.get("matches", [])) > 0 if issues else False,
          "issues": issues.get("matches", [])[:3] if issues else [],
      }
    output: known_issues
    on_error: continue

  - name: get_scaling_gotchas
    description: "Get scaling-related gotchas from knowledge"
    tool: knowledge_query
    args:
      project: "automation-analytics-backend"
      persona: "devops"
      section: "gotchas"
    output: scaling_gotchas_raw
    on_error: continue

  - name: parse_scaling_gotchas
    description: "Parse scaling-related gotchas"
    compute: |
      gotchas_result = scaling_gotchas_raw if 'scaling_gotchas_raw' in dir() and scaling_gotchas_raw else {}

      scaling_gotchas = []
      if isinstance(gotchas_result, dict) and gotchas_result.get('found'):
          content = gotchas_result.get('content', [])
          if isinstance(content, list):
              # Filter for scaling-related gotchas
              for g in content:
                  g_str = str(g).lower()
                  if any(kw in g_str for kw in ['scale', 'replica', 'pod', 'resource', 'memory', 'cpu', 'oom']):
                      scaling_gotchas.append(g)

      result = {
          'gotchas': scaling_gotchas[:5],
          'has_gotchas': len(scaling_gotchas) > 0,
      }
    output: scale_gotchas
    on_error: continue

  - name: load_scale_history
    description: "Load previous scaling history for this deployment"
    compute: |
      # Load learned patterns
      patterns = memory.read_memory("learned/patterns") or {}
      scale_history = patterns.get("deployment_scales", [])

      # Find history for this specific deployment
      deployment_history = [
          s for s in scale_history
          if s.get("deployment") == inputs.deployment and s.get("namespace") == inputs.namespace
      ]

      # Check if we have common scaling patterns
      common_scales = {}
      for h in deployment_history:
          to_rep = h.get("to_replicas")
          if to_rep:
              common_scales[to_rep] = common_scales.get(to_rep, 0) + 1

      result = {
          "previous_scales": len(deployment_history),
          "recent_scales": deployment_history[-5:],
          "common_replica_counts": sorted(common_scales.keys(), key=lambda x: common_scales[x], reverse=True)[:3],
      }
    output: scale_history
    on_error: continue

  # ==================== GET CURRENT STATE ====================

  - name: get_current_deployment
    description: "Get current deployment info"
    tool: kubectl_get_deployments
    args:
      namespace: "{{ inputs.namespace }}"
      environment: "{{ inputs.environment }}"
    output: deployments_raw
    on_error: auto_heal  # K8s cluster - may need kube_login

  - name: parse_current_state
    description: "Parse current deployment state"
    compute: |
      deploy_text = str(deployments_raw) if deployments_raw else ""

      # Find our deployment
      current_replicas = None
      ready_replicas = None
      found = False

      for line in deploy_text.split("\n"):
          if inputs.deployment.lower() in line.lower():
              found = True
              parts = line.split()
              if len(parts) >= 2:
                  # Format is usually: NAME READY UP-TO-DATE AVAILABLE
                  ready_str = parts[1] if len(parts) > 1 else ""
                  if "/" in ready_str:
                      ready, total = ready_str.split("/")
                      ready_replicas = int(ready) if ready.isdigit() else 0
                      current_replicas = int(total) if total.isdigit() else 0
              break

      result = {
          "found": found,
          "current_replicas": current_replicas,
          "ready_replicas": ready_replicas,
          "deployment_list": deploy_text[:500] if deploy_text else "No deployments",
      }
    output: current_state

  # ==================== SCALE DEPLOYMENT ====================

  - name: scale_deployment
    description: "Scale the deployment to desired replicas"
    condition: "current_state.found"
    tool: kubectl_scale
    args:
      resource: "deployment/{{ inputs.deployment }}"
      replicas: "{{ inputs.replicas }}"
      namespace: "{{ inputs.namespace }}"
      environment: "{{ inputs.environment }}"
    output: scale_result
    on_error: auto_heal  # K8s cluster - may need kube_login

  - name: parse_scale_result
    description: "Parse scale result"
    compute: |
      scale_text = str(scale_result) if 'scale_result' in dir() and scale_result else ""

      success = "scaled" in scale_text.lower() or "error" not in scale_text.lower()

      result = {
          "success": success,
          "message": scale_text[:200] if scale_text else "No response",
      }
    output: scale_status
    on_error: continue

  # ==================== MONITOR ROLLOUT ====================

  - name: check_rollout_status
    description: "Check rollout status"
    condition: "scale_status.success and inputs.wait"
    tool: kubectl_rollout
    args:
      action: "status"
      resource: "deployment/{{ inputs.deployment }}"
      namespace: "{{ inputs.namespace }}"
      environment: "{{ inputs.environment }}"
    output: rollout_status_raw
    on_error: continue

  - name: parse_rollout
    description: "Parse rollout status"
    condition: "rollout_status_raw"
    compute: |
      rollout_text = str(rollout_status_raw) if rollout_status_raw else ""

      complete = "successfully rolled out" in rollout_text.lower()
      in_progress = "waiting" in rollout_text.lower() or "progressing" in rollout_text.lower()

      result = {
          "complete": complete,
          "in_progress": in_progress,
          "message": rollout_text[:300] if rollout_text else "Unknown",
      }
    output: rollout_status
    on_error: continue

  # ==================== GET FINAL STATE ====================

  - name: get_final_state
    description: "Get final deployment state"
    condition: "scale_status.success"
    tool: kubectl_get_deployments
    args:
      namespace: "{{ inputs.namespace }}"
      environment: "{{ inputs.environment }}"
    output: final_deployments_raw
    on_error: auto_heal  # K8s cluster - may need kube_login

  - name: get_pod_status
    description: "Get pod status after scaling"
    condition: "scale_status.success"
    tool: kubectl_get_pods
    args:
      namespace: "{{ inputs.namespace }}"
      environment: "{{ inputs.environment }}"
    output: pods_after_raw
    on_error: auto_heal  # K8s cluster - may need kube_login

  - name: parse_final_state
    description: "Parse final state"
    compute: |
      deploy_text = str(final_deployments_raw) if 'final_deployments_raw' in dir() and final_deployments_raw else ""
      pods_text = str(pods_after_raw) if 'pods_after_raw' in dir() and pods_after_raw else ""

      # Count pods for our deployment
      pod_count = 0
      running_count = 0
      for line in pods_text.split("\n"):
          if inputs.deployment.lower() in line.lower():
              pod_count += 1
              if "running" in line.lower():
                  running_count += 1

      result = {
          "pod_count": pod_count,
          "running_count": running_count,
          "all_running": pod_count == running_count and pod_count > 0,
          "pods_preview": pods_text[:400] if pods_text else "",
      }
    output: final_state
    on_error: continue

  # ==================== MEMORY ====================

  - name: log_scaling
    description: "Log scaling action"
    condition: "scale_status.success"
    tool: memory_session_log
    args:
      action: "Scaled {{ inputs.deployment }}"
      details: "{{ current_state.current_replicas }} ‚Üí {{ inputs.replicas }} replicas in {{ inputs.namespace }}"
    on_error: continue

  - name: learn_scale_pattern
    description: "Learn from this scaling for future reference"
    condition: "scale_status.success"
    compute: |
      from datetime import datetime

      # Load patterns
      patterns = memory.read_memory("learned/patterns") or {}
      if "deployment_scales" not in patterns:
          patterns["deployment_scales"] = []

      # Record this scale operation
      scale_record = {
          "deployment": inputs.deployment,
          "namespace": inputs.namespace,
          "environment": inputs.environment,
          "from_replicas": current_state.current_replicas if current_state else None,
          "to_replicas": inputs.replicas,
          "success": scale_status.success if scale_status else False,
          "all_running": final_state.all_running if final_state else False,
          "timestamp": datetime.now().isoformat(),
      }

      patterns["deployment_scales"].append(scale_record)

      # Keep last 100 scale records
      patterns["deployment_scales"] = patterns["deployment_scales"][-100:]

      memory.write_memory("learned/patterns", patterns)
      result = "scale pattern learned"
    output: pattern_learn_result
    on_error: continue

  - name: track_scale_failures
    description: "Track scaling failures for auto-remediation"
    condition: "scale_status and not scale_status.success"
    compute: |
      from datetime import datetime

      # Load patterns
      patterns = memory.read_memory("learned/patterns") or {}
      if "scale_failures" not in patterns:
          patterns["scale_failures"] = []

      # Track this failure
      key = f"{inputs.namespace}/{inputs.deployment}"
      existing = [f for f in patterns["scale_failures"] if f.get("key") == key]

      if existing:
          existing[0]["count"] = existing[0].get("count", 1) + 1
          existing[0]["last_failure"] = datetime.now().isoformat()
          existing[0]["last_error"] = scale_status.message if scale_status else None
      else:
          patterns["scale_failures"].append({
              "key": key,
              "deployment": inputs.deployment,
              "namespace": inputs.namespace,
              "target_replicas": inputs.replicas,
              "count": 1,
              "first_failure": datetime.now().isoformat(),
              "last_failure": datetime.now().isoformat(),
              "last_error": scale_status.message if scale_status else None,
          })

      # Keep top 30 problematic deployments
      patterns["scale_failures"] = sorted(
          patterns["scale_failures"],
          key=lambda x: x.get("count", 0),
          reverse=True
      )[:30]

      memory.write_memory("learned/patterns", patterns)
      result = "scale failure tracked"
    output: failure_tracking_result
    on_error: continue

  - name: update_environment_state
    description: "Update environment state after scaling"
    condition: "scale_status.success"
    compute: |
      from datetime import datetime

      # Update environment state
      env_data = memory.read_memory("state/environments") or {}
      if "environments" not in env_data:
          env_data["environments"] = {}

      env_key = inputs.environment
      if env_key not in env_data["environments"]:
          env_data["environments"][env_key] = {"status": "unknown", "deployments": {}}

      env = env_data["environments"][env_key]
      env["deployments"][inputs.deployment] = {
          "last_scale": datetime.now().isoformat(),
          "replicas": inputs.replicas,
          "running": final_state.running_count if final_state else 0,
      }

      memory.write_memory("state/environments", env_data)
      result = "environment state updated"
    output: env_update_result
    on_error: continue

  # ==================== SEMANTIC SEARCH ====================

  - name: search_deployment_code
    description: "Search for code related to this deployment"
    tool: code_search
    args:
      query: "{{ inputs.deployment }} kubernetes deployment scaling"
      project: "automation-analytics-backend"
      limit: 3
    output: deployment_code_raw
    on_error: continue

  - name: parse_deployment_code
    description: "Parse deployment code search results"
    condition: "deployment_code_raw"
    compute: |
      code_result = deployment_code_raw if deployment_code_raw else {}

      related_code = []
      if isinstance(code_result, dict) and code_result.get('results'):
          for r in code_result.get('results', [])[:3]:
              related_code.append({
                  'file': r.get('file_path', ''),
                  'score': r.get('score', 0),
              })

      result = {
          'code': related_code,
          'count': len(related_code),
      }
    output: deployment_code_analysis
    on_error: continue

  # ==================== LEARNING FROM FAILURES ====================

  - name: detect_scale_failures
    description: "Detect failure patterns from scale operations"
    compute: |
      errors_detected = []

      # Check kubectl failures
      deployments_text = str(deployments_raw) if 'deployments_raw' in dir() and deployments_raw else ""
      scale_text = str(scale_result) if 'scale_result' in dir() and scale_result else ""
      combined = deployments_text + scale_text

      if "unauthorized" in combined.lower() or "forbidden" in combined.lower():
          errors_detected.append({
              "tool": "kubectl_scale",
              "pattern": "unauthorized",
              "cause": "Kubernetes auth expired",
              "fix": "Run kube_login(cluster='stage' or 'prod') to refresh credentials"
          })
      if "not found" in combined.lower() and "deployment" in combined.lower():
          errors_detected.append({
              "tool": "kubectl_scale",
              "pattern": "deployment not found",
              "cause": "Deployment name is incorrect or doesn't exist",
              "fix": "List deployments with kubectl_get_deployments()"
          })

      result = errors_detected
    output: scale_errors_detected
    on_error: continue

  - name: learn_scale_auth_failure
    description: "Learn from k8s auth failures"
    condition: "scale_errors_detected and any(e.get('pattern') == 'unauthorized' for e in scale_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "kubectl_scale"
      error_pattern: "unauthorized"
      root_cause: "Kubernetes auth expired"
      fix_description: "Run kube_login(cluster='stage' or 'prod') to refresh credentials"
    output: scale_auth_fix_learned
    on_error: continue

outputs:
  - name: report
    value: |
      ## ‚öñÔ∏è Scale Deployment

      **Deployment:** `{{ inputs.deployment }}`
      **Namespace:** `{{ inputs.namespace }}`
      **Environment:** {{ inputs.environment }}

      ---

      {% if not current_state.found %}
      ### ‚ùå Deployment Not Found

      Deployment `{{ inputs.deployment }}` not found in namespace `{{ inputs.namespace }}`.

      **Available deployments:**
      ```
      {{ current_state.deployment_list }}
      ```

      {% elif not scale_status.success %}
      ### ‚ùå Scaling Failed

      {{ scale_status.message }}

      {% else %}
      ### ‚úÖ Scaling {{ "Complete" if rollout_status and rollout_status.complete else "In Progress" }}

      | Metric | Before | After |
      |--------|--------|-------|
      | Replicas | {{ current_state.current_replicas }} | {{ inputs.replicas }} |
      | Ready | {{ current_state.ready_replicas }} | {{ final_state.running_count if final_state else "..." }} |

      {% if rollout_status %}
      **Rollout Status:** {{ "‚úÖ Complete" if rollout_status.complete else "üîÑ In Progress" }}
      ```
      {{ rollout_status.message }}
      ```
      {% endif %}

      {% if final_state %}
      **Pods Running:** {{ final_state.running_count }}/{{ final_state.pod_count }}
      {% endif %}

      {% endif %}

      ---

      ### Commands

      **Check deployment:**
      ```python
      kubectl_get_deployments(namespace='{{ inputs.namespace }}', environment='{{ inputs.environment }}')
      ```

      **Check pods:**
      ```python
      kubectl_get_pods(namespace='{{ inputs.namespace }}', environment='{{ inputs.environment }}')
      ```

      **Rollback if needed:**
      ```python
      kubectl_rollout(action='undo', resource='deployment/{{ inputs.deployment }}', namespace='{{ inputs.namespace }}', environment='{{ inputs.environment }}')
      ```

      {% if scale_gotchas and scale_gotchas.has_gotchas %}
      ---

      ### ‚ö†Ô∏è Scaling Gotchas

      {% for gotcha in scale_gotchas.gotchas[:3] %}
      - {{ gotcha }}
      {% endfor %}
      {% endif %}

      {% if known_issues and known_issues.has_known_issues %}
      ---

      ### üí° Known Issues

      {% for issue in known_issues.issues[:3] %}
      - {{ issue.pattern if issue.pattern else issue }}
      {% endfor %}
      {% endif %}

  - name: context
    value:
      deployment: "{{ inputs.deployment }}"
      scaled: "{{ scale_status.success if scale_status else false }}"
      from_replicas: "{{ current_state.current_replicas }}"
      to_replicas: "{{ inputs.replicas }}"
      rollout_complete: "{{ rollout_status.complete if rollout_status else false }}"
