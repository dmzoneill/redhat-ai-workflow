# Skill: Weekly Summary
# Generate a summary of work from session logs
# Enhanced with knowledge and learning metrics

name: weekly_summary
description: |
  Generate a summary of work from session logs.

  Aggregates session logs from the past week (or specified period)
  and provides a summary of:
  - Issues worked on
  - MRs created/reviewed
  - Deployments and debugging sessions
  - Patterns learned
  - **NEW:** Knowledge growth metrics
  - **NEW:** Vector search usage stats
  - **NEW:** Learned patterns summary

  Useful for weekly reports or sprint reviews.

version: "2.0"

inputs:
  - name: days
    type: integer
    required: false
    default: 7
    description: "Number of days to look back (default: 7)"

  - name: format
    type: string
    required: false
    default: "markdown"
    description: "Output format: 'markdown' or 'slack'"

  - name: repo
    type: string
    required: false
    default: "automation-analytics-backend"
    description: "Repository to get commit history from"

  - name: slack_format
    type: boolean
    required: false
    default: false
    description: "Use Slack link format"

steps:

  # ==================== PROACTIVE ISSUE DETECTION ====================

  - name: check_gitlab_known_issues
    description: "Check for known GitLab issues before starting"
    tool: check_known_issues
    args:
      tool_name: "gitlab_mr_list"
      error_text: ""
    output: gitlab_known_issues
    on_error: continue

  - name: check_jira_known_issues
    description: "Check for known Jira issues before starting"
    tool: check_known_issues
    args:
      tool_name: "jira_search"
      error_text: ""
    output: jira_known_issues
    on_error: continue

  - name: init_autoheal
    description: "Initialize failure tracking"
    compute: |
      result = {"failures": []}
    output: autoheal_state
    on_error: continue

  # ==================== GATHER DATA FROM MULTIPLE SOURCES ====================

  - name: get_recent_commits
    description: "Get commits from the past week"
    tool: git_log
    args:
      repo: "{{ inputs.repo }}"
      since: "{{ inputs.days }} days ago"
      author: ""
    output: commits_raw
    on_error: continue

  - name: get_my_jira_issues
    description: "Get my recently updated Jira issues"
    tool: jira_my_issues
    args:
      max_results: 20
    output: jira_issues_raw
    on_error: auto_heal  # Jira API - may need auth refresh

  - name: get_my_mrs
    description: "Get my recent merge requests"
    tool: gitlab_mr_list
    args:
      project: "automation-analytics/automation-analytics-backend"
      state: "all"
      per_page: 10
    output: mrs_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  - name: get_recent_releases
    description: "Get recent Konflux releases"
    tool: konflux_list_releases
    args:
      namespace: "aap-aa-tenant"
      limit: 5
    output: releases_raw
    on_error: continue

  - name: get_recent_images
    description: "Get recent image tags from Quay"
    tool: quay_list_aa_tags
    args:
      limit: 10
    output: quay_tags_raw
    on_error: continue

  - name: search_slack_discussions
    description: "Search Slack for relevant team discussions"
    tool: slack_search_messages
    args:
      query: "in:team-automation-analytics after:{{ inputs.days }}d"
      limit: 20
    output: slack_discussions_raw
    on_error: continue

  - name: parse_slack_discussions
    description: "Parse Slack discussions"
    compute: |
      slack_text = str(slack_discussions_raw) if 'slack_discussions_raw' in dir() and slack_discussions_raw else ""

      discussions = []
      for line in slack_text.split("\n")[:20]:
          if line.strip():
              discussions.append(line.strip()[:100])

      # Look for important topics
      important = []
      keywords = ["deploy", "release", "bug", "urgent", "hotfix", "incident", "outage"]
      for msg in discussions:
          if any(kw in msg.lower() for kw in keywords):
              important.append(msg)

      result = {
          "total": len(discussions),
          "discussions": discussions[:10],
          "important": important[:5],
          "has_slack": len(discussions) > 0,
      }
    output: slack_activity
    on_error: continue

  # ==================== READ SESSION LOGS ====================

  - name: get_session_logs
    description: "Read session logs from memory directory"
    compute: |
      from pathlib import Path
      from datetime import datetime, timedelta
      import yaml

      days = inputs.get("days", 7)
      cutoff = datetime.now() - timedelta(days=days)

      logs_dir = Path.home() / "src/redhat-ai-workflow/memory/logs"
      session_entries = []

      if logs_dir.exists():
          for log_file in sorted(logs_dir.glob("*.yaml"), reverse=True):
              try:
                  # Parse date from filename (e.g., 2024-01-15.yaml)
                  date_str = log_file.stem
                  log_date = datetime.strptime(date_str, "%Y-%m-%d")

                  if log_date >= cutoff:
                      with open(log_file) as f:
                          data = yaml.safe_load(f) or {}

                      for entry in data.get("entries", []):
                          entry["date"] = date_str
                          session_entries.append(entry)
              except (ValueError, yaml.YAMLError) as e:
                  continue

      result = {
          "entries": session_entries,
          "count": len(session_entries),
          "days_covered": days,
      }
    output: logs

  - name: load_current_work
    description: "Load current work state"
    tool: memory_read
    args:
      key: "state/current_work"
    output: current_work_raw
    on_error: continue

  # ==================== KNOWLEDGE & LEARNING METRICS ====================

  - name: load_learned_patterns
    description: "Load learned patterns from memory"
    tool: memory_read
    args:
      key: "learned/patterns"
    output: learned_patterns_raw
    on_error: continue

  - name: load_tool_fixes
    description: "Load tool fixes from memory"
    tool: memory_read
    args:
      key: "learned/tool_fixes"
    output: tool_fixes_raw
    on_error: continue

  - name: get_knowledge_stats
    description: "Get knowledge base statistics"
    tool: knowledge_query
    args:
      project: "automation-analytics-backend"
      section: "metadata"
    output: knowledge_metadata_raw
    on_error: continue

  - name: get_vector_stats
    description: "Get vector index statistics"
    tool: code_stats
    args:
      project: "automation-analytics-backend"
    output: vector_stats_raw
    on_error: continue

  - name: parse_learning_metrics
    description: "Parse learning and knowledge metrics"
    compute: |
      import re
      import yaml

      metrics = {
          "patterns_count": 0,
          "tool_fixes_count": 0,
          "knowledge_confidence": 0,
          "vector_files": 0,
          "vector_chunks": 0,
          "vector_searches": 0,
          "recent_patterns": [],
          "recent_fixes": [],
      }

      # Parse learned patterns
      if 'learned_patterns_raw' in dir() and learned_patterns_raw:
          try:
              if isinstance(learned_patterns_raw, str):
                  patterns_data = yaml.safe_load(learned_patterns_raw) or {}
              else:
                  patterns_data = learned_patterns_raw or {}

              # Count all pattern categories
              for key, value in patterns_data.items():
                  if isinstance(value, list):
                      metrics["patterns_count"] += len(value)
                      # Get recent patterns
                      for p in value[-3:]:
                          if isinstance(p, dict):
                              metrics["recent_patterns"].append(p.get("pattern", str(p))[:50])
          except:
              pass

      # Parse tool fixes
      if 'tool_fixes_raw' in dir() and tool_fixes_raw:
          try:
              if isinstance(tool_fixes_raw, str):
                  fixes_data = yaml.safe_load(tool_fixes_raw) or {}
              else:
                  fixes_data = tool_fixes_raw or {}

              fixes_list = fixes_data.get("fixes", [])
              metrics["tool_fixes_count"] = len(fixes_list)

              # Get recent fixes
              for fix in fixes_list[-3:]:
                  if isinstance(fix, dict):
                      metrics["recent_fixes"].append(fix.get("tool_name", "unknown"))
          except:
              pass

      # Parse knowledge metadata
      if 'knowledge_metadata_raw' in dir() and knowledge_metadata_raw:
          raw_text = str(knowledge_metadata_raw)
          conf_match = re.search(r'confidence[:\s]+(\d+)', raw_text, re.I)
          if conf_match:
              metrics["knowledge_confidence"] = int(conf_match.group(1))

      # Parse vector stats
      if 'vector_stats_raw' in dir() and vector_stats_raw:
          raw_text = str(vector_stats_raw)
          files_match = re.search(r'(\d+)\s*files', raw_text)
          chunks_match = re.search(r'(\d+)\s*chunks', raw_text)
          searches_match = re.search(r'(\d+)\s*searches', raw_text)

          if files_match:
              metrics["vector_files"] = int(files_match.group(1))
          if chunks_match:
              metrics["vector_chunks"] = int(chunks_match.group(1))
          if searches_match:
              metrics["vector_searches"] = int(searches_match.group(1))

      learning_metrics = metrics
    output: learning_metrics
    on_error: continue

  - name: parse_external_data
    description: "Parse data from git, jira, gitlab, konflux"
    compute: |
      # Parse commits
      commits_text = str(commits_raw) if commits_raw else ""
      commit_count = commits_text.count("\n") if commits_text else 0

      # Parse Jira issues
      jira_text = str(jira_issues_raw) if jira_issues_raw else ""
      import re
      jira_keys = re.findall(r'AAP-\d+', jira_text)
      unique_jira = list(set(jira_keys))[:10]

      # Parse MRs
      mrs_text = str(mrs_raw) if mrs_raw else ""
      mr_ids = re.findall(r'!(\d+)', mrs_text)
      unique_mrs = list(set(mr_ids))[:10]

      # Parse releases
      releases_text = str(releases_raw) if releases_raw else ""
      release_count = releases_text.lower().count("release") if releases_text else 0

      # Parse Quay tags
      quay_text = str(quay_tags_raw) if quay_tags_raw else ""
      tag_count = len(re.findall(r'[a-f0-9]{40}', quay_text))

      result = {
          "commit_count": commit_count,
          "jira_issues": unique_jira,
          "gitlab_mrs": unique_mrs,
          "release_count": release_count,
          "image_count": tag_count,
          "commits_sample": commits_text[:500] if commits_text else "",
      }
    output: external_data

  - name: analyze_logs
    description: "Analyze session logs for summary"
    compute: |
      import re
      from collections import defaultdict
      import yaml

      entries = logs.get("entries", [])

      # Categories to track
      issues_worked = set()
      mrs_created = set()
      mrs_reviewed = set()
      deployments = []
      debug_sessions = []
      patterns_learned = []
      notifications_sent = []
      other_actions = []

      for entry in entries:
          action = entry.get("action", "")
          details = entry.get("details", "")
          date = entry.get("date", "")

          # Extract Jira issues
          jira_matches = re.findall(r"AAP-\d+", action + " " + details)
          issues_worked.update(jira_matches)

          # Extract MR IDs
          mr_matches = re.findall(r"!(\d+)", action)
          for mr_id in mr_matches:
              if "Created MR" in action or "create" in action.lower():
                  mrs_created.add(mr_id)
              elif "Reviewed" in action or "review" in action.lower():
                  mrs_reviewed.add(mr_id)

          # Categorize actions
          action_lower = action.lower()
          if "deployed" in action_lower or "ephemeral" in action_lower:
              deployments.append({"date": date, "action": action})
          elif "debug" in action_lower or "investigated" in action_lower:
              debug_sessions.append({"date": date, "action": action})
          elif "learned pattern" in action_lower:
              patterns_learned.append({"date": date, "action": action})
          elif "notified" in action_lower:
              notifications_sent.append({"date": date, "action": action})
          elif "Started work" in action or "Closed issue" in action:
              pass  # Already captured via issues_worked
          else:
              other_actions.append({"date": date, "action": action})

      # Parse current work for active items
      current_work = {}
      if current_work_raw and isinstance(current_work_raw, str):
          try:
              current_work = yaml.safe_load(current_work_raw) or {}
          except:
              pass

      result = {
          "issues_worked": list(issues_worked),
          "mrs_created": list(mrs_created),
          "mrs_reviewed": list(mrs_reviewed),
          "deployments": deployments[:10],
          "debug_sessions": debug_sessions[:10],
          "patterns_learned": patterns_learned,
          "notifications": notifications_sent[:10],
          "other": other_actions[:20],
          "active_issues": current_work.get("active_issues", []),
          "open_mrs": current_work.get("open_mrs", []),
          "total_entries": logs.get("count", 0),
      }
    output: analysis

  - name: format_summary
    description: "Format summary for output"
    compute: |
      from scripts.common.parsers import linkify_jira_keys, linkify_mr_ids
      lines = []
      fmt = inputs.get("format", "markdown")
      is_slack = inputs.get("slack_format", fmt == "slack")

      if is_slack:
          lines.append("*ðŸ“Š Weekly Summary*\n")
      else:
          lines.append("# ðŸ“Š Weekly Summary\n")
          lines.append(f"*{analysis['total_entries']} session entries over {logs['days_covered']} days*\n")

      # Issues worked
      if analysis["issues_worked"]:
          if is_slack:
              lines.append(f"*Issues Worked:* {len(analysis['issues_worked'])}")
              linked_issues = [linkify_jira_keys(issue, slack_format=True) for issue in analysis["issues_worked"][:10]]
              lines.append(", ".join(linked_issues))
          else:
              lines.append("## ðŸŽ« Issues Worked")
              for issue in analysis["issues_worked"][:10]:
                  lines.append(f"- [{issue}](https://issues.redhat.com/browse/{issue})")
          lines.append("")

      # MRs
      if analysis["mrs_created"] or analysis["mrs_reviewed"]:
          if is_slack:
              created = [linkify_mr_ids(f"!{mr}", slack_format=True) for mr in analysis["mrs_created"][:5]]
              reviewed = [linkify_mr_ids(f"!{mr}", slack_format=True) for mr in analysis["mrs_reviewed"][:5]]
              lines.append(f"*MRs:* {len(analysis['mrs_created'])} created ({', '.join(created)}), {len(analysis['mrs_reviewed'])} reviewed ({', '.join(reviewed)})")
          else:
              lines.append("## ðŸ”€ Merge Requests")
              if analysis["mrs_created"]:
                  created = [linkify_mr_ids(f"!{mr}") for mr in analysis["mrs_created"][:5]]
                  lines.append(f"**Created:** {', '.join(created)}")
              if analysis["mrs_reviewed"]:
                  reviewed = [linkify_mr_ids(f"!{mr}") for mr in analysis["mrs_reviewed"][:5]]
                  lines.append(f"**Reviewed:** {', '.join(reviewed)}")
          lines.append("")

      # Deployments
      if analysis["deployments"]:
          if is_slack:
              lines.append(f"*Deployments:* {len(analysis['deployments'])}")
          else:
              lines.append("## ðŸš€ Deployments")
              for d in analysis["deployments"][:5]:
                  lines.append(f"- {d['date']}: {linkify_jira_keys(linkify_mr_ids(d['action']))}")
          lines.append("")

      # Debug sessions
      if analysis["debug_sessions"]:
          if is_slack:
              lines.append(f"*Debug Sessions:* {len(analysis['debug_sessions'])}")
          else:
              lines.append("## ðŸ” Debug Sessions")
              for d in analysis["debug_sessions"][:5]:
                  lines.append(f"- {d['date']}: {linkify_jira_keys(linkify_mr_ids(d['action']))}")
          lines.append("")

      # Patterns learned
      if analysis["patterns_learned"]:
          if is_slack:
              lines.append(f"*Patterns Learned:* {len(analysis['patterns_learned'])}")
          else:
              lines.append("## ðŸ“š Patterns Learned")
              for p in analysis["patterns_learned"]:
                  lines.append(f"- {p['date']}: {p['action']}")
          lines.append("")

      # Currently active
      if analysis["active_issues"] or analysis["open_mrs"]:
          if not is_slack:
              lines.append("## ðŸŽ¯ Currently Active")
              if analysis["active_issues"]:
                  lines.append("**Active Issues:**")
                  for issue in analysis["active_issues"][:5]:
                      key = issue.get('key', '?')
                      summary = linkify_jira_keys(issue.get('summary', 'No summary')[:50])
                      lines.append(f"- [{key}](https://issues.redhat.com/browse/{key}): {summary}")
              if analysis["open_mrs"]:
                  lines.append("**Open MRs:**")
                  for mr in analysis["open_mrs"][:5]:
                      mr_id = mr.get('id', '?')
                      title = linkify_jira_keys(mr.get('title', 'No title')[:50])
                      lines.append(f"- [!{mr_id}](https://gitlab.cee.redhat.com/automation-analytics/automation-analytics-backend/-/merge_requests/{mr_id}): {title}")
          lines.append("")

      result = "\n".join(lines)
    output: formatted_summary

  # ==================== SEMANTIC SEARCH ====================

  - name: search_weekly_code
    description: "Search for code related to weekly activity"
    condition: "analysis and analysis.get('issues_worked')"
    tool: code_search
    args:
      query: "weekly summary {{ analysis.issues_worked[0] if analysis.issues_worked else '' }} activity"
      project: "automation-analytics-backend"
      limit: 3
    output: weekly_code_raw
    on_error: continue

  - name: parse_weekly_code
    description: "Parse weekly code search results"
    condition: "weekly_code_raw"
    compute: |
      code_result = weekly_code_raw if weekly_code_raw else {}

      related_code = []
      if isinstance(code_result, dict) and code_result.get('found'):
          for item in code_result.get('content', []):
              related_code.append(item.get('path', '') + ":" + str(item.get('line_number', '')))

      result = {
          "has_code": len(related_code) > 0,
          "code_snippets": related_code[:5],
      }
    output: weekly_code_search
    on_error: continue

  # ==================== LEARNING FROM FAILURES ====================

  - name: detect_weekly_failures
    description: "Detect failure patterns from weekly summary data gathering"
    compute: |
      errors_detected = []

      # Check git failures
      commit_text = str(recent_commits_raw) if 'recent_commits_raw' in dir() and recent_commits_raw else ""
      if "permission denied" in commit_text.lower():
          errors_detected.append({
              "tool": "git_log",
              "pattern": "permission denied",
              "cause": "Git repository access denied",
              "fix": "Check repository permissions and SSH keys"
          })

      # Check GitLab failures
      gitlab_text = str(gitlab_mrs_raw) if 'gitlab_mrs_raw' in dir() and gitlab_mrs_raw else ""
      if "no such host" in gitlab_text.lower():
          errors_detected.append({
              "tool": "gitlab_mr_list",
              "pattern": "no such host",
              "cause": "VPN not connected - internal GitLab not reachable",
              "fix": "Run vpn_connect() to connect to Red Hat VPN"
          })

      # Check Jira failures
      jira_text = str(jira_issues_raw) if 'jira_issues_raw' in dir() and jira_issues_raw else ""
      if "command timed out" in jira_text.lower():
          errors_detected.append({
              "tool": "jira_search",
              "pattern": "command timed out",
              "cause": "Jira API timeout",
              "fix": "Check VPN connection and retry"
          })

      # Check Konflux failures
      konflux_text = str(konflux_releases_raw) if 'konflux_releases_raw' in dir() and konflux_releases_raw else ""
      if "no route to host" in konflux_text.lower():
          errors_detected.append({
              "tool": "konflux_list_releases",
              "pattern": "no route to host",
              "cause": "VPN not connected - Konflux cluster not reachable",
              "fix": "Run vpn_connect() and kube_login('konflux')"
          })

      result = errors_detected
    output: weekly_errors_detected
    on_error: continue

  - name: learn_weekly_vpn_failure
    description: "Learn from GitLab VPN failures"
    condition: "weekly_errors_detected and any(e.get('pattern') == 'no such host' for e in weekly_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "gitlab_mr_list"
      error_pattern: "no such host"
      root_cause: "VPN not connected - internal GitLab not reachable"
      fix_description: "Run vpn_connect() to connect to Red Hat VPN"
    output: weekly_vpn_fix_learned
    on_error: continue

  - name: learn_weekly_konflux_failure
    description: "Learn from Konflux VPN failures"
    condition: "weekly_errors_detected and any(e.get('pattern') == 'no route to host' for e in weekly_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "konflux_list_releases"
      error_pattern: "no route to host"
      root_cause: "VPN not connected - Konflux cluster not reachable"
      fix_description: "Run vpn_connect() and kube_login('konflux')"
    output: weekly_konflux_fix_learned
    on_error: continue

  - name: log_weekly_session
    description: "Log weekly summary generation to session"
    tool: memory_session_log
    args:
      action: "Generated weekly summary"
      details: "{{ analysis.total_entries }} session entries, {{ external_data.commit_count if external_data else 0 }} commits"
    on_error: continue

outputs:
  - name: summary
    value: |
      {{ formatted_summary }}

      ## ðŸ“ˆ Activity Metrics

      | Source | Count |
      |--------|-------|
      | Commits ({{ inputs.repo }}) | {{ external_data.commit_count }} |
      | Jira Issues Touched | {{ external_data.jira_issues|length }} |
      | GitLab MRs | {{ external_data.gitlab_mrs|length }} |
      | Konflux Releases | {{ external_data.release_count }} |
      | Images Built | {{ external_data.image_count }} |

      {% if external_data.jira_issues %}
      ### Jira Issues
      {% for issue in external_data.jira_issues[:5] %}
      - [{{ issue }}](https://issues.redhat.com/browse/{{ issue }})
      {% endfor %}
      {% endif %}

      {% if external_data.gitlab_mrs %}
      ### GitLab MRs
      {% for mr in external_data.gitlab_mrs[:5] %}
      - !{{ mr }}
      {% endfor %}
      {% endif %}

      ## ðŸ§  Knowledge & Learning

      | Metric | Value |
      |--------|-------|
      | Knowledge Confidence | {{ learning_metrics.knowledge_confidence }}% |
      | Learned Patterns | {{ learning_metrics.patterns_count }} |
      | Tool Fixes | {{ learning_metrics.tool_fixes_count }} |
      | Vector Index Files | {{ learning_metrics.vector_files }} |
      | Vector Index Chunks | {{ learning_metrics.vector_chunks }} |
      | Code Searches | {{ learning_metrics.vector_searches }} |

      {% if learning_metrics.recent_patterns %}
      ### Recent Patterns Learned
      {% for pattern in learning_metrics.recent_patterns %}
      - {{ pattern }}
      {% endfor %}
      {% endif %}

      {% if learning_metrics.recent_fixes %}
      ### Recent Tool Fixes
      {% for fix in learning_metrics.recent_fixes %}
      - {{ fix }}
      {% endfor %}
      {% endif %}

      ---

      *Generated from {{ analysis.total_entries }} session log entries + live data.*
