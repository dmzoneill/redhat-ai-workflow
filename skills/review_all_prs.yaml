# Skill: Review All Open PRs
# Batch review of open MRs with intelligent follow-up

name: review_all_prs
description: |
  Review all open MRs in a project.

  Resolves project from repo_name or current directory if not explicitly provided.

  Automatically excludes your own MRs (detected from system username).

  For each MR (authored by others):
  - If I gave feedback and author addressed it ‚Üí approve
  - If I gave feedback and author didn't respond ‚Üí skip
  - If I gave feedback and author responded but issues remain ‚Üí more feedback
  - If no previous review from me ‚Üí run full review

  Also shows your own MRs that have feedback from others.

version: "1.2"

inputs:
  - name: project
    type: string
    required: false
    default: ""
    description: "GitLab project path (resolved from repo_name if not provided)"

  - name: repo_name
    type: string
    required: false
    description: "Repository name from config (e.g., 'automation-analytics-backend')"

  - name: reviewer
    type: string
    required: false
    default: ""
    description: "Filter by reviewer username (leave empty for all open MRs)"

  - name: limit
    type: integer
    required: false
    default: 10
    description: "Maximum number of MRs to process"

  - name: dry_run
    type: boolean
    required: false
    default: false
    description: "If true, show what would happen without taking action"

  - name: include_my_mrs
    type: boolean
    required: false
    default: true
    description: "Show my own MRs that have feedback to respond to"

  - name: auto_rebase
    type: boolean
    required: false
    default: true
    description: "Automatically rebase my MRs that have merge conflicts"

# No hardcoded constants - resolved dynamically

steps:
  # ==================== RESOLVE PROJECT ====================

  - name: resolve_project
    description: "Determine which GitLab project to check"
    compute: |
      import os
      from scripts.common.config_loader import load_config

      gitlab_project = None

      # Load config using shared loader
      config = load_config()
      repos = config.get("repositories", {})

      # Explicit project
      if inputs.project:
          gitlab_project = inputs.project
      # Repo name from config
      elif inputs.repo_name and inputs.repo_name in repos:
          gitlab_project = repos[inputs.repo_name].get("gitlab")
      # Fall back to cwd
      else:
          cwd = os.getcwd()
          for name, cfg in repos.items():
              if cfg.get("path") == cwd:
                  gitlab_project = cfg.get("gitlab")
                  break

      if not gitlab_project:
          gitlab_project = "automation-analytics/automation-analytics-backend"

      result = {"gitlab_project": gitlab_project}
    output: resolved
  # ==================== GET CURRENT USER ====================

  # Step 0: Get current system username
  - name: get_username
    description: "Get current system username to exclude own MRs"
    compute: |
      import getpass
      import os
      from scripts.common.config_loader import load_config

      # Get username from config or fallback to system
      cfg = load_config()

      # Use config if available, otherwise fallback to system user
      gitlab_username = cfg.get("user", {}).get("gitlab_username") or \
                        cfg.get("user", {}).get("username") or \
                        os.getenv('USER') or getpass.getuser()

      result = gitlab_username
    output: my_username

  # ==================== GATHER MRs ====================

  # Step 1: Get list of open MRs
  - name: list_open_mrs
    description: "Fetch all open MRs from GitLab"
    tool: gitlab_mr_list
    args:
      project: "{{ resolved.gitlab_project }}"
      state: opened
      per_page: "{{ inputs.limit }}"
    output: open_mrs

  # Step 2: Parse MR list and separate own vs others
  - name: parse_mrs
    description: "Extract MR IDs, separate own MRs from others to review using shared parsers"
    compute: |
      from scripts.common.parsers import parse_mr_list, separate_mrs_by_author

      # Parse all MRs with author information
      all_mrs = parse_mr_list(open_mrs or "", include_author=True)

      # Separate into own vs others using shared function
      separated = separate_mrs_by_author(all_mrs, my_username)

      result = {
        'to_review': separated['to_review'][:inputs.limit],
        'my_mrs': separated['my_mrs'],
        'my_username': my_username
      }
    output: parsed_mrs

  # ==================== CHECK MY MRs FOR MERGE CONFLICTS ====================

  # Step 2b: Get detailed status of first of my MRs to check for conflicts
  - name: check_my_mr_status
    description: "Check if any of my MRs have merge conflicts"
    condition: "parsed_mrs.get('my_mrs') and len(parsed_mrs['my_mrs']) > 0 and inputs.include_my_mrs"
    tool: gitlab_mr_view
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ parsed_mrs['my_mrs'][0]['iid'] }}"
    output: my_first_mr_details
    on_error: continue

  # Step 2c: Detect if my MR has merge conflicts
  - name: detect_merge_conflicts
    description: "Check for merge conflicts or needs rebase using shared parser"
    condition: "my_first_mr_details"
    compute: |
      from scripts.common.parsers import analyze_mr_status

      mr = parsed_mrs['my_mrs'][0] if parsed_mrs.get('my_mrs') else {}

      # Use shared MR status analyzer
      analysis = analyze_mr_status(my_first_mr_details or "")

      result = {
        'has_conflicts': analysis['has_conflicts'],
        'needs_rebase': analysis['needs_rebase'],
        'mr_id': mr.get('iid'),
        'title': mr.get('title', ''),
        'reason': analysis['action'] if analysis['needs_rebase'] else None
      }
    output: my_mr_conflict_status

  # Step 2d: Auto-rebase my MR if it has conflicts
  - name: auto_rebase_my_mr
    description: "Automatically rebase my MR if it has conflicts"
    condition: "my_mr_conflict_status and my_mr_conflict_status.get('needs_rebase') and inputs.auto_rebase and not inputs.dry_run"
    tool: skill_run
    args:
      skill_name: rebase_pr
      inputs: '{"mr_id": {{ my_mr_conflict_status.get("mr_id", 0) }}, "force_push": false}'
    output: rebase_result
    on_error: continue

  # Step 3: Process each MR (excluding my own)
  - name: process_mrs
    description: "Analyze each MR for review status"
    compute: |
      results = []
      mrs_to_review = parsed_mrs.get('to_review', [])

      for mr in mrs_to_review:
        mr_id = mr.get('iid')
        if not mr_id:
          continue

        mr_result = {
          'iid': mr_id,
          'title': mr.get('title', ''),
          'author': mr.get('author', 'unknown'),
          'action': 'pending',
          'reason': '',
          'my_last_feedback': None,
          'author_replied': False,
          'concerns_addressed': False
        }

        results.append(mr_result)

      result = results
    output: mr_analysis

  # Step 4: For each MR, get comments and determine status
  # Note: This is a simplified version - in practice you'd call gitlab_mr_comments for each
  - name: analyze_review_status
    description: "Determine review status for each MR"
    compute: |
      # This would ideally call gitlab_mr_comments for each MR
      # For now, we'll set up the structure and the actual tool calls happen below

      result = {
        'needs_review': [],
        'can_approve': [],
        'needs_followup': [],
        'skip': []
      }
    output: review_status

# ==================== INDIVIDUAL MR PROCESSING ====================

  # Step 5: Get comments for first pending MR
  # Note: This is a simplified approach - a full implementation would loop
  - name: get_mr_comments
    description: "Get comments for MRs to check review status"
    condition: "len(mr_analysis) > 0"
    tool: gitlab_mr_view
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ mr_analysis[0]['iid'] }}"
    output: first_mr_details
    on_error: continue

  # Step 6: Analyze first MR's review status
  - name: analyze_first_mr
    description: "Check if first MR needs action using shared parsers"
    condition: "len(mr_analysis) > 0 and first_mr_details"
    compute: |
      from scripts.common.parsers import analyze_mr_status, analyze_review_status

      mr = mr_analysis[0]
      details = first_mr_details or ""

      # Get base analysis from shared parser
      analysis = analyze_mr_status(details, my_username)

      # Get review-specific analysis from shared parser
      author = mr.get('author', '')
      review_status = analyze_review_status(details, my_username, author)

      # Enhance with unresolved issues check
      if review_status['my_feedback_exists'] and review_status['author_replied']:
        if analysis['unresolved']:
          action = "needs_followup"
          reason = "Author replied but unresolved issues"
        else:
          action = "can_approve"
          reason = "Author addressed feedback"
      else:
        action = review_status['recommended_action']
        reason = review_status['reason']

      mr['action'] = action
      mr['reason'] = reason

      result = mr
    output: first_mr_action

  # Step 7: Run full review if needed
  - name: run_full_review
    description: "Run full review for MR that needs it"
    condition: "first_mr_action and first_mr_action.get('action') == 'needs_full_review' and not inputs.dry_run"
    tool: skill_run
    args:
      skill_name: review_pr
      inputs: '{"mr_id": {{ first_mr_action.get("iid", 0) }}, "skip_tests": true}'
    output: full_review_result
    on_error: continue

  # Step 8: Approve MR if author addressed feedback
  - name: auto_approve
    description: "Approve MR where author addressed feedback"
    condition: "first_mr_action and first_mr_action.get('action') == 'can_approve' and not inputs.dry_run"
    tool: gitlab_mr_approve
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ first_mr_action.get('iid') }}"
    output: approve_result
    on_error: continue

  # Step 9: Post follow-up if needed
  - name: post_followup
    description: "Post follow-up comment for unresolved issues"
    condition: "first_mr_action and first_mr_action.get('action') == 'needs_followup' and not inputs.dry_run"
    tool: gitlab_mr_comment
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ first_mr_action.get('iid') }}"
      comment: |
        ## Follow-up Review

        I see you've responded to my feedback. However, there appear to be unresolved discussions.

        Could you please address the outstanding items so we can move forward?

        ---
        *Automated follow-up by AI Workflow*
    output: followup_result
    on_error: continue

  # Step 10: Build summary
  - name: build_summary
    description: "Compile batch review results"
    compute: |
      my_username = parsed_mrs.get('my_username', 'unknown')
      my_mrs = parsed_mrs.get('my_mrs', [])
      mrs_to_review = parsed_mrs.get('to_review', [])

      lines = ["## Batch PR Review Summary", ""]
      lines.append(f"**Project:** {resolved['gitlab_project']}")
      lines.append(f"**Current User:** {my_username}")
      lines.append(f"**MRs to Review (by others):** {len(mrs_to_review)}")
      lines.append(f"**My Open MRs:** {len(my_mrs)}")
      lines.append(f"**Dry Run:** {'Yes' if inputs.dry_run else 'No'}")
      lines.append("")

      # Categorize actions
      actions = {
        'can_approve': [],
        'needs_full_review': [],
        'needs_followup': [],
        'skip': []
      }

      if first_mr_action:
        action = first_mr_action.get('action', 'skip')
        actions.get(action, actions['skip']).append(first_mr_action)

      # Summary by category
      if actions['can_approve']:
        lines.append("### ‚úÖ Approved")
        for mr in actions['can_approve']:
          status = "Would approve" if inputs.dry_run else "Approved"
          lines.append(f"- !{mr['iid']}: {mr.get('title', '')} ({status})")
        lines.append("")

      if actions['needs_full_review']:
        lines.append("### üîç Full Review Performed")
        for mr in actions['needs_full_review']:
          lines.append(f"- !{mr['iid']}: {mr.get('title', '')}")
        lines.append("")

      if actions['needs_followup']:
        lines.append("### üí¨ Follow-up Posted")
        for mr in actions['needs_followup']:
          status = "Would post follow-up" if inputs.dry_run else "Follow-up posted"
          lines.append(f"- !{mr['iid']}: {mr.get('title', '')} ({status})")
        lines.append("")

      if actions['skip']:
        lines.append("### ‚è≠Ô∏è Skipped (Waiting for Author)")
        for mr in actions['skip']:
          lines.append(f"- !{mr['iid']}: {mr.get('title', '')} - {mr.get('reason', 'No action needed')}")
        lines.append("")

      # Show my own MRs that might need attention
      if my_mrs and inputs.include_my_mrs:
        lines.append("---")
        lines.append("")
        lines.append("### üìù Your Open MRs")

        # Check if we rebased any MR
        if my_mr_conflict_status and my_mr_conflict_status.get('needs_rebase'):
          first_mr_id = my_mr_conflict_status.get('mr_id')
          reason = my_mr_conflict_status.get('reason', 'needs rebase')

          if rebase_result:
            lines.append(f"- !{first_mr_id}: {my_mr_conflict_status.get('title', '')} üîÑ **Rebased!**")
            lines.append(f"  - {reason}")
            lines.append(f"  - *See rebase results below*")
          elif inputs.auto_rebase and not inputs.dry_run:
            lines.append(f"- !{first_mr_id}: {my_mr_conflict_status.get('title', '')} ‚ö†Ô∏è **Rebase attempted**")
          elif inputs.dry_run:
            lines.append(f"- !{first_mr_id}: {my_mr_conflict_status.get('title', '')} üîÑ **Would rebase** (dry run)")
            lines.append(f"  - {reason}")
          else:
            lines.append(f"- !{first_mr_id}: {my_mr_conflict_status.get('title', '')} ‚ö†Ô∏è **Needs rebase**")
            lines.append(f"  - {reason}")
            lines.append(f"  - Run: `skill_run(\"rebase_pr\", '{{\"mr_id\": {first_mr_id}}}')`")

        # Show remaining MRs
        for mr in my_mrs[:5]:
          if my_mr_conflict_status and mr.get('iid') == my_mr_conflict_status.get('mr_id'):
            continue  # Already shown above
          lines.append(f"- !{mr.get('iid', '?')}: {mr.get('title', '')}")

        lines.append("")
        lines.append("*Check these for feedback from reviewers. Use:*")
        lines.append(f"*`gitlab_mr_view(project=\"{resolved['gitlab_project']}\", mr_id=<id>)` to see comments*")
        lines.append("")

      # Remaining MRs
      remaining = len(mr_analysis) - 1
      if remaining > 0:
        lines.append(f"---")
        lines.append(f"*{remaining} more MRs not processed in this batch.*")
        lines.append(f"*Run again to continue processing.*")

      result = '\n'.join(lines)
    output: batch_summary

  # Step 11: Emit batch review hook
  - name: emit_batch_review_hook
    description: "Notify team channel about batch review"
    condition: "not inputs.dry_run"
    compute: |
      # emit_event is available from skill engine safe_globals
      # Count actions taken
      approved = 1 if first_mr_action and first_mr_action.get('action') == 'can_approve' else 0
      changes = 1 if first_mr_action and first_mr_action.get('action') in ['needs_full_review', 'needs_followup'] else 0
      total = len(mr_analysis) if mr_analysis else 0

      if emit_event and total > 0:
          emit_event("batch_review_completed", {
              "count": str(total),
              "approved": str(approved),
              "changes": str(changes),
              "project": resolved.get("gitlab_project", ""),
          })
          result = "hook sent"
      elif total == 0:
          result = "no MRs to report"
      else:
          result = "hook skipped: emit_event not available"
    output: batch_hook_result
    on_error: continue

  # ==================== MEMORY INTEGRATION ====================

  - name: build_memory_context
    description: "Build context for memory updates"
    compute: |
      from datetime import datetime

      # Count actions
      approved = 0
      feedback = 0
      total = len(mr_analysis) if mr_analysis else 0

      for mr in (mr_analysis or []):
          action = mr.get("action", "")
          if action == "can_approve":
              approved += 1
          elif action in ["needs_full_review", "needs_followup"]:
              feedback += 1

      result = {
          "timestamp": datetime.now().isoformat(),
          "total": total,
          "approved": approved,
          "feedback": feedback,
      }
    output: memory_context

  - name: log_batch_review
    description: "Log batch review to session"
    condition: "memory_context.total > 0"
    tool: memory_session_log
    args:
      action: "Batch reviewed {{ memory_context.total }} MRs"
      details: "Approved: {{ memory_context.approved }}, Feedback: {{ memory_context.feedback }}"
    on_error: continue

  - name: update_teammate_prefs
    description: "Update teammate preferences with review counts"
    condition: "memory_context.total > 0 and mr_analysis"
    compute: |
      # Use shared memory helpers
      data = memory.read_memory("learned/teammate_preferences")
      teammates = data.get("teammates", {}) if isinstance(data.get("teammates"), dict) else {}

      # Track reviews given to each author
      for mr in (mr_analysis or []):
          author = mr.get("author", "unknown")
          if author and author != "unknown":
              if author not in teammates:
                  teammates[author] = {"reviews_received": 0}
              teammates[author]["reviews_received"] = teammates[author].get("reviews_received", 0) + 1
              teammates[author]["last_reviewed_at"] = memory_context["timestamp"]

      data["teammates"] = teammates
      memory.write_memory("learned/teammate_preferences", data)
      result = "updated"
    output: prefs_update_result
    on_error: continue

# ==================== OUTPUT ====================

outputs:
  - name: summary
    value: |
      {{ batch_summary }}

      ---

      ## Quick Actions

      - **Review next batch:** `skill_run("review_all_prs", '{}')`
      - **Review specific MR:** `skill_run("review_pr", '{"mr_id": 123}')`
      - **Approve specific MR:** `gitlab_mr_approve(project="...", mr_id=123)`

  - name: stats
    value:
      total_mrs: "{{ len(mr_analysis) }}"
      processed: 1
      dry_run: "{{ inputs.dry_run }}"
