# Skill: Review All Open PRs
# Batch review of open MRs with intelligent follow-up

name: review_all_prs
description: |
  Review all open MRs in a project.

  Resolves project from repo_name or current directory if not explicitly provided.

  Automatically excludes your own MRs (detected from system username).

  For each MR (authored by others):
  - If I gave feedback and author addressed it ‚Üí approve
  - If I gave feedback and author didn't respond ‚Üí skip
  - If I gave feedback and author responded but issues remain ‚Üí more feedback
  - If no previous review from me ‚Üí run full review

  Also shows your own MRs that have feedback from others.

version: "1.3"

inputs:
  - name: project
    type: string
    required: false
    default: ""
    description: "GitLab project path (resolved from repo_name if not provided)"

  - name: repo_name
    type: string
    required: false
    description: "Repository name from config (e.g., 'automation-analytics-backend')"

  - name: reviewer
    type: string
    required: false
    default: ""
    description: "Filter by reviewer username (leave empty for all open MRs)"

  - name: limit
    type: integer
    required: false
    default: 10
    description: "Maximum number of MRs to process"

  - name: dry_run
    type: boolean
    required: false
    default: false
    description: "If true, show what would happen without taking action"

  - name: include_my_mrs
    type: boolean
    required: false
    default: true
    description: "Show my own MRs that have feedback to respond to"

  - name: auto_rebase
    type: boolean
    required: false
    default: true
    description: "Automatically rebase my MRs that have merge conflicts"

  - name: slack_format
    type: boolean
    required: false
    default: false
    description: "Use Slack link format in summary"

# No hardcoded constants - resolved dynamically

steps:

  # ==================== PROACTIVE ISSUE DETECTION ====================

  - name: check_gitlab_known_issues
    description: "Check for known GitLab issues before starting"
    tool: check_known_issues
    args:
      tool_name: "gitlab_mr_list"
      error_text: ""
    output: gitlab_known_issues
    on_error: continue

  - name: init_autoheal
    description: "Initialize failure tracking"
    compute: |
      result = {"gitlab_failures": []}
    output: autoheal_state
    on_error: continue

  # ==================== KNOWLEDGE INTEGRATION ====================

  - name: get_review_patterns
    description: "Get code review patterns from knowledge"
    tool: knowledge_query
    args:
      project: "automation-analytics-backend"
      persona: "developer"
      section: "patterns.coding"
    output: review_patterns_raw
    on_error: continue

  - name: parse_review_patterns
    description: "Parse code review patterns"
    compute: |
      patterns_result = review_patterns_raw if 'review_patterns_raw' in dir() and review_patterns_raw else {}

      review_patterns = []
      if isinstance(patterns_result, dict) and patterns_result.get('found'):
          content = patterns_result.get('content', [])
          if isinstance(content, list):
              review_patterns = content[:5]

      result = {
          'patterns': review_patterns,
          'has_patterns': len(review_patterns) > 0,
      }
    output: code_review_patterns
    on_error: continue

  - name: get_project_gotchas
    description: "Get project gotchas for review context"
    tool: knowledge_query
    args:
      project: "automation-analytics-backend"
      persona: "developer"
      section: "gotchas"
    output: review_gotchas_raw
    on_error: continue

  - name: parse_review_gotchas
    description: "Parse review-relevant gotchas"
    compute: |
      gotchas_result = review_gotchas_raw if 'review_gotchas_raw' in dir() and review_gotchas_raw else {}

      review_gotchas = []
      if isinstance(gotchas_result, dict) and gotchas_result.get('found'):
          content = gotchas_result.get('content', [])
          if isinstance(content, list):
              review_gotchas = content[:5]

      result = {
          'gotchas': review_gotchas,
          'has_gotchas': len(review_gotchas) > 0,
      }
    output: project_review_gotchas
    on_error: continue

  # ==================== RESOLVE PROJECT ====================

  - name: resolve_project
    description: "Determine which GitLab project to check"
    compute: |
      import os
      from scripts.common.config_loader import load_config

      gitlab_project = None

      # Load config using shared loader
      config = load_config()
      repos = config.get("repositories", {})

      # Explicit project (inputs is a dict, use .get())
      if inputs.get("project"):
          gitlab_project = inputs.get("project")
      # Repo name from config
      elif inputs.get("repo_name") and inputs.get("repo_name") in repos:
          gitlab_project = repos[inputs.get("repo_name")].get("gitlab")
      # Fall back to cwd
      else:
          cwd = os.getcwd()
          for name, cfg in repos.items():
              if cfg.get("path") == cwd:
                  gitlab_project = cfg.get("gitlab")
                  break

      if not gitlab_project:
          gitlab_project = "automation-analytics/automation-analytics-backend"

      result = {"gitlab_project": gitlab_project}
    output: resolved
  # ==================== GET CURRENT USER ====================

  # Step 0: Get current system username
  - name: get_username
    description: "Get current system username to exclude own MRs"
    compute: |
      import getpass
      import os
      from scripts.common.config_loader import load_config

      # Get username from config or fallback to system
      cfg = load_config()

      # Use config if available, otherwise fallback to system user
      gitlab_username = cfg.get("user", {}).get("gitlab_username") or \
                        cfg.get("user", {}).get("username") or \
                        os.getenv('USER') or getpass.getuser()

      result = gitlab_username
    output: my_username

  # ==================== GATHER MRs ====================

  # Step 1: Get list of open MRs
  - name: list_open_mrs
    description: "Fetch all open MRs from GitLab"
    tool: gitlab_mr_list
    args:
      project: "{{ resolved.gitlab_project }}"
      state: opened
      per_page: "{{ inputs.limit }}"
    output: open_mrs
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Step 2: Parse MR list and separate own vs others
  - name: parse_mrs
    description: "Extract MR IDs, separate own MRs from others to review using shared parsers"
    compute: |
      from scripts.common.parsers import parse_mr_list, separate_mrs_by_author

      # Parse all MRs with author information
      all_mrs = parse_mr_list(open_mrs or "", include_author=True)

      # Separate into own vs others using shared function
      separated = separate_mrs_by_author(all_mrs, my_username)

      # inputs is a dict, use .get() with default
      limit = inputs.get("limit", 10)

      result = {
        'to_review': separated['to_review'][:limit],
        'my_mrs': separated['my_mrs'],
        'my_username': my_username
      }
    output: parsed_mrs

  # ==================== CHECK MY MRs FOR MERGE CONFLICTS ====================

  # Step 2b-prep: Extract my first MR info for tool calls
  - name: prep_my_first_mr
    description: "Extract my first MR info for subsequent tool calls"
    condition: "parsed_mrs.get('my_mrs') and len(parsed_mrs.get('my_mrs', [])) > 0 and inputs.get('include_my_mrs', True)"
    compute: |
      my_mrs = parsed_mrs.get('my_mrs', [])
      first_mr = my_mrs[0] if my_mrs else {}
      result = {
        'iid': first_mr.get('iid', 0),
        'title': first_mr.get('title', ''),
        'has_mr': len(my_mrs) > 0
      }
    output: my_first_mr_info

  # Step 2b: Get detailed status of first of my MRs to check for conflicts
  - name: check_my_mr_status
    description: "Check if any of my MRs have merge conflicts"
    condition: "my_first_mr_info and my_first_mr_info.get('iid', 0) > 0"
    tool: gitlab_mr_view
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ my_first_mr_info.iid }}"
    output: my_first_mr_details
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Step 2c: Detect if my MR has merge conflicts
  - name: detect_merge_conflicts
    description: "Check for merge conflicts or needs rebase using shared parser"
    condition: "my_first_mr_details and isinstance(my_first_mr_details, str)"
    compute: |
      from scripts.common.parsers import analyze_mr_status

      # Use my_first_mr_info which was already extracted
      mr_iid = my_first_mr_info.get('iid', 0) if isinstance(my_first_mr_info, dict) else 0
      mr_title = my_first_mr_info.get('title', '') if isinstance(my_first_mr_info, dict) else ''

      # Use shared MR status analyzer
      analysis = analyze_mr_status(my_first_mr_details or "")

      result = {
        'has_conflicts': analysis['has_conflicts'],
        'needs_rebase': analysis['needs_rebase'],
        'mr_id': mr_iid,
        'title': mr_title,
        'reason': analysis['action'] if analysis['needs_rebase'] else None
      }
    output: my_mr_conflict_status

  # Step 2d: Auto-rebase my MR if it has conflicts
  - name: auto_rebase_my_mr
    description: "Automatically rebase my MR if it has conflicts"
    condition: "my_mr_conflict_status and isinstance(my_mr_conflict_status, dict) and my_mr_conflict_status.get('needs_rebase') and inputs.get('auto_rebase', True) and not inputs.get('dry_run', False)"
    tool: skill_run
    args:
      skill_name: rebase_pr
      inputs: '{"mr_id": {{ my_mr_conflict_status.mr_id }}, "force_push": false}'
    output: rebase_result
    on_error: continue

  # Step 3: Process each MR (excluding my own)
  - name: process_mrs
    description: "Analyze each MR for review status"
    compute: |
      results = []
      mrs_to_review = parsed_mrs.get('to_review', [])

      for mr in mrs_to_review:
        mr_id = mr.get('iid')
        if not mr_id:
          continue

        mr_result = {
          'iid': mr_id,
          'title': mr.get('title', ''),
          'author': mr.get('author', 'unknown'),
          'action': 'pending',
          'reason': '',
          'my_last_feedback': None,
          'author_replied': False,
          'concerns_addressed': False
        }

        results.append(mr_result)

      result = results
    output: mr_analysis

  # Step 4: For each MR, get comments and determine status
  # Note: This is a simplified version - in practice you'd call gitlab_mr_comments for each
  - name: analyze_review_status
    description: "Determine review status for each MR"
    compute: |
      # This would ideally call gitlab_mr_comments for each MR
      # For now, we'll set up the structure and the actual tool calls happen below

      result = {
        'needs_review': [],
        'can_approve': [],
        'needs_followup': [],
        'skip': []
      }
    output: review_status

  # ==================== INDIVIDUAL MR PROCESSING ====================

  # Step 5a: Extract first MR info for tool calls
  - name: prep_first_mr
    description: "Extract first MR info for subsequent tool calls"
    condition: "len(mr_analysis) > 0"
    compute: |
      first_mr = mr_analysis[0] if mr_analysis else {}
      result = {
        'iid': first_mr.get('iid', 0),
        'title': first_mr.get('title', ''),
        'author': first_mr.get('author', 'unknown')
      }
    output: first_mr_info

  # Step 5b: Get comments for first pending MR
  # Note: This is a simplified approach - a full implementation would loop
  - name: get_mr_comments
    description: "Get comments for MRs to check review status"
    condition: "first_mr_info and first_mr_info.get('iid', 0) > 0"
    tool: gitlab_mr_view
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ first_mr_info.iid }}"
    output: first_mr_details
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Step 6: Analyze first MR's review status
  - name: analyze_first_mr
    description: "Check if first MR needs action using shared parsers"
    condition: "isinstance(mr_analysis, list) and len(mr_analysis) > 0 and 'first_mr_details' in dir() and first_mr_details and isinstance(first_mr_details, str)"
    compute: |
      import re
      from scripts.common.parsers import analyze_mr_status, analyze_review_status, extract_author_from_mr
      from scripts.common.config_loader import load_config

      mr = mr_analysis[0] if isinstance(mr_analysis, list) and mr_analysis else {}
      details = first_mr_details if isinstance(first_mr_details, str) else ""

      # Extract author from MR details (more reliable than list output)
      mr_author = extract_author_from_mr(details) or mr.get('author', '') if isinstance(mr, dict) else ''
      mr_author_lower = mr_author.lower() if mr_author else ''

      # Build set of my identities to check if this is my own MR
      my_identities = {my_username.lower()}
      try:
        cfg = load_config()
        user_cfg = cfg.get("user", {})
        for key in ["username", "gitlab_username", "jira_username"]:
          if user_cfg.get(key):
            my_identities.add(user_cfg[key].lower())
        if user_cfg.get("email"):
          my_identities.add(user_cfg["email"].lower())
          my_identities.add(user_cfg["email"].split("@")[0].lower())
        for alias in user_cfg.get("email_aliases", []):
          my_identities.add(alias.lower())
          my_identities.add(alias.split("@")[0].lower())
      except Exception:
        pass

      # Check if this is my own MR - skip if so
      is_my_mr = any(identity in mr_author_lower or mr_author_lower == identity for identity in my_identities)

      if is_my_mr:
        action = "skip"
        reason = "This is my own MR"
      else:
        # Get base analysis from shared parser
        analysis = analyze_mr_status(details, my_username)

        # Get review-specific analysis from shared parser
        review_status = analyze_review_status(details, my_username, mr_author)

        # Enhance with unresolved issues check
        if review_status['my_feedback_exists'] and review_status['author_replied']:
          if analysis['unresolved']:
            action = "needs_followup"
            reason = "Author replied but unresolved issues"
          else:
            action = "can_approve"
            reason = "Author addressed feedback"
        else:
          action = review_status['recommended_action']
          reason = review_status['reason']

      if isinstance(mr, dict):
        mr['action'] = action
        mr['reason'] = reason
        mr['author'] = mr_author  # Update with extracted author
        result = mr
      else:
        result = {'action': action, 'reason': reason, 'iid': 0, 'author': mr_author}
    output: first_mr_action

  # Step 7: Run full review if needed
  - name: run_full_review
    description: "Run full review for MR that needs it"
    condition: "first_mr_action and isinstance(first_mr_action, dict) and first_mr_action.get('action') == 'needs_full_review' and not inputs.get('dry_run', False)"
    tool: skill_run
    args:
      skill_name: review_pr
      inputs: '{"mr_id": {{ first_mr_info.iid }}, "skip_tests": true}'
    output: full_review_result
    on_error: continue

  # Step 8: Approve MR if author addressed feedback
  - name: auto_approve
    description: "Approve MR where author addressed feedback"
    condition: "first_mr_action and isinstance(first_mr_action, dict) and first_mr_action.get('action') == 'can_approve' and not inputs.get('dry_run', False)"
    tool: gitlab_mr_approve
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ first_mr_info.iid }}"
    output: approve_result
    on_error: continue

  # Step 9: Post follow-up if needed
  - name: post_followup
    description: "Post follow-up comment for unresolved issues"
    condition: "first_mr_action and isinstance(first_mr_action, dict) and first_mr_action.get('action') == 'needs_followup' and not inputs.get('dry_run', False)"
    tool: gitlab_mr_comment
    args:
      project: "{{ resolved.gitlab_project }}"
      mr_id: "{{ first_mr_info.iid }}"
      message: |
        ## Follow-up Review

        I see you've responded to my feedback. However, there appear to be unresolved discussions.

        Could you please address the outstanding items so we can move forward?

        ---
        *Automated follow-up by AI Workflow*
    output: followup_result
    on_error: continue

  # Step 10: Build summary
  - name: build_summary
    description: "Compile batch review results"
    compute: |
      from scripts.common.parsers import linkify_mr_ids, linkify_jira_keys
      is_slack = inputs.get('slack_format', True)

      # Safely get values, handling cases where vars might be error strings
      _parsed = parsed_mrs if isinstance(parsed_mrs, dict) else {}
      my_username = _parsed.get('my_username', 'unknown')
      my_mrs = _parsed.get('my_mrs', [])
      mrs_to_review = _parsed.get('to_review', [])

      _resolved = resolved if isinstance(resolved, dict) else {}
      gitlab_project = _resolved.get('gitlab_project', 'unknown')

      lines = ["## Batch PR Review Summary", ""]
      lines.append(f"**Project:** {gitlab_project}")
      lines.append(f"**Current User:** {my_username}")
      lines.append(f"**MRs to Review (by others):** {len(mrs_to_review)}")
      lines.append(f"**My Open MRs:** {len(my_mrs)}")
      lines.append(f"**Dry Run:** {'Yes' if inputs.get('dry_run', False) else 'No'}")
      lines.append("")

      # Categorize actions
      actions = {
        'can_approve': [],
        'needs_full_review': [],
        'needs_followup': [],
        'skip': []
      }

      # Only process first_mr_action if it's a dict (not an error string)
      _first_mr_action = first_mr_action if isinstance(first_mr_action, dict) else None
      if _first_mr_action:
        action = _first_mr_action.get('action', 'skip')
        actions.get(action, actions['skip']).append(_first_mr_action)

      # Summary by category
      if actions['can_approve']:
        lines.append("### ‚úÖ Approved")
        for mr in actions['can_approve']:
          status = "Would approve" if inputs.get('dry_run', False) else "Approved"
          mr_id = linkify_mr_ids(f"!{mr['iid']}", slack_format=is_slack)
          title = linkify_jira_keys(mr.get('title', ''), slack_format=is_slack)
          lines.append(f"- {mr_id}: {title} ({status})")
        lines.append("")

      if actions['needs_full_review']:
        lines.append("### üîç Full Review Performed")
        for mr in actions['needs_full_review']:
          mr_id = linkify_mr_ids(f"!{mr['iid']}", slack_format=is_slack)
          title = linkify_jira_keys(mr.get('title', ''), slack_format=is_slack)
          lines.append(f"- {mr_id}: {title}")
        lines.append("")

      if actions['needs_followup']:
        lines.append("### üí¨ Follow-up Posted")
        for mr in actions['needs_followup']:
          status = "Would post follow-up" if inputs.get('dry_run', False) else "Follow-up posted"
          mr_id = linkify_mr_ids(f"!{mr['iid']}", slack_format=is_slack)
          title = linkify_jira_keys(mr.get('title', ''), slack_format=is_slack)
          lines.append(f"- {mr_id}: {title} ({status})")
        lines.append("")

      if actions['skip']:
        lines.append("### ‚è≠Ô∏è Skipped (Waiting for Author)")
        for mr in actions['skip']:
          mr_id = linkify_mr_ids(f"!{mr['iid']}", slack_format=is_slack)
          title = linkify_jira_keys(mr.get('title', ''), slack_format=is_slack)
          lines.append(f"- {mr_id}: {title} - {mr.get('reason', 'No action needed')}")
        lines.append("")

      # Show my own MRs that might need attention
      _my_mr_status = my_mr_conflict_status if 'my_mr_conflict_status' in dir() and isinstance(my_mr_conflict_status, dict) else None
      if my_mrs and inputs.get('include_my_mrs', True):
        lines.append("---")
        lines.append("")
        lines.append("### üìù Your Open MRs")

        # Check if we rebased any MR
        if _my_mr_status and _my_mr_status.get('needs_rebase'):
          first_mr_id = _my_mr_status.get('mr_id')
          reason = _my_mr_status.get('reason', 'needs rebase')
          mr_id_str = linkify_mr_ids(f"!{first_mr_id}", slack_format=is_slack)
          title = linkify_jira_keys(_my_mr_status.get('title', ''), slack_format=is_slack)

          _rebase_result = rebase_result if 'rebase_result' in dir() else None
          if _rebase_result:
            lines.append(f"- {mr_id_str}: {title} üîÑ **Rebased!**")
            lines.append(f"  - {reason}")
            lines.append(f"  - *See rebase results below*")
          elif inputs.get('auto_rebase', True) and not inputs.get('dry_run', False):
            lines.append(f"- {mr_id_str}: {title} ‚ö†Ô∏è **Rebase attempted**")
          elif inputs.get('dry_run', False):
            lines.append(f"- {mr_id_str}: {title} üîÑ **Would rebase** (dry run)")
            lines.append(f"  - {reason}")
          else:
            lines.append(f"- {mr_id_str}: {title} ‚ö†Ô∏è **Needs rebase**")
            lines.append(f"  - {reason}")
            lines.append(f"  - Run: `skill_run(\"rebase_pr\", '{{\"mr_id\": {first_mr_id}}}')`")

        # Show remaining MRs
        for mr in my_mrs[:5]:
          if _my_mr_status and mr.get('iid') == _my_mr_status.get('mr_id'):
            continue  # Already shown above
          mr_id_str = linkify_mr_ids(f"!{mr.get('iid', '?')}", slack_format=is_slack)
          title = linkify_jira_keys(mr.get('title', ''), slack_format=is_slack)
          lines.append(f"- {mr_id_str}: {title}")

        lines.append("")
        lines.append("*Check these for feedback from reviewers. Use:*")
        lines.append(f"*`gitlab_mr_view(project=\"{gitlab_project}\", mr_id=<id>)` to see comments*")
        lines.append("")

      # Remaining MRs
      _mr_analysis = mr_analysis if isinstance(mr_analysis, list) else []
      remaining = len(_mr_analysis) - 1
      if remaining > 0:
        lines.append(f"---")
        lines.append(f"*{remaining} more MRs not processed in this batch.*")
        lines.append(f"*Run again to continue processing.*")

      result = '\n'.join(lines)
    output: batch_summary

  # Step 11: Emit batch review hook
  - name: emit_batch_review_hook
    description: "Notify team channel about batch review"
    condition: "not inputs.get('dry_run', False)"
    compute: |
      # emit_event is available from skill engine safe_globals
      # Safely handle cases where vars might be error strings
      _first_mr_action = first_mr_action if isinstance(first_mr_action, dict) else None
      _mr_analysis = mr_analysis if isinstance(mr_analysis, list) else []
      _resolved = resolved if isinstance(resolved, dict) else {}

      # Count actions taken
      approved = 1 if _first_mr_action and _first_mr_action.get('action') == 'can_approve' else 0
      changes = 1 if _first_mr_action and _first_mr_action.get('action') in ['needs_full_review', 'needs_followup'] else 0
      total = len(_mr_analysis)

      if emit_event and total > 0:
          emit_event("batch_review_completed", {
              "count": str(total),
              "approved": str(approved),
              "changes": str(changes),
              "project": _resolved.get("gitlab_project", ""),
          })
          result = "hook sent"
      elif total == 0:
          result = "no MRs to report"
      else:
          result = "hook skipped: emit_event not available"
    output: batch_hook_result
    on_error: continue

  # ==================== MEMORY INTEGRATION ====================

  - name: build_memory_context
    description: "Build context for memory updates"
    compute: |
      from datetime import datetime

      # Safely handle mr_analysis that might be an error string
      _mr_analysis = mr_analysis if isinstance(mr_analysis, list) else []

      # Count actions
      approved = 0
      feedback = 0
      total = len(_mr_analysis)

      for mr in _mr_analysis:
          action = mr.get("action", "") if isinstance(mr, dict) else ""
          if action == "can_approve":
              approved += 1
          elif action in ["needs_full_review", "needs_followup"]:
              feedback += 1

      result = {
          "timestamp": datetime.now().isoformat(),
          "total": total,
          "approved": approved,
          "feedback": feedback,
      }
    output: memory_context

  - name: log_batch_review
    description: "Log batch review to session"
    condition: "isinstance(memory_context, dict) and memory_context.get('total', 0) > 0"
    tool: memory_session_log
    args:
      action: "Batch reviewed {{ memory_context.total }} MRs"
      details: "Approved: {{ memory_context.approved }}, Feedback: {{ memory_context.feedback }}"
    on_error: continue

  - name: update_teammate_prefs
    description: "Update teammate preferences with review counts"
    condition: "isinstance(memory_context, dict) and memory_context.get('total', 0) > 0 and isinstance(mr_analysis, list)"
    compute: |
      # Use shared memory helpers
      data = memory.read_memory("learned/teammate_preferences")
      teammates = data.get("teammates", {}) if isinstance(data, dict) and isinstance(data.get("teammates"), dict) else {}

      # Safely handle mr_analysis
      _mr_analysis = mr_analysis if isinstance(mr_analysis, list) else []
      _memory_context = memory_context if isinstance(memory_context, dict) else {}

      # Track reviews given to each author
      for mr in _mr_analysis:
          if not isinstance(mr, dict):
              continue
          author = mr.get("author", "unknown")
          if author and author != "unknown":
              if author not in teammates:
                  teammates[author] = {"reviews_received": 0}
              teammates[author]["reviews_received"] = teammates[author].get("reviews_received", 0) + 1
              teammates[author]["last_reviewed_at"] = _memory_context.get("timestamp", "")

      if data is None:
          data = {}
      data["teammates"] = teammates
      memory.write_memory("learned/teammate_preferences", data)
      result = "updated"
    output: prefs_update_result
    on_error: continue

  # ==================== SEMANTIC SEARCH ====================

  - name: search_review_code
    description: "Search for code related to MRs being reviewed"
    condition: "first_mr_info and first_mr_info.get('iid', 0) > 0"
    tool: code_search
    args:
      query: "{{ first_mr_info.title if first_mr_info else '' }} code review"
      project: "automation-analytics-backend"
      limit: 3
    output: review_code_raw
    on_error: continue

  - name: parse_review_code
    description: "Parse review code search results"
    condition: "review_code_raw"
    compute: |
      code_result = review_code_raw if review_code_raw else {}

      related_code = []
      if isinstance(code_result, dict) and code_result.get('results'):
          for r in code_result.get('results', [])[:3]:
              related_code.append({
                  'file': r.get('file_path', ''),
                  'score': r.get('score', 0),
              })

      result = {
          'code': related_code,
          'count': len(related_code),
      }
    output: review_code_analysis
    on_error: continue

  # ==================== LEARNING FROM FAILURES ====================

  - name: detect_batch_review_failures
    description: "Detect failure patterns from batch PR review"
    compute: |
      errors_detected = []

      # Check GitLab API failures
      mrs_text = str(open_mrs) if 'open_mrs' in dir() and open_mrs else ""

      if "no such host" in mrs_text.lower() or "dial tcp" in mrs_text.lower():
          errors_detected.append({
              "tool": "gitlab_mr_list",
              "pattern": "no such host",
              "cause": "VPN not connected - internal GitLab not reachable",
              "fix": "Run vpn_connect() to connect to Red Hat VPN"
          })
      if "unauthorized" in mrs_text.lower() or "401" in mrs_text:
          errors_detected.append({
              "tool": "gitlab_mr_list",
              "pattern": "unauthorized",
              "cause": "GitLab authentication failed or token expired",
              "fix": "Check GitLab token in config.json"
          })

      result = errors_detected
    output: batch_review_errors_detected
    on_error: continue

  - name: learn_batch_review_vpn_failure
    description: "Learn from GitLab VPN failures"
    condition: "batch_review_errors_detected and any(e.get('pattern') == 'no such host' for e in batch_review_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "gitlab_mr_list"
      error_pattern: "no such host"
      root_cause: "VPN not connected - internal GitLab not reachable"
      fix_description: "Run vpn_connect() to connect to Red Hat VPN"
    output: batch_review_vpn_fix_learned
    on_error: continue

# ==================== OUTPUT ====================

outputs:
  - name: summary
    value: |
      {{ batch_summary }}

      ---

      ## Quick Actions

      - **Review next batch:** `skill_run("review_all_prs", '{}')`
      - **Review specific MR:** `skill_run("review_pr", '{"mr_id": 123}')`
      - **Approve specific MR:** `gitlab_mr_approve(project="...", mr_id=123)`

      {% if code_review_patterns and code_review_patterns.has_patterns %}
      ---

      ### üìã Code Review Patterns

      {% for pattern in code_review_patterns.patterns[:3] %}
      - {{ pattern }}
      {% endfor %}
      {% endif %}

      {% if project_review_gotchas and project_review_gotchas.has_gotchas %}
      ---

      ### ‚ö†Ô∏è Project Gotchas (Check in Reviews)

      {% for gotcha in project_review_gotchas.gotchas[:3] %}
      - {{ gotcha }}
      {% endfor %}
      {% endif %}

  - name: stats
    value:
      total_mrs: "{{ len(mr_analysis) }}"
      processed: 1
      dry_run: "{{ inputs.dry_run }}"
