# Skill: Investigate Slack Alert
# Automatically responds to Prometheus alerts in Slack channels
#
# Flow:
# 1. Parse the alert message from Slack
# 2. Immediately reply "Looking into this..."
# 3. Investigate pods, logs, and errors in the namespace
# 4. Search for existing Jira issues that match
# 5. Create new Jira if needed (with special format for billing alerts)
# 6. Reply with Jira link and summary

name: investigate_slack_alert
description: |
  Investigate Prometheus alerts from Slack and create/link Jira issues.
  
  ## Trigger
  Called when a message from app-sre-alerts is detected in alert channels:
  - C01CPSKFG0P (stage alerts)
  - C01L1K82AP5 (prod alerts)
  
  ## Behavior
  1. Immediately reply to acknowledge ("Looking into this...")
  2. Parse alert name, namespace, severity from Slack message
  3. Check pod status and logs for errors
  4. Search for existing Jira issues matching the alert
  5. If no match, create a new Jira issue
  6. For billing alerts: use special "BillingEvent XXXXX" format
  7. Reply with Jira link and investigation summary
  
  ## Billing Alert Special Handling
  Billing alerts (containing "billing", "subscription", "vcpu", etc.) get:
  - Higher priority
  - Special Jira format: "BillingEvent XXXXX - [Processor] Error: ..."
  - Numbered sequentially from existing billing events
version: "1.0"

inputs:
  - name: channel_id
    type: string
    required: true
    description: "Slack channel ID where alert was posted"
  
  - name: message_ts
    type: string
    required: true
    description: "Slack message timestamp (for threading replies)"
  
  - name: message_text
    type: string
    required: true
    description: "The alert message content (HTML or text)"
  
  - name: alert_url
    type: string
    required: false
    description: "URL to AlertManager (extracted from message)"

steps:
  # ==================== LOAD CONFIG ====================
  
  - name: load_config
    description: "Load alert channel configuration"
    compute: |
      from scripts.common.config_loader import load_config
      
      config = load_config()
      slack_config = config.get("slack", {}).get("listener", {})
      alert_channels = slack_config.get("alert_channels", {})
      jira_config = config.get("jira", {})
      namespaces = config.get("namespaces", {})
      
      # Get channel-specific config
      channel_info = alert_channels.get(inputs.channel_id, {
          "environment": "unknown",
          "namespace": namespaces.get("stage", {}).get("main", "tower-analytics-stage"),
          "cluster": "unknown",
          "severity": "medium"
      })
      
      result = {
          "channel_info": channel_info,
          "environment": channel_info.get("environment", "stage"),
          "namespace": channel_info.get("namespace", namespaces.get("stage", {}).get("main", "tower-analytics-stage")),
          "cluster": channel_info.get("cluster", "unknown"),
          "jira_config": jira_config,
          "namespaces": namespaces,
          "jira_url": jira_config.get("url", "https://issues.redhat.com"),
      }
    output: cfg

  # ==================== PARSE ALERT ====================
  
  - name: parse_alert_message
    description: "Extract alert details from Slack message using shared parser"
    compute: |
      from scripts.common.parsers import parse_prometheus_alert
      
      # Use shared parser for alert parsing
      parsed = parse_prometheus_alert(inputs.message_text or "")
      
      # Use namespace from parser or fall back to config
      namespace = parsed["namespace"] or cfg.get("namespace")
      
      result = {
          "alert_name": parsed["alert_name"],
          "firing_count": parsed["firing_count"],
          "description": parsed["description"],
          "links": parsed["links"],
          "is_billing": parsed["is_billing"],
          "namespace": namespace,
          "severity": "high" if parsed["is_billing"] or cfg.get("environment") == "production" else "medium"
      }
    output: alert_info

  # ==================== ACKNOWLEDGE ====================
  
  - name: acknowledge_alert
    description: "Reply to Slack thread to acknowledge we're looking into it"
    tool: slack_reply
    args:
      channel: "{{ inputs.channel_id }}"
      thread_ts: "{{ inputs.message_ts }}"
      text: "üëÄ Looking into this..."
    output: ack_result
    on_error: continue

  # ==================== INVESTIGATE ====================
  
  - name: get_kubeconfig
    description: "Determine correct kubeconfig for environment"
    compute: |
      import os
      
      env = cfg.get("environment", "stage")
      ns_config = cfg.get("namespaces", {})
      
      if env == "production":
          kubeconfig = ns_config.get("production", {}).get("kubeconfig", 
                       os.path.expanduser("~/.kube/config.p"))
      else:
          kubeconfig = ns_config.get("stage", {}).get("kubeconfig",
                       os.path.expanduser("~/.kube/config.s"))
      
      result = kubeconfig
    output: kubeconfig

  - name: check_pods
    description: "Get pod status in the affected namespace"
    tool: kubectl_get_pods
    args:
      namespace: "{{ alert_info.namespace }}"
      environment: "{{ cfg.environment }}"
    output: pod_status
    on_error: continue

  - name: analyze_pod_status
    description: "Identify unhealthy pods using shared parser"
    compute: |
      from scripts.common.parsers import parse_kubectl_pods
      
      # Use shared parser to get structured pod data
      pods = parse_kubectl_pods(str(pod_status) if pod_status else "")
      
      # Filter unhealthy pods
      unhealthy = []
      error_pods = []
      
      for pod in pods:
          if not pod.get("healthy"):
              unhealthy.append({
                  "pod": pod["name"],
                  "status": pod["status"],
                  "ready": pod.get("ready", "?/?")
              })
          
          # Find pods related to alert that have errors
          if any(term in pod["name"].lower() for term in ['processor', 'api', 'rollup', 'exporter']):
              if 'error' in pod["status"].lower() or 'crash' in pod["status"].lower():
                  error_pods.append(pod["name"])
      
      result = {
          "unhealthy_pods": unhealthy[:5],
          "error_pods": error_pods[:3],
          "total_issues": len(unhealthy)
      }
    output: pod_analysis

  - name: get_recent_errors
    description: "Get recent error logs from processor pods"
    condition: "{{ pod_analysis.get('error_pods') or alert_info.get('alert_name', '').lower().find('processor') >= 0 }}"
    tool: kubectl_logs
    args:
      namespace: "{{ alert_info.namespace }}"
      selector: "app=automation-analytics-processor-ingress"
      tail: 50
      environment: "{{ cfg.environment }}"
    output: recent_logs
    on_error: continue

  - name: extract_error_patterns
    description: "Extract error patterns from logs using shared parser"
    compute: |
      from scripts.common.parsers import parse_error_logs
      
      logs = str(recent_logs) if recent_logs else ""
      
      # Use shared parser for error extraction
      errors_found = parse_error_logs(logs, max_errors=5)
      
      result = {
          "errors": errors_found,
          "has_errors": len(errors_found) > 0
      }
    output: error_analysis

  # ==================== SEARCH JIRA ====================
  
  - name: search_existing_jira
    description: "Search for existing Jira issues matching this alert"
    tool: jira_search
    args:
      jql: "project = AAP AND summary ~ '{{ alert_info.alert_name }}' AND status NOT IN (Done, Closed) ORDER BY created DESC"
      max_results: 5
    output: existing_issues
    on_error: continue

  - name: search_billing_issues
    description: "Search for billing event issues if this is a billing alert"
    condition: "{{ alert_info.is_billing }}"
    tool: jira_search
    args:
      jql: "project = AAP AND summary ~ 'BillingEvent' ORDER BY created DESC"
      max_results: 10
    output: billing_issues
    on_error: continue

  - name: analyze_existing_issues
    description: "Determine if we have a matching issue or need to create one using shared parsers"
    compute: |
      from scripts.common.parsers import extract_all_jira_keys, extract_billing_event_number
      
      existing = str(existing_issues) if existing_issues else ""
      billing = str(billing_issues) if billing_issues else ""
      
      # Find all Jira keys in existing issues using shared parser
      issue_matches = extract_all_jira_keys(existing)
      matching_issue = issue_matches[0] if issue_matches else None
      
      # For billing alerts, get the next billing event number using shared parser
      next_billing_number = 1
      if alert_info.get("is_billing"):
          next_billing_number = extract_billing_event_number(billing)
      
      result = {
          "has_existing": matching_issue is not None,
          "existing_issue": matching_issue,
          "next_billing_number": next_billing_number,
          "all_matches": issue_matches[:3]
      }
    output: jira_analysis

  # ==================== CREATE OR LINK JIRA ====================
  
  - name: build_jira_summary
    description: "Build the Jira issue summary"
    condition: "{{ not jira_analysis.has_existing }}"
    compute: |
      alert_name = alert_info.get("alert_name", "Unknown Alert")
      description = alert_info.get("description", "")[:100]
      
      if alert_info.get("is_billing"):
          # Format: BillingEvent XXXXX - [Processor] Error: description
          number = jira_analysis.get("next_billing_number", 1)
          summary = f"BillingEvent {number:05d} - {alert_name}"
      else:
          summary = f"Alert: {alert_name}"
      
      result = summary[:250]  # Jira summary limit
    output: jira_summary

  - name: build_jira_description
    description: "Build the Jira issue description"
    condition: "{{ not jira_analysis.has_existing }}"
    compute: |
      import datetime
      
      alert = alert_info
      pods = pod_analysis
      errors = error_analysis
      
      # Build Slack link
      channel = inputs.channel_id
      ts = inputs.message_ts.replace(".", "")
      slack_link = f"https://redhat-internal.slack.com/archives/{channel}/p{ts}"
      
      description = "## Alert Details\n\n"
      description += f"**Alert:** {alert.get('alert_name', 'Unknown')}\n"
      description += f"**Environment:** {cfg.get('environment', 'unknown')}\n"
      description += f"**Namespace:** {alert.get('namespace', 'unknown')}\n"
      description += f"**Firing Count:** {alert.get('firing_count', 1)}\n"
      description += f"**Severity:** {alert.get('severity', 'medium')}\n"
      description += f"**Detected:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
      description += "## Links\n\n"
      description += f"- [Slack Thread]({slack_link})\n"
      
      # Add alert links
      links = alert.get('links', {})
      if links.get('grafana'):
          description += f"- [Grafana Dashboard]({links['grafana']})\n"
      if links.get('prometheus'):
          description += f"- [Prometheus Query]({links['prometheus']})\n"
      if links.get('alertmanager'):
          description += f"- [AlertManager]({links['alertmanager']})\n"
      if links.get('runbook'):
          description += f"- [Runbook]({links['runbook']})\n"
      
      # Add investigation findings
      if pods.get('unhealthy_pods'):
          description += "\n## Unhealthy Pods\n"
          for pod in pods['unhealthy_pods'][:5]:
              description += f"- {pod.get('pod')}: {pod.get('status', pod.get('ready', 'unknown'))}\n"
      
      if errors.get('errors'):
          description += "\n## Error Patterns Found\n"
          description += "```\n"
          for err in errors['errors'][:3]:
              description += f"{err[:150]}\n"
          description += "```\n"
      
      description += f"\n---\n*Auto-created by AA Workflow Agent*"
      
      result = description
    output: jira_description

  - name: create_jira_issue
    description: "Create a new Jira issue for this alert"
    condition: "{{ not jira_analysis.has_existing }}"
    tool: jira_create_issue
    args:
      project: "AAP"
      issue_type: "{{ 'Bug' if alert_info.is_billing else 'Story' }}"
      summary: "{{ jira_summary }}"
      description: "{{ jira_description }}"
      labels: "{{ ['billing', 'prometheus-alert', 'auto-created'] if alert_info.is_billing else ['prometheus-alert', 'auto-created'] }}"
      component: "Automation Analytics"
    output: created_issue
    on_error: continue

  - name: get_jira_key
    description: "Extract the Jira issue key using shared parser"
    compute: |
      from scripts.common.parsers import extract_jira_key
      
      if jira_analysis.get("has_existing"):
          issue_key = jira_analysis.get("existing_issue")
          action = "linked"
      else:
          created = str(created_issue) if created_issue else ""
          issue_key = extract_jira_key(created)
          action = "created"
      
      result = {
          "key": issue_key,
          "action": action,
          "url": f"{cfg.get('jira', {}).get('url', 'https://issues.redhat.com')}/browse/{issue_key}" if issue_key else None
      }
    output: jira_result

  # ==================== REPLY TO SLACK ====================
  
  - name: build_response
    description: "Build the Slack response with findings"
    compute: |
      alert = alert_info
      pods = pod_analysis
      errors = error_analysis
      jira = jira_result
      
      # Build response
      if alert.get("is_billing"):
          response = f"üî¥ **Billing Alert Investigation**\n\n"
      else:
          response = f"üîç **Alert Investigation**\n\n"
      
      response += f"**Alert:** {alert.get('alert_name', 'Unknown')}\n"
      response += f"**Environment:** {cfg.get('environment', 'unknown')}\n"
      response += f"**Namespace:** `{alert.get('namespace', 'unknown')}`\n\n"
      
      # Pod status
      if pods.get("unhealthy_pods"):
          response += f"‚ö†Ô∏è **{pods.get('total_issues', 0)} unhealthy pods detected**\n"
      else:
          response += "‚úÖ All pods appear healthy\n"
      
      # Errors
      if errors.get("has_errors"):
          response += f"‚ùå Found {len(errors.get('errors', []))} error pattern(s) in logs\n"
      
      response += "\n"
      
      # Jira link
      if jira.get("key"):
          response += f"üìã **Jira:** [{jira['key']}]({jira['url']}) ({jira['action']})\n"
      else:
          response += "‚ö†Ô∏è Could not create/find Jira issue\n"
      
      # Quick links
      links = alert.get('links', {})
      if links:
          response += "\n**Quick Links:**\n"
          if links.get('grafana'):
              response += f"‚Ä¢ [Dashboard]({links['grafana']})\n"
          if links.get('runbook'):
              response += f"‚Ä¢ [Runbook]({links['runbook']})\n"
      
      result = response
    output: slack_response

  - name: reply_with_findings
    description: "Reply to Slack with investigation findings"
    tool: slack_reply
    args:
      channel: "{{ inputs.channel_id }}"
      thread_ts: "{{ inputs.message_ts }}"
      text: "{{ slack_response }}"
    output: reply_result
    on_error: continue

outputs:
  - name: summary
    value: |
      ## üîç Alert Investigation Complete
      
      **Alert:** {{ alert_info.alert_name }}
      **Environment:** {{ cfg.environment }}
      **Is Billing:** {{ 'üî¥ Yes' if alert_info.is_billing else 'No' }}
      
      ### Findings
      - Unhealthy pods: {{ pod_analysis.total_issues }}
      - Error patterns: {{ 'Yes' if error_analysis.has_errors else 'None found' }}
      
      ### Jira
      {% if jira_result.key %}
      - **Issue:** [{{ jira_result.key }}]({{ jira_result.url }})
      - **Action:** {{ jira_result.action }}
      {% else %}
      - ‚ö†Ô∏è No Jira issue created/found
      {% endif %}
      
      ### Slack Response
      {{ 'Sent' if reply_result else 'Failed to send' }}
  
  - name: context
    value:
      alert_name: "{{ alert_info.alert_name }}"
      environment: "{{ cfg.environment }}"
      namespace: "{{ alert_info.namespace }}"
      is_billing: "{{ alert_info.is_billing }}"
      jira_key: "{{ jira_result.key }}"
      jira_action: "{{ jira_result.action }}"

