# Skill: Investigate Slack Alert
# Automatically responds to Prometheus alerts in Slack channels
#
# Flow:
# 1. Parse the alert message from Slack
# 2. Immediately reply "Looking into this..."
# 3. Investigate pods, logs, and errors in the namespace
# 4. Search for existing Jira issues that match
# 5. Create new Jira if needed (with special format for billing alerts)
# 6. Reply with Jira link and summary

name: investigate_slack_alert
description: |
  Investigate Prometheus alerts from Slack and create/link Jira issues.

  ## Trigger
  Called when a message from app-sre-alerts is detected in alert channels:
  - C01CPSKFG0P (stage alerts)
  - C01L1K82AP5 (prod alerts)

  ## Behavior
  1. Immediately reply to acknowledge ("Looking into this...")
  2. Parse alert name, namespace, severity from Slack message
  3. Check pod status and logs for errors
  4. Search for existing Jira issues matching the alert
  5. If no match, create a new Jira issue
  6. For billing alerts: use special "BillingEvent XXXXX" format
  7. Reply with Jira link and investigation summary

  ## Billing Alert Special Handling
  Billing alerts (containing "billing", "subscription", "vcpu", etc.) get:
  - Higher priority
  - Special Jira format: "BillingEvent XXXXX - [Processor] Error: ..."
  - Numbered sequentially from existing billing events
version: "1.0"

inputs:
  - name: channel_id
    type: string
    required: true
    description: "Slack channel ID where alert was posted"

  - name: message_ts
    type: string
    required: true
    description: "Slack message timestamp (for threading replies)"

  - name: message_text
    type: string
    required: true
    description: "The alert message content (HTML or text)"

  - name: alert_url
    type: string
    required: false
    description: "URL to AlertManager (extracted from message)"

steps:

  - name: init_autoheal
    description: "Initialize failure tracking"
    compute: |
      result = {"failures": []}
    output: autoheal_state
    on_error: continue

  # ==================== KNOWLEDGE INTEGRATION ====================

  - name: get_alert_patterns
    description: "Get alert investigation patterns from knowledge base"
    tool: knowledge_query
    args:
      project: "automation-analytics-backend"
      persona: "devops"
      query: "gotchas"
    output: alert_knowledge_raw
    on_error: continue

  - name: parse_alert_knowledge
    description: "Parse alert knowledge for investigation context"
    compute: |
      knowledge_text = str(alert_knowledge_raw) if alert_knowledge_raw else ""

      gotchas = []
      runbooks = []

      # Extract gotchas related to alerts/monitoring
      if knowledge_text:
          for line in knowledge_text.split("\n"):
              line_lower = line.lower()
              if any(kw in line_lower for kw in ["alert", "prometheus", "grafana", "pod", "crash", "oom", "memory", "cpu"]):
                  gotchas.append(line.strip()[:150])
              if any(kw in line_lower for kw in ["runbook", "procedure", "when", "if"]):
                  runbooks.append(line.strip()[:150])

      result = {
          "has_knowledge": len(gotchas) > 0 or len(runbooks) > 0,
          "gotchas": gotchas[:5],
          "runbooks": runbooks[:3],
      }
    output: alert_knowledge
    on_error: continue

  - name: check_alert_known_issues
    description: "Check for known alert patterns"
    compute: |
      # Check known issues for alerts
      alert_issues = memory.check_known_issues("alert", "") or {}
      prometheus_issues = memory.check_known_issues("prometheus", "") or {}

      all_issues = []
      for issues in [alert_issues, prometheus_issues]:
          if issues and issues.get("matches"):
              all_issues.extend(issues.get("matches", [])[:2])

      result = {
          "has_known_issues": len(all_issues) > 0,
          "issues": all_issues[:5],
      }
    output: alert_known_issues
    on_error: continue

  # ==================== LOAD CONFIG ====================

  - name: load_config
    description: "Load alert channel configuration"
    compute: |
      from scripts.common.config_loader import load_config

      config = load_config()
      slack_config = config.get("slack", {}).get("listener", {})
      alert_channels = slack_config.get("alert_channels", {})
      jira_config = config.get("jira", {})
      namespaces = config.get("namespaces", {})

      # Get channel-specific config
      channel_info = alert_channels.get(inputs.channel_id, {
          "environment": "unknown",
          "namespace": namespaces.get("stage", {}).get("main", "tower-analytics-stage"),
          "cluster": "unknown",
          "severity": "medium"
      })

      result = {
          "channel_info": channel_info,
          "environment": channel_info.get("environment", "stage"),
          "namespace": channel_info.get("namespace", namespaces.get("stage", {}).get("main", "tower-analytics-stage")),
          "cluster": channel_info.get("cluster", "unknown"),
          "jira_config": jira_config,
          "namespaces": namespaces,
          "jira_url": jira_config.get("url", "https://issues.redhat.com"),
      }
    output: cfg

  # ==================== PARSE ALERT ====================

  - name: parse_alert_message
    description: "Extract alert details from Slack message using shared parser"
    compute: |
      from scripts.common.parsers import parse_prometheus_alert

      # Use shared parser for alert parsing
      parsed = parse_prometheus_alert(inputs.message_text or "")

      # Use namespace from parser or fall back to config
      namespace = parsed["namespace"] or cfg.get("namespace")

      result = {
          "alert_name": parsed["alert_name"],
          "firing_count": parsed["firing_count"],
          "description": parsed["description"],
          "links": parsed["links"],
          "is_billing": parsed["is_billing"],
          "namespace": namespace,
          "severity": "high" if parsed["is_billing"] or cfg.get("environment") == "production" else "medium"
      }
    output: alert_info

  # ==================== ACKNOWLEDGE ====================

  - name: acknowledge_alert
    description: "Reply to Slack thread to acknowledge we're looking into it"
    tool: slack_send_message
    args:
      target: "{{ inputs.channel_id }}"
      thread_ts: "{{ inputs.message_ts }}"
      text: "üëÄ Looking into this..."
    output: ack_result
    on_error: continue

  # ==================== INVESTIGATE ====================

  - name: get_kubeconfig
    description: "Determine correct kubeconfig for environment"
    compute: |
      import os

      env = cfg.get("environment", "stage")
      ns_config = cfg.get("namespaces", {})

      if env == "production":
          kubeconfig = ns_config.get("production", {}).get("kubeconfig",
                       os.path.expanduser("~/.kube/config.p"))
      else:
          kubeconfig = ns_config.get("stage", {}).get("kubeconfig",
                       os.path.expanduser("~/.kube/config.s"))

      result = kubeconfig
    output: kubeconfig

  - name: check_pods
    description: "Get pod status in the affected namespace"
    tool: kubectl_get_pods
    args:
      namespace: "{{ alert_info.namespace }}"
      environment: "{{ cfg.environment }}"
    output: pod_status
    on_error: auto_heal  # K8s cluster - may need kube_login

  - name: analyze_pod_status
    description: "Identify unhealthy pods using shared parser"
    compute: |
      from scripts.common.parsers import parse_kubectl_pods

      # Check if kubectl_get_pods failed
      pod_status_str = str(pod_status) if pod_status else ""
      tool_failed = (
          not pod_status or
          "error" in pod_status_str.lower()[:200] or
          "forbidden" in pod_status_str.lower() or
          "unauthorized" in pod_status_str.lower() or
          "connection refused" in pod_status_str.lower()
      )

      if tool_failed:
          result = {
              "unhealthy_pods": [],
              "error_pods": [],
              "total_issues": 0,
              "tool_failed": True,
              "error_reason": pod_status_str[:300] if pod_status_str else "kubectl_get_pods returned no data"
          }
      else:
          # Use shared parser to get structured pod data
          pods = parse_kubectl_pods(pod_status_str)

          # Filter unhealthy pods
          unhealthy = []
          error_pods = []

          for pod in pods:
              if not pod.get("healthy"):
                  unhealthy.append({
                      "pod": pod["name"],
                      "status": pod["status"],
                      "ready": pod.get("ready", "?/?")
                  })

              # Find pods related to alert that have errors
              if any(term in pod["name"].lower() for term in ['processor', 'api', 'rollup', 'exporter']):
                  if 'error' in pod["status"].lower() or 'crash' in pod["status"].lower():
                      error_pods.append(pod["name"])

          result = {
              "unhealthy_pods": unhealthy[:5],
              "error_pods": error_pods[:3],
              "total_issues": len(unhealthy),
              "tool_failed": False
          }
    output: pod_analysis

  - name: get_recent_errors
    description: "Get recent error logs from processor pods"
    condition: "{{ pod_analysis.get('error_pods') or alert_info.get('alert_name', '').lower().find('processor') >= 0 }}"
    tool: kubectl_logs
    args:
      namespace: "{{ alert_info.namespace }}"
      selector: "app=automation-analytics-processor-ingress"
      tail: 50
      environment: "{{ cfg.environment }}"
    output: recent_logs
    on_error: auto_heal  # K8s cluster - may need kube_login

  - name: extract_error_patterns
    description: "Extract error patterns from logs using shared parser"
    compute: |
      from scripts.common.parsers import parse_error_logs

      logs = str(recent_logs) if recent_logs else ""

      # Check if kubectl_logs failed
      tool_failed = (
          not recent_logs or
          "error" in logs.lower()[:200] or
          "forbidden" in logs.lower() or
          "unauthorized" in logs.lower() or
          "no pod found" in logs.lower()
      )

      if tool_failed:
          result = {
              "errors": [],
              "has_errors": False,
              "tool_failed": True,
              "error_reason": logs[:300] if logs else "kubectl_logs returned no data"
          }
      else:
          # Use shared parser for error extraction
          errors_found = parse_error_logs(logs, max_errors=5)

          result = {
              "errors": errors_found,
              "has_errors": len(errors_found) > 0,
              "tool_failed": False
          }
    output: error_analysis

  - name: search_related_code
    description: "Find code related to the alert"
    condition: "alert_info and alert_info.get('alert_name')"
    tool: code_search
    args:
      query: "{{ alert_info.alert_name }} {{ alert_info.description[:50] if alert_info.description else '' }}"
      project: "automation-analytics-backend"
      limit: 5
      min_score: 0.5
    output: alert_code_search_raw
    on_error: continue

  - name: parse_alert_code_search
    description: "Parse code search results for alert"
    compute: |
      search_result = alert_code_search_raw if 'alert_code_search_raw' in dir() and alert_code_search_raw else {}

      related_code = []
      if isinstance(search_result, dict) and search_result.get('results'):
          for r in search_result.get('results', [])[:5]:
              related_code.append({
                  'file': r.get('file_path', 'unknown'),
                  'score': r.get('score', 0),
                  'preview': r.get('code_chunk', '')[:100] if r.get('code_chunk') else '',
              })

      result = {
          'code': related_code,
          'has_related_code': len(related_code) > 0,
          'count': len(related_code),
      }
    output: alert_related_code
    on_error: continue

  # ==================== SEARCH JIRA ====================

  - name: search_existing_jira
    description: "Search for existing Jira issues matching this alert"
    tool: jira_search
    args:
      jql: "project = AAP AND summary ~ '{{ alert_info.alert_name }}' AND status NOT IN (Done, Closed) ORDER BY created DESC"
      max_results: 5
    output: existing_issues
    on_error: auto_heal  # Jira API - may need auth refresh

  - name: search_billing_issues
    description: "Search for billing event issues if this is a billing alert"
    condition: "{{ alert_info.is_billing }}"
    tool: jira_search
    args:
      jql: "project = AAP AND summary ~ 'BillingEvent' ORDER BY created DESC"
      max_results: 10
    output: billing_issues
    on_error: auto_heal  # Jira API - may need auth refresh

  - name: analyze_existing_issues
    description: "Determine if we have a matching issue or need to create one using shared parsers"
    compute: |
      from scripts.common.parsers import extract_all_jira_keys, extract_billing_event_number

      existing = str(existing_issues) if existing_issues else ""
      billing = str(billing_issues) if billing_issues else ""

      # Find all Jira keys in existing issues using shared parser
      issue_matches = extract_all_jira_keys(existing)
      matching_issue = issue_matches[0] if issue_matches else None

      # For billing alerts, get the next billing event number using shared parser
      next_billing_number = 1
      if alert_info.get("is_billing"):
          next_billing_number = extract_billing_event_number(billing)

      result = {
          "has_existing": matching_issue is not None,
          "existing_issue": matching_issue,
          "next_billing_number": next_billing_number,
          "all_matches": issue_matches[:3]
      }
    output: jira_analysis

  # ==================== CREATE OR LINK JIRA ====================

  - name: build_jira_summary
    description: "Build the Jira issue summary"
    condition: "{{ not jira_analysis.has_existing }}"
    compute: |
      alert_name = alert_info.get("alert_name", "Unknown Alert")
      description = alert_info.get("description", "")[:100]

      if alert_info.get("is_billing"):
          # Format: BillingEvent XXXXX - [Processor] Error: description
          number = jira_analysis.get("next_billing_number", 1)
          summary = f"BillingEvent {number:05d} - {alert_name}"
      else:
          summary = f"Alert: {alert_name}"

      result = summary[:250]  # Jira summary limit
    output: jira_summary

  - name: build_jira_description
    description: "Build the Jira issue description"
    condition: "{{ not jira_analysis.has_existing }}"
    compute: |
      import datetime

      alert = alert_info
      pods = pod_analysis
      errors = error_analysis

      # Build Slack link
      channel = inputs.channel_id
      ts = inputs.message_ts.replace(".", "")
      slack_link = f"https://redhat-internal.slack.com/archives/{channel}/p{ts}"

      description = "## Alert Details\n\n"
      description += f"**Alert:** {alert.get('alert_name', 'Unknown')}\n"
      description += f"**Environment:** {cfg.get('environment', 'unknown')}\n"
      description += f"**Namespace:** {alert.get('namespace', 'unknown')}\n"
      description += f"**Firing Count:** {alert.get('firing_count', 1)}\n"
      description += f"**Severity:** {alert.get('severity', 'medium')}\n"
      description += f"**Detected:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
      description += "## Links\n\n"
      description += f"- [Slack Thread]({slack_link})\n"

      # Add alert links
      links = alert.get('links', {})
      if links.get('grafana'):
          description += f"- [Grafana Dashboard]({links['grafana']})\n"
      if links.get('prometheus'):
          description += f"- [Prometheus Query]({links['prometheus']})\n"
      if links.get('alertmanager'):
          description += f"- [AlertManager]({links['alertmanager']})\n"
      if links.get('runbook'):
          description += f"- [Runbook]({links['runbook']})\n"

      # Add investigation findings
      if pods.get('unhealthy_pods'):
          description += "\n## Unhealthy Pods\n"
          for pod in pods['unhealthy_pods'][:5]:
              description += f"- {pod.get('pod')}: {pod.get('status', pod.get('ready', 'unknown'))}\n"

      if errors.get('errors'):
          description += "\n## Error Patterns Found\n"
          description += "```\n"
          for err in errors['errors'][:3]:
              description += f"{err[:150]}\n"
          description += "```\n"

      description += f"\n---\n*Auto-created by AA Workflow Agent*"

      result = description
    output: jira_description

  - name: create_jira_issue
    description: "Create a new Jira issue for this alert"
    condition: "{{ not jira_analysis.has_existing }}"
    tool: jira_create_issue
    args:
      project: "AAP"
      issue_type: "{{ 'Bug' if alert_info.is_billing else 'Story' }}"
      summary: "{{ jira_summary }}"
      description: "{{ jira_description }}"
      labels: "{{ ['billing', 'prometheus-alert', 'auto-created'] if alert_info.is_billing else ['prometheus-alert', 'auto-created'] }}"
      component: "Automation Analytics"
    output: created_issue
    on_error: continue

  - name: get_jira_key
    description: "Extract the Jira issue key using shared parser"
    compute: |
      from scripts.common.parsers import extract_jira_key

      if jira_analysis.get("has_existing"):
          issue_key = jira_analysis.get("existing_issue")
          action = "linked"
      else:
          created = str(created_issue) if created_issue else ""
          issue_key = extract_jira_key(created)
          action = "created"

      result = {
          "key": issue_key,
          "action": action,
          "url": f"{cfg.get('jira', {}).get('url', 'https://issues.redhat.com')}/browse/{issue_key}" if issue_key else None
      }
    output: jira_result

  # ==================== REPLY TO SLACK ====================

  - name: build_response
    description: "Build the Slack response with findings"
    compute: |
      alert = alert_info
      pods = pod_analysis
      errors = error_analysis
      jira = jira_result

      # Build response
      if alert.get("is_billing"):
          response = f"üî¥ **Billing Alert Investigation**\n\n"
      else:
          response = f"üîç **Alert Investigation**\n\n"

      response += f"**Alert:** {alert.get('alert_name', 'Unknown')}\n"
      response += f"**Environment:** {cfg.get('environment', 'unknown')}\n"
      response += f"**Namespace:** `{alert.get('namespace', 'unknown')}`\n\n"

      # Pod status
      if pods.get("tool_failed"):
          response += f"‚ö†Ô∏è **Unable to check pod status**\n"
          response += f"Error: {pods.get('error_reason', 'kubectl_get_pods failed')[:150]}\n"
          response += f"\nCommon causes:\n"
          response += f"- VPN disconnected\n"
          response += f"- Kubeconfig expired (run `kube_login {cfg.get('environment', 'stage')}`)\n"
          response += f"- Wrong namespace\n"
      elif pods.get("unhealthy_pods"):
          response += f"‚ö†Ô∏è **{pods.get('total_issues', 0)} unhealthy pods detected**\n"
      else:
          response += "‚úÖ All pods appear healthy\n"

      # Errors
      if errors.get("tool_failed"):
          response += f"‚ö†Ô∏è **Unable to get pod logs**\n"
          response += f"Error: {errors.get('error_reason', 'kubectl_logs failed')[:150]}\n"
      elif errors.get("has_errors"):
          response += f"‚ùå Found {len(errors.get('errors', []))} error pattern(s) in logs\n"

      response += "\n"

      # Jira link
      if jira.get("key"):
          response += f"üìã **Jira:** [{jira['key']}]({jira['url']}) ({jira['action']})\n"
      else:
          response += "‚ö†Ô∏è Could not create/find Jira issue\n"

      # Quick links
      links = alert.get('links', {})
      if links:
          response += "\n**Quick Links:**\n"
          if links.get('grafana'):
              response += f"‚Ä¢ [Dashboard]({links['grafana']})\n"
          if links.get('runbook'):
              response += f"‚Ä¢ [Runbook]({links['runbook']})\n"

      result = response
    output: slack_response

  - name: reply_with_findings
    description: "Reply to Slack with investigation findings"
    tool: slack_send_message
    args:
      target: "{{ inputs.channel_id }}"
      thread_ts: "{{ inputs.message_ts }}"
      text: "{{ slack_response }}"
    output: reply_result
    on_error: continue

  # ==================== MEMORY INTEGRATION ====================

  - name: update_environment_status
    description: "Update environment status after investigation"
    compute: |
      # Use shared memory helpers
      env_key = cfg.get("environment", "production")
      total_issues = pod_analysis.get("total_issues", 0) if pod_analysis else 0
      status = "issues" if total_issues > 0 else "healthy"

      memory.update_field(
          "state/environments",
          f"environments.{env_key}.status",
          status
      )
      memory.update_field(
          "state/environments",
          f"environments.{env_key}.last_check",
          memory.get_timestamp()
      )
      memory.update_field(
          "state/environments",
          f"environments.{env_key}.notes",
          f"Slack alert: {alert_info.get('alert_name', 'unknown')}"
      )
      result = "environment updated"
    output: env_update_result
    on_error: continue

  - name: log_slack_investigation
    description: "Log Slack alert investigation to session"
    tool: memory_session_log
    args:
      action: "Investigated Slack alert: {{ alert_info.alert_name if alert_info else 'unknown' }}"
      details: "Environment: {{ cfg.environment }}, Issues: {{ pod_analysis.total_issues if pod_analysis else 0 }}"
    on_error: continue

  # ==================== LEARNING FROM FAILURES ====================

  - name: detect_slack_alert_failures
    description: "Detect failure patterns from Slack alert investigation"
    compute: |
      errors_detected = []

      # Check Kubernetes failures
      pods_text = str(pods_raw) if 'pods_raw' in dir() and pods_raw else ""
      if "forbidden" in pods_text.lower() or "unauthorized" in pods_text.lower():
          errors_detected.append({
              "tool": "kubectl_get_pods",
              "pattern": "forbidden",
              "cause": "Kubernetes auth expired or insufficient permissions",
              "fix": "Run kube_login('stage') or kube_login('prod')"
          })
      if "no route to host" in pods_text.lower():
          errors_detected.append({
              "tool": "kubectl_get_pods",
              "pattern": "no route to host",
              "cause": "VPN not connected - cluster not reachable",
              "fix": "Run vpn_connect()"
          })

      # Check Kibana failures
      logs_text = str(logs_raw) if 'logs_raw' in dir() and logs_raw else ""
      if "unauthorized" in logs_text.lower() or "authentication required" in logs_text.lower():
          errors_detected.append({
              "tool": "kibana_search_logs",
              "pattern": "authentication required",
              "cause": "Kibana session expired",
              "fix": "Open Kibana in browser to refresh session"
          })

      # Check Jira failures
      jira_text = str(jira_search_raw) if 'jira_search_raw' in dir() and jira_search_raw else ""
      if "unauthorized" in jira_text.lower():
          errors_detected.append({
              "tool": "jira_search",
              "pattern": "unauthorized",
              "cause": "Jira authentication failed",
              "fix": "Check Jira credentials in config.json"
          })

      # Check Slack failures
      slack_text = str(reply_result) if 'reply_result' in dir() and reply_result else ""
      if "not_in_channel" in slack_text.lower() or "channel_not_found" in slack_text.lower():
          errors_detected.append({
              "tool": "slack_reply",
              "pattern": "not_in_channel",
              "cause": "Bot not added to Slack channel",
              "fix": "Invite the bot to the alert channel"
          })

      result = errors_detected
    output: slack_alert_errors_detected
    on_error: continue

  - name: learn_slack_alert_k8s_failure
    description: "Learn from Kubernetes auth failures"
    condition: "slack_alert_errors_detected and any(e.get('pattern') == 'forbidden' for e in slack_alert_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "kubectl_get_pods"
      error_pattern: "forbidden"
      root_cause: "Kubernetes auth expired or insufficient permissions"
      fix_description: "Run kube_login('stage') or kube_login('prod')"
    output: slack_alert_k8s_fix_learned
    on_error: continue

  - name: learn_slack_alert_vpn_failure
    description: "Learn from VPN failures"
    condition: "slack_alert_errors_detected and any(e.get('pattern') == 'no route to host' for e in slack_alert_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "kubectl_get_pods"
      error_pattern: "no route to host"
      root_cause: "VPN not connected - cluster not reachable"
      fix_description: "Run vpn_connect()"
    output: slack_alert_vpn_fix_learned
    on_error: continue

  - name: learn_slack_alert_kibana_failure
    description: "Learn from Kibana auth failures"
    condition: "slack_alert_errors_detected and any(e.get('pattern') == 'authentication required' for e in slack_alert_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "kibana_search_logs"
      error_pattern: "authentication required"
      root_cause: "Kibana session expired"
      fix_description: "Open Kibana in browser to refresh session"
    output: slack_alert_kibana_fix_learned
    on_error: continue

outputs:
  - name: summary
    value: |
      ## üîç Alert Investigation Complete

      **Alert:** {{ alert_info.alert_name }}
      **Environment:** {{ cfg.environment }}
      **Is Billing:** {{ 'üî¥ Yes' if alert_info.is_billing else 'No' }}

      ### Findings
      - Unhealthy pods: {{ pod_analysis.total_issues }}
      - Error patterns: {{ 'Yes' if error_analysis.has_errors else 'None found' }}

      ### Jira
      {% if jira_result.key %}
      - **Issue:** [{{ jira_result.key }}]({{ jira_result.url }})
      - **Action:** {{ jira_result.action }}
      {% else %}
      - ‚ö†Ô∏è No Jira issue created/found
      {% endif %}

      {% if alert_related_code and alert_related_code.has_related_code %}
      ### üîç Related Code

      {% for code in alert_related_code.code[:3] %}
      - `{{ code.file }}` ({{ "%.0f"|format(code.score * 100) }}% match)
      {% endfor %}
      {% endif %}

      {% if alert_known_issues and alert_known_issues.has_known_issues %}
      ### üí° Known Issues

      {% for issue in alert_known_issues.issues[:3] %}
      - {{ issue.pattern if issue.pattern else issue }}
      {% endfor %}
      {% endif %}

      ### Slack Response
      {{ 'Sent' if reply_result else 'Failed to send' }}

  - name: context
    value:
      alert_name: "{{ alert_info.alert_name }}"
      environment: "{{ cfg.environment }}"
      namespace: "{{ alert_info.namespace }}"
      is_billing: "{{ alert_info.is_billing }}"
      jira_key: "{{ jira_result.key }}"
      jira_action: "{{ jira_result.action }}"
