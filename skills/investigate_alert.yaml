# Skill: Investigate Alert
# Quick triage of a firing alert, escalates to debug_prod for deeper investigation

name: investigate_alert
description: |
  Quick investigation of a firing Prometheus alert.

  Steps:
  1. Get current firing alerts
  2. Quick health check (pods, deployments)
  3. Check recent events
  4. Look for known patterns
  5. Escalate to debug_prod if serious

  Use this for quick triage. For deep investigation, use debug_prod directly.

  Resolves namespaces and paths from config.json.
version: "2.1"

inputs:
  - name: environment
    type: string
    required: true
    enum: ["stage", "production", "prod"]
    description: "Environment to investigate"

  - name: namespace
    type: string
    required: false
    description: "Namespace: 'main' or 'billing' (defaults to main)"

  - name: alert_name
    type: string
    required: false
    description: "Specific alert to investigate"

  - name: auto_escalate
    type: boolean
    required: false
    default: true
    description: "Auto-run debug_prod if critical issues found"

# No hardcoded constants - resolved dynamically from config.json

steps:
  # ==================== LOAD CONFIG ====================

  - name: load_config
    description: "Load namespace configuration"
    compute: |
      from scripts.common.config_loader import load_config

      config = load_config()
      ns_config = config.get("namespaces", {})
      ai_config = config.get("app_interface", {})

      result = {
          "namespace_stage": ns_config.get("stage", {}).get("main", "tower-analytics-stage"),
          "namespace_prod_main": ns_config.get("production", {}).get("main", "tower-analytics-prod"),
          "namespace_prod_billing": ns_config.get("production", {}).get("billing", "tower-analytics-prod-billing"),
          "alerts_stage": ai_config.get("alerts", {}).get("stage", "resources/insights-stage/tower-analytics-stage"),
          "alerts_prod_main": ai_config.get("alerts", {}).get("prod_main", "resources/insights-prod/tower-analytics-prod"),
          "alerts_prod_billing": ai_config.get("alerts", {}).get("prod_billing", "resources/insights-prod/tower-analytics-prod-billing"),
          "jira_url": config.get("jira", {}).get("url", "https://issues.redhat.com"),
      }
    output: cfg
  # ==================== RESOLVE NAMESPACE ====================

  - name: resolve_namespace
    description: "Determine the target namespace"
    compute: |
      env = inputs.environment.lower()
      ns_input = (inputs.namespace or "main").lower()

      if env == "stage":
          namespace = cfg["namespace_stage"]
          env_label = "stage"
      elif "billing" in ns_input:
          namespace = cfg["namespace_prod_billing"]
          env_label = "production"
      else:
          namespace = cfg["namespace_prod_main"]
          env_label = "production"

      result = {
          "namespace": namespace,
          "environment": env_label,
          "is_prod": env_label == "production",
      }
    output: target

  # ==================== GET ALERTS ====================

  - name: get_firing_alerts
    description: "Get currently firing alerts"
    tool: prometheus_alerts
    args:
      environment: "{{ target.environment }}"
      namespace: "{{ target.namespace }}"
      state: firing
    output: alerts_raw
    on_error: continue

  - name: parse_alerts
    description: "Parse alert data using shared parser"
    compute: |
      from scripts.common.parsers import parse_alertmanager_output

      # Use shared parser for alert data
      alerts = parse_alertmanager_output(str(alerts_raw) if alerts_raw else "")

      # Check if specific alert is in the list
      if inputs.alert_name:
          filtered = [a for a in alerts if inputs.alert_name.lower() in a.get("name", "").lower()]
          alerts = filtered if filtered else alerts

      has_critical = any(a.get("severity") == "critical" for a in alerts)
      has_high = any(a.get("severity") in ["high", "warning"] for a in alerts)

      result = {
          "alerts": alerts[:10],
          "count": len(alerts),
          "has_critical": has_critical,
          "has_high": has_high,
          "max_severity": "critical" if has_critical else ("high" if has_high else "low"),
      }
    output: alert_info

  # ==================== QUICK HEALTH CHECK ====================

  - name: get_pod_status
    description: "Get pod status"
    tool: kubectl_get_pods
    args:
      namespace: "{{ target.namespace }}"
      environment: "{{ target.environment }}"
    output: pods_raw
    on_error: continue

  - name: parse_pod_health
    description: "Parse pod health"
    compute: |
      # parsers is available from skill engine safe_globals
      # Use shared parser
      pods = parsers.parse_kubectl_pods(str(pods_raw) if pods_raw else "") if parsers else []

      # Count health states
      running = sum(1 for p in pods if p.get("status") == "Running")
      not_running = sum(1 for p in pods if p.get("status") in ["CrashLoopBackOff", "Error", "Pending", "ImagePullBackOff"])
      high_restarts = sum(1 for p in pods if int(str(p.get("restarts", "0")).split()[0]) > 5)

      healthy = not_running == 0 and high_restarts == 0

      issues = []
      if not_running > 0:
          issues.append(f"{not_running} pod(s) not running")
      if high_restarts > 0:
          issues.append(f"{high_restarts} pod(s) with high restarts")

      result = {
          "running": running,
          "not_running": not_running,
          "high_restarts": high_restarts,
          "healthy": healthy,
          "issues": issues,
      }
    output: pod_health

  # ==================== GET RECENT EVENTS ====================

  - name: get_events
    description: "Get recent warning events"
    tool: kubectl_get_events
    args:
      namespace: "{{ target.namespace }}"
      environment: "{{ target.environment }}"
      field_selector: "type=Warning"
    output: events_raw
    on_error: continue

  - name: parse_events
    description: "Parse events"
    compute: |
      events = []
      if events_raw:
          lines = str(events_raw).split("\n")
          for line in lines[:10]:
              if line.strip():
                  events.append(line.strip()[:100])

      result = events[:5]
    output: recent_events

  # ==================== CHECK KNOWN PATTERNS ====================

  - name: check_patterns
    description: "Match against known error patterns"
    compute: |
      # Use shared memory helpers
      from scripts.common.config_loader import load_config

      data = memory.read_memory("learned/patterns")
      known_patterns = data.get("error_patterns", []) if isinstance(data.get("error_patterns"), list) else []
      matches = []

      # Check alerts against patterns
      for alert in alert_info.get("alerts", []):
          alert_name = alert.get("name", "").lower()
          for pattern in known_patterns:
              if pattern.get("pattern", "").lower() in alert_name:
                  matches.append({
                      "pattern": pattern.get("pattern"),
                      "cause": pattern.get("cause"),
                      "fix": pattern.get("fix"),
                  })

      # Check pod issues against patterns
      for issue in pod_health.get("issues", []):
          issue_lower = issue.lower()
          for pattern in known_patterns:
              if pattern.get("pattern", "").lower() in issue_lower:
                  matches.append({
                      "pattern": pattern.get("pattern"),
                      "cause": pattern.get("cause"),
                      "fix": pattern.get("fix"),
                  })

      result = matches[:5]
    output: pattern_matches

  # ==================== DETERMINE SEVERITY ====================

  - name: assess_severity
    description: "Determine overall severity"
    compute: |
      severity = "low"
      needs_escalation = False

      if alert_info.get("has_critical"):
          severity = "critical"
          needs_escalation = True
      elif not pod_health.get("healthy"):
          severity = "high"
          needs_escalation = True
      elif alert_info.get("has_high"):
          severity = "medium"
      elif len(recent_events) > 3:
          severity = "medium"

      result = {
          "severity": severity,
          "needs_escalation": needs_escalation and inputs.auto_escalate,
          "reason": "Critical alerts or unhealthy pods" if needs_escalation else "Routine check",
      }
    output: assessment

  # ==================== ESCALATE TO DEBUG_PROD ====================

  - name: escalate_to_debug_prod
    description: "Run full debug_prod for serious issues"
    condition: "assessment.needs_escalation and target.is_prod"
    tool: skill_run
    args:
      skill_name: debug_prod
      inputs: |
        {
          "namespace": "{{ 'billing' if 'billing' in target.namespace else 'main' }}",
          "alert_name": "{{ inputs.alert_name or '' }}",
          "time_range": "1h"
        }
    output: debug_result
    on_error: continue

  # Emit alert hooks
  - name: emit_alert_hooks
    description: "Notify team channel about critical alerts"
    compute: |
      # emit_event is available from skill engine safe_globals
      if emit_event:
          # Emit critical alert if applicable
          if assessment.get("severity") == "critical":
              alert_name = inputs.alert_name or "unknown"
              if alert_info.get("alerts"):
                  alert_name = alert_info["alerts"][0].get("name", alert_name)

              emit_event("alert_critical", {
                  "alert_name": alert_name,
                  "namespace": target.get("namespace", ""),
                  "environment": target.get("environment", ""),
              })
              result = "critical hook sent"

          # Emit escalated if we ran debug_prod
          elif assessment.get("needs_escalation") and debug_result:
              alert_name = inputs.alert_name or "unknown"
              emit_event("alert_escalated", {
                  "alert_name": alert_name,
                  "namespace": target.get("namespace", ""),
              })
              result = "escalated hook sent"
          else:
              result = "no hook needed"
      else:
          result = "hook skipped: emit_event not available"
    output: alert_hook_result
    on_error: continue

  # ==================== MEMORY INTEGRATION ====================

  - name: build_memory_context
    description: "Build context for memory updates"
    compute: |
      from datetime import datetime

      alert_names = [a.get("name", "unknown") for a in alert_info.get("alerts", [])[:3]]

      result = {
          "timestamp": datetime.now().isoformat(),
          "alert_summary": ", ".join(alert_names) if alert_names else "No alerts",
      }
    output: memory_context

  - name: log_session_investigation
    description: "Log alert investigation to session"
    tool: memory_session_log
    args:
      action: "Investigated alerts in {{ target.environment }}/{{ target.namespace }}"
      details: "Severity: {{ assessment.severity }}, Alerts: {{ memory_context.alert_summary }}"
    on_error: continue

  - name: update_environment_status
    description: "Update environment status in memory"
    compute: |
      # Use shared memory helpers
      data = memory.read_memory("state/environments")
      if "environments" not in data:
          data["environments"] = {}

      env_key = "stage" if target.get("environment") == "stage" else "production"
      if env_key not in data["environments"]:
          data["environments"][env_key] = {}

      env_data = data["environments"][env_key]
      env_data["status"] = "healthy" if pod_health.get("healthy") and alert_info.get("count", 0) == 0 else "issues"
      env_data["last_check"] = memory_context["timestamp"]
      env_data["alerts"] = [a.get("name", "") for a in alert_info.get("alerts", [])[:5]]

      # Add known issues if critical
      if assessment.get("severity") == "critical":
          known = env_data.get("known_issues", [])
          for alert in alert_info.get("alerts", []):
              if alert.get("severity") == "critical":
                  issue = {
                      "alert": alert.get("name", ""),
                      "since": memory_context["timestamp"],
                      "status": "investigating"
                  }
                  if issue not in known:
                      known.append(issue)
          env_data["known_issues"] = known[:10]

      memory.write_memory("state/environments", data)
      result = "environment status updated"
    output: env_update_result
    on_error: continue

# ==================== OUTPUT ====================

outputs:
  - name: report
    value: |
      ## üîç Alert Investigation: {{ target.namespace }}

      **Environment:** {{ target.environment }}
      **Severity:** {{ "üî¥ CRITICAL" if assessment.severity == "critical" else ("üü† HIGH" if assessment.severity == "high" else ("üü° MEDIUM" if assessment.severity == "medium" else "üü¢ LOW")) }}

      ---

      ### üö® Alerts ({{ alert_info.count }})
      {% if alert_info.count == 0 %}
      ‚úÖ No alerts currently firing
      {% else %}
      {% for alert in alert_info.alerts[:5] %}
      - {{ "üî¥" if alert.severity == "critical" else "üü†" }} **{{ alert.name }}** ({{ alert.severity }})
        {{ alert.message if alert.message else "" }}
      {% endfor %}
      {% endif %}

      ### üè• Pod Health
      {{ "‚úÖ Healthy" if pod_health.healthy else "‚ö†Ô∏è Issues Detected" }}
      - Running: {{ pod_health.running }} pods
      {% if pod_health.issues %}
      - Issues:
      {% for issue in pod_health.issues %}
        - {{ issue }}
      {% endfor %}
      {% endif %}

      ### ‚ö†Ô∏è Recent Events
      {% if recent_events %}
      {% for event in recent_events[:3] %}
      - {{ event }}
      {% endfor %}
      {% else %}
      No warning events
      {% endif %}

      {% if pattern_matches %}
      ### üí° Known Patterns Matched
      {% for match in pattern_matches %}
      - **{{ match.pattern }}**
        - Cause: {{ match.cause }}
        - Fix: {{ match.fix }}
      {% endfor %}
      {% endif %}

      ---

      {% if debug_result %}
      ### üî¨ Deep Investigation Results

      Escalated to `debug_prod`. See detailed results above.
      {% elif assessment.needs_escalation %}
      ### üî¨ Recommended: Deep Investigation

      Run: `skill_run("debug_prod", '{"namespace": "{{ 'billing' if 'billing' in target.namespace else 'main' }}"}')`
      {% else %}
      ### ‚úÖ Quick Actions

      - View pod logs: `kubectl_logs(namespace="{{ target.namespace }}", pod="<pod-name>")`
      - Get more details: `skill_run("debug_prod", '{"namespace": "{{ 'billing' if 'billing' in target.namespace else 'main' }}"}')`
      - Check Grafana dashboards
      {% endif %}

  - name: severity
    value: "{{ assessment.severity }}"

  - name: context
    value:
      namespace: "{{ target.namespace }}"
      environment: "{{ target.environment }}"
      alert_count: "{{ alert_info.count }}"
      healthy: "{{ pod_health.healthy }}"
      escalated: "{{ debug_result is defined }}"
