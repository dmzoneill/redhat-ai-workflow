# Skill: Investigate Alert
# Quick triage of a firing alert, escalates to debug_prod for deeper investigation

name: investigate_alert
description: |
  Quick investigation of a firing Prometheus alert.

  Steps:
  1. Get current firing alerts
  2. Quick health check (pods, deployments)
  3. Check recent events
  4. Look for known patterns
  5. Escalate to debug_prod if serious

  Use this for quick triage. For deep investigation, use debug_prod directly.

  Resolves namespaces and paths from config.json.
version: "2.2"

inputs:
  - name: environment
    type: string
    required: true
    enum: ["stage", "production", "prod"]
    description: "Environment to investigate"

  - name: namespace
    type: string
    required: false
    description: "Namespace: 'main' or 'billing' (defaults to main)"

  - name: alert_name
    type: string
    required: false
    description: "Specific alert to investigate"

  - name: auto_escalate
    type: boolean
    required: false
    default: true
    description: "Auto-run debug_prod if critical issues found"

# No hardcoded constants - resolved dynamically from config.json

steps:
  # ==================== LOAD CONFIG ====================

  - name: load_config
    description: "Load namespace configuration"
    compute: |
      from scripts.common.config_loader import load_config

      config = load_config()
      ns_config = config.get("namespaces", {})
      ai_config = config.get("app_interface", {})

      result = {
          "namespace_stage": ns_config.get("stage", {}).get("main", "tower-analytics-stage"),
          "namespace_prod_main": ns_config.get("production", {}).get("main", "tower-analytics-prod"),
          "namespace_prod_billing": ns_config.get("production", {}).get("billing", "tower-analytics-prod-billing"),
          "alerts_stage": ai_config.get("alerts", {}).get("stage", "resources/insights-stage/tower-analytics-stage"),
          "alerts_prod_main": ai_config.get("alerts", {}).get("prod_main", "resources/insights-prod/tower-analytics-prod"),
          "alerts_prod_billing": ai_config.get("alerts", {}).get("prod_billing", "resources/insights-prod/tower-analytics-prod-billing"),
          "jira_url": config.get("jira", {}).get("url", "https://issues.redhat.com"),
      }
    output: cfg
  # ==================== RESOLVE NAMESPACE ====================

  - name: resolve_namespace
    description: "Determine the target namespace"
    compute: |
      env = inputs.environment.lower()
      ns_input = (inputs.namespace or "main").lower()

      if env == "stage":
          namespace = cfg["namespace_stage"]
          env_label = "stage"
      elif "billing" in ns_input:
          namespace = cfg["namespace_prod_billing"]
          env_label = "production"
      else:
          namespace = cfg["namespace_prod_main"]
          env_label = "production"

      result = {
          "namespace": namespace,
          "environment": env_label,
          "is_prod": env_label == "production",
      }
    output: target

  # ==================== GET ALERTS ====================

  - name: get_firing_alerts
    description: "Get currently firing alerts"
    tool: prometheus_alerts
    args:
      environment: "{{ target.environment }}"
      namespace: "{{ target.namespace }}"
      state: firing
    output: alerts_raw
    on_error: continue

  - name: parse_alerts
    description: "Parse alert data using shared parser"
    compute: |
      from scripts.common.parsers import parse_alertmanager_output

      # Use shared parser for alert data
      alerts = parse_alertmanager_output(str(alerts_raw) if alerts_raw else "")

      # Check if specific alert is in the list
      if inputs.alert_name:
          filtered = [a for a in alerts if inputs.alert_name.lower() in a.get("name", "").lower()]
          alerts = filtered if filtered else alerts

      has_critical = any(a.get("severity") == "critical" for a in alerts)
      has_high = any(a.get("severity") in ["high", "warning"] for a in alerts)

      result = {
          "alerts": alerts[:10],
          "count": len(alerts),
          "has_critical": has_critical,
          "has_high": has_high,
          "max_severity": "critical" if has_critical else ("high" if has_high else "low"),
      }
    output: alert_info

  # ==================== QUICK HEALTH CHECK ====================

  - name: get_namespace_health
    description: "Get overall namespace health"
    tool: k8s_namespace_health
    args:
      namespace: "{{ target.namespace }}"
      environment: "{{ target.environment }}"
    output: namespace_health_raw
    on_error: continue

  - name: parse_namespace_health
    description: "Parse namespace health"
    compute: |
      health_text = str(namespace_health_raw) if 'namespace_health_raw' in dir() and namespace_health_raw else ""

      healthy = "healthy" in health_text.lower() or "ready" in health_text.lower()
      has_issues = "error" in health_text.lower() or "failed" in health_text.lower()

      result = {
        "healthy": healthy and not has_issues,
        "preview": health_text[:400] if health_text else "",
      }
    output: ns_health_info
    on_error: continue

  - name: get_pod_status
    description: "Get pod status"
    tool: kubectl_get_pods
    args:
      namespace: "{{ target.namespace }}"
      environment: "{{ target.environment }}"
    output: pods_raw
    on_error: continue

  - name: get_resource_usage
    description: "Get CPU/memory usage for pods"
    tool: kubectl_top_pods
    args:
      namespace: "{{ target.namespace }}"
      environment: "{{ target.environment }}"
    output: resource_usage_raw
    on_error: continue

  - name: parse_resource_usage
    description: "Parse resource usage for high consumption"
    compute: |
      usage_text = str(resource_usage_raw) if 'resource_usage_raw' in dir() and resource_usage_raw else ""

      high_cpu = []
      high_memory = []

      for line in usage_text.split("\n"):
        parts = line.split()
        if len(parts) >= 3:
          pod_name = parts[0]
          cpu = parts[1] if len(parts) > 1 else ""
          mem = parts[2] if len(parts) > 2 else ""

          # Check for high CPU (>500m) or memory (>1Gi)
          if "m" in cpu:
            try:
              cpu_val = int(cpu.replace("m", ""))
              if cpu_val > 500:
                high_cpu.append({"pod": pod_name, "cpu": cpu})
            except:
              pass

          if "Gi" in mem:
            try:
              mem_val = float(mem.replace("Gi", ""))
              if mem_val > 1.0:
                high_memory.append({"pod": pod_name, "memory": mem})
            except:
              pass

      result = {
        "high_cpu": high_cpu[:5],
        "high_memory": high_memory[:5],
        "raw_preview": usage_text[:500] if usage_text else "",
      }
    output: resource_analysis
    on_error: continue

  # Query error rate trend to see if issue is worsening
  - name: query_error_trend
    description: "Query error rate trend over last hour"
    tool: prometheus_query_range
    args:
      query: "sum(rate(http_requests_total{namespace=\"{{ target.namespace }}\",code=~\"5..\"}[5m]))"
      start: "1h"
      end: "now"
      step: "5m"
      environment: "{{ target.environment }}"
    output: error_trend_raw
    on_error: continue

  - name: parse_error_trend
    description: "Analyze error rate trend"
    compute: |
      trend_text = str(error_trend_raw) if 'error_trend_raw' in dir() and error_trend_raw else ""

      import re
      values = re.findall(r'(\d+\.?\d*)', trend_text)

      trend = "unknown"
      current_rate = None
      if len(values) >= 2:
          try:
              recent = float(values[-1])
              earlier = float(values[0])
              current_rate = recent

              if recent > earlier * 2:
                  trend = "increasing_fast"
              elif recent > earlier * 1.2:
                  trend = "increasing"
              elif recent < earlier * 0.5:
                  trend = "decreasing"
              else:
                  trend = "stable"
          except:
              pass

      result = {
          "trend": trend,
          "current_rate": current_rate,
          "is_worsening": trend in ["increasing", "increasing_fast"],
          "preview": trend_text[:300] if trend_text else "",
      }
    output: error_trend
    on_error: continue

  - name: parse_pod_health
    description: "Parse pod health"
    compute: |
      # parsers is available from skill engine safe_globals
      # Use shared parser
      pods = parsers.parse_kubectl_pods(str(pods_raw) if pods_raw else "") if parsers else []

      # Count health states
      running = sum(1 for p in pods if p.get("status") == "Running")
      not_running = sum(1 for p in pods if p.get("status") in ["CrashLoopBackOff", "Error", "Pending", "ImagePullBackOff"])
      high_restarts = sum(1 for p in pods if int(str(p.get("restarts", "0")).split()[0]) > 5)

      healthy = not_running == 0 and high_restarts == 0

      issues = []
      if not_running > 0:
          issues.append(f"{not_running} pod(s) not running")
      if high_restarts > 0:
          issues.append(f"{high_restarts} pod(s) with high restarts")

      result = {
          "running": running,
          "not_running": not_running,
          "high_restarts": high_restarts,
          "healthy": healthy,
          "issues": issues,
      }
    output: pod_health

  # ==================== GET RECENT EVENTS ====================

  - name: get_events
    description: "Get recent warning events"
    tool: kubectl_get_events
    args:
      namespace: "{{ target.namespace }}"
      environment: "{{ target.environment }}"
      field_selector: "type=Warning"
    output: events_raw
    on_error: continue

  - name: parse_events
    description: "Parse events"
    compute: |
      events = []
      if events_raw:
          lines = str(events_raw).split("\n")
          for line in lines[:10]:
              if line.strip():
                  events.append(line.strip()[:100])

      result = events[:5]
    output: recent_events

  # ==================== SEARCH LOGS IN KIBANA ====================

  - name: search_kibana_errors
    description: "Search Kibana for recent errors in namespace"
    tool: kibana_search_logs
    args:
      query: "error OR exception OR critical"
      environment: "{{ target.environment }}"
      namespace: "{{ target.namespace }}"
      limit: 10
    output: kibana_errors_raw
    on_error: continue

  - name: parse_kibana_errors
    description: "Parse Kibana error results"
    compute: |
      kibana_text = str(kibana_errors_raw) if kibana_errors_raw else ""

      # Check if we got actual results vs auth error
      if "Log In" in kibana_text or "403" in kibana_text:
          result = {
              "found": False,
              "error_count": 0,
              "errors": [],
              "auth_issue": True,
              "kibana_url": None
          }
      else:
          # Extract error lines
          lines = [l.strip() for l in kibana_text.split("\n") if l.strip()]
          error_lines = [l for l in lines if any(kw in l.lower() for kw in ["error", "exception", "traceback"])]

          # Extract Kibana URL if present
          kibana_url = None
          for line in lines:
              if "kibana" in line.lower() and "http" in line.lower():
                  import re
                  urls = re.findall(r'https?://[^\s\)]+', line)
                  if urls:
                      kibana_url = urls[0]
                      break

          result = {
              "found": len(error_lines) > 0,
              "error_count": len(error_lines),
              "errors": error_lines[:5],
              "auth_issue": False,
              "kibana_url": kibana_url
          }
    output: kibana_results

  # ==================== GET GRAFANA LINK ====================

  - name: get_grafana_link
    description: "Get Grafana dashboard link for namespace"
    tool: prometheus_grafana_link
    args:
      environment: "{{ target.environment }}"
      namespace: "{{ target.namespace }}"
    output: grafana_link_raw
    on_error: continue

  - name: parse_grafana_link
    description: "Extract Grafana URL"
    compute: |
      link_text = str(grafana_link_raw) if grafana_link_raw else ""
      import re
      urls = re.findall(r'https?://[^\s\)]+grafana[^\s\)]*', link_text)
      result = urls[0] if urls else None
    output: grafana_url
    on_error: continue

  # ==================== CHECK KNOWN PATTERNS ====================

  - name: check_patterns
    description: "Match against known error patterns"
    compute: |
      # Use shared memory helpers
      from scripts.common.config_loader import load_config

      data = memory.read_memory("learned/patterns")
      known_patterns = data.get("error_patterns", []) if isinstance(data.get("error_patterns"), list) else []
      matches = []

      # Check alerts against patterns
      for alert in alert_info.get("alerts", []):
          alert_name = alert.get("name", "").lower()
          for pattern in known_patterns:
              if pattern.get("pattern", "").lower() in alert_name:
                  matches.append({
                      "pattern": pattern.get("pattern"),
                      "cause": pattern.get("cause"),
                      "fix": pattern.get("fix"),
                  })

      # Check pod issues against patterns
      for issue in pod_health.get("issues", []):
          issue_lower = issue.lower()
          for pattern in known_patterns:
              if pattern.get("pattern", "").lower() in issue_lower:
                  matches.append({
                      "pattern": pattern.get("pattern"),
                      "cause": pattern.get("cause"),
                      "fix": pattern.get("fix"),
                  })

      result = matches[:5]
    output: pattern_matches

  # ==================== DETERMINE SEVERITY ====================

  - name: assess_severity
    description: "Determine overall severity"
    compute: |
      severity = "low"
      needs_escalation = False

      if alert_info.get("has_critical"):
          severity = "critical"
          needs_escalation = True
      elif not pod_health.get("healthy"):
          severity = "high"
          needs_escalation = True
      elif alert_info.get("has_high"):
          severity = "medium"
      elif len(recent_events) > 3:
          severity = "medium"
      elif kibana_results.get("error_count", 0) > 5:
          severity = "medium"

      result = {
          "severity": severity,
          "needs_escalation": needs_escalation and inputs.auto_escalate,
          "reason": "Critical alerts or unhealthy pods" if needs_escalation else "Routine check",
      }
    output: assessment

  # ==================== OFFER TO SILENCE ALERT ====================

  - name: check_existing_silences
    description: "Check if alerts are already silenced"
    condition: "alert_info.alerts"
    tool: alertmanager_list_silences
    args:
      environment: "{{ target.environment }}"
    output: existing_silences_raw
    on_error: continue

  - name: parse_existing_silences
    description: "Parse existing silences"
    condition: "existing_silences_raw"
    compute: |
      silences_text = str(existing_silences_raw) if existing_silences_raw else ""

      # Check which alerts are already silenced
      silenced_alerts = []
      for alert in alert_info.get("alerts", []):
          alert_name = alert.get("name", "")
          if alert_name.lower() in silences_text.lower():
              silenced_alerts.append(alert_name)

      result = {
          "any_silenced": len(silenced_alerts) > 0,
          "silenced_alerts": silenced_alerts,
          "total_silences": silences_text.count("silence") // 2 if silences_text else 0,
      }
    output: silences_check
    on_error: continue

  - name: build_silence_info
    description: "Build silence recommendation for critical/high alerts"
    condition: "assessment.severity in ['critical', 'high'] and alert_info.alerts"
    compute: |
      # Get the most critical alert for potential silencing
      alerts = alert_info.get("alerts", [])
      if not alerts:
          result = None
      else:
          alert = alerts[0]
          alert_name = alert.get("name", "unknown")

          # Check if already silenced
          already_silenced = False
          if 'silences_check' in dir() and silences_check:
              already_silenced = alert_name in silences_check.get("silenced_alerts", [])

          result = {
              "alert_name": alert_name,
              "can_silence": not already_silenced,
              "already_silenced": already_silenced,
              "silence_duration": "2h",
              "silence_command": f'skill_run("silence_alert", \'{{"alert_name": "{alert_name}", "environment": "{target.get("environment")}", "duration": "2h"}}\')'
          }
    output: silence_info
    on_error: continue

  # ==================== ESCALATE TO DEBUG_PROD ====================

  - name: escalate_to_debug_prod
    description: "Run full debug_prod for serious issues"
    condition: "assessment.needs_escalation and target.is_prod"
    tool: skill_run
    args:
      skill_name: debug_prod
      inputs: |
        {
          "namespace": "{{ 'billing' if 'billing' in target.namespace else 'main' }}",
          "alert_name": "{{ inputs.alert_name or '' }}",
          "time_range": "1h"
        }
    output: debug_result
    on_error: continue

  # Emit alert hooks
  - name: emit_alert_hooks
    description: "Notify team channel about critical alerts"
    compute: |
      # emit_event is available from skill engine safe_globals
      if emit_event:
          # Emit critical alert if applicable
          if assessment.get("severity") == "critical":
              alert_name = inputs.alert_name or "unknown"
              if alert_info.get("alerts"):
                  alert_name = alert_info["alerts"][0].get("name", alert_name)

              emit_event("alert_critical", {
                  "alert_name": alert_name,
                  "namespace": target.get("namespace", ""),
                  "environment": target.get("environment", ""),
              })
              result = "critical hook sent"

          # Emit escalated if we ran debug_prod
          elif assessment.get("needs_escalation") and debug_result:
              alert_name = inputs.alert_name or "unknown"
              emit_event("alert_escalated", {
                  "alert_name": alert_name,
                  "namespace": target.get("namespace", ""),
              })
              result = "escalated hook sent"
          else:
              result = "no hook needed"
      else:
          result = "hook skipped: emit_event not available"
    output: alert_hook_result
    on_error: continue

  # ==================== MEMORY INTEGRATION ====================

  - name: build_memory_context
    description: "Build context for memory updates"
    compute: |
      from datetime import datetime

      alert_names = [a.get("name", "unknown") for a in alert_info.get("alerts", [])[:3]]

      result = {
          "timestamp": datetime.now().isoformat(),
          "alert_summary": ", ".join(alert_names) if alert_names else "No alerts",
      }
    output: memory_context

  - name: log_session_investigation
    description: "Log alert investigation to session"
    tool: memory_session_log
    args:
      action: "Investigated alerts in {{ target.environment }}/{{ target.namespace }}"
      details: "Severity: {{ assessment.severity }}, Alerts: {{ memory_context.alert_summary }}"
    on_error: continue

  - name: update_environment_status
    description: "Update environment status in memory"
    compute: |
      # Use shared memory helpers
      data = memory.read_memory("state/environments")
      if "environments" not in data:
          data["environments"] = {}

      env_key = "stage" if target.get("environment") == "stage" else "production"
      if env_key not in data["environments"]:
          data["environments"][env_key] = {}

      env_data = data["environments"][env_key]
      env_data["status"] = "healthy" if pod_health.get("healthy") and alert_info.get("count", 0) == 0 else "issues"
      env_data["last_check"] = memory_context["timestamp"]
      env_data["alerts"] = [a.get("name", "") for a in alert_info.get("alerts", [])[:5]]

      # Add known issues if critical
      if assessment.get("severity") == "critical":
          known = env_data.get("known_issues", [])
          for alert in alert_info.get("alerts", []):
              if alert.get("severity") == "critical":
                  issue = {
                      "alert": alert.get("name", ""),
                      "since": memory_context["timestamp"],
                      "status": "investigating"
                  }
                  if issue not in known:
                      known.append(issue)
          env_data["known_issues"] = known[:10]

      memory.write_memory("state/environments", data)
      result = "environment status updated"
    output: env_update_result
    on_error: continue

# ==================== OUTPUT ====================

outputs:
  - name: report
    value: |
      ## üîç Alert Investigation: {{ target.namespace }}

      **Environment:** {{ target.environment }}
      **Severity:** {{ "üî¥ CRITICAL" if assessment.severity == "critical" else ("üü† HIGH" if assessment.severity == "high" else ("üü° MEDIUM" if assessment.severity == "medium" else "üü¢ LOW")) }}

      ---

      ### üö® Alerts ({{ alert_info.count }})
      {% if alert_info.count == 0 %}
      ‚úÖ No alerts currently firing
      {% else %}
      {% for alert in alert_info.alerts[:5] %}
      - {{ "üî¥" if alert.severity == "critical" else "üü†" }} **{{ alert.name }}** ({{ alert.severity }})
        {{ alert.message if alert.message else "" }}
      {% endfor %}
      {% endif %}

      ### üè• Pod Health
      {{ "‚úÖ Healthy" if pod_health.healthy else "‚ö†Ô∏è Issues Detected" }}
      - Running: {{ pod_health.running }} pods
      {% if pod_health.issues %}
      - Issues:
      {% for issue in pod_health.issues %}
        - {{ issue }}
      {% endfor %}
      {% endif %}

      ### üìä Resource Usage
      {% if resource_analysis and (resource_analysis.high_cpu or resource_analysis.high_memory) %}
      {% if resource_analysis.high_cpu %}
      **High CPU:**
      {% for pod in resource_analysis.high_cpu %}
      - {{ pod.pod }}: {{ pod.cpu }}
      {% endfor %}
      {% endif %}
      {% if resource_analysis.high_memory %}
      **High Memory:**
      {% for pod in resource_analysis.high_memory %}
      - {{ pod.pod }}: {{ pod.memory }}
      {% endfor %}
      {% endif %}
      {% else %}
      ‚úÖ Resource usage normal
      {% endif %}

      ### ‚ö†Ô∏è Recent Events
      {% if recent_events %}
      {% for event in recent_events[:3] %}
      - {{ event }}
      {% endfor %}
      {% else %}
      No warning events
      {% endif %}

      ### üìã Log Errors (Kibana)
      {% if kibana_results.auth_issue %}
      ‚ö†Ô∏è Kibana auth required - open in browser first
      {% elif kibana_results.found %}
      Found **{{ kibana_results.error_count }}** errors in logs:
      {% for err in kibana_results.errors[:3] %}
      - `{{ err[:80] }}`
      {% endfor %}
      {% else %}
      ‚úÖ No recent errors in logs
      {% endif %}
      {% if kibana_results.kibana_url %}
      üîó [Open in Kibana]({{ kibana_results.kibana_url }})
      {% endif %}

      {% if pattern_matches %}
      ### üí° Known Patterns Matched
      {% for match in pattern_matches %}
      - **{{ match.pattern }}**
        - Cause: {{ match.cause }}
        - Fix: {{ match.fix }}
      {% endfor %}
      {% endif %}

      ---

      ### üìä Dashboards
      {% if grafana_url %}
      - üìà [Grafana Dashboard]({{ grafana_url }})
      {% endif %}
      {% if kibana_results.kibana_url %}
      - üìã [Kibana Logs]({{ kibana_results.kibana_url }})
      {% endif %}

      {% if silence_info %}
      ### üîá Alert Silencing

      {% if silence_info.already_silenced %}
      ‚úÖ **{{ silence_info.alert_name }}** is already silenced

      List current silences:
      ```python
      alertmanager_list_silences(environment='{{ target.environment }}')
      ```
      {% elif silence_info.can_silence %}
      To silence **{{ silence_info.alert_name }}** for {{ silence_info.silence_duration }} while investigating:
      ```python
      {{ silence_info.silence_command }}
      ```
      {% endif %}
      {% endif %}

      {% if debug_result %}
      ### üî¨ Deep Investigation Results

      Escalated to `debug_prod`. See detailed results above.
      {% elif assessment.needs_escalation %}
      ### üî¨ Recommended: Deep Investigation

      Run: `skill_run("debug_prod", '{"namespace": "{{ 'billing' if 'billing' in target.namespace else 'main' }}"}')`
      {% else %}
      ### ‚úÖ Quick Actions

      - View pod logs: `kubectl_logs(namespace="{{ target.namespace }}", pod="<pod-name>")`
      - Get more details: `skill_run("debug_prod", '{"namespace": "{{ 'billing' if 'billing' in target.namespace else 'main' }}"}')`
      - Search logs: `kibana_search_logs(query="error", namespace="{{ target.namespace }}")`
      {% endif %}

  - name: severity
    value: "{{ assessment.severity }}"

  - name: context
    value:
      namespace: "{{ target.namespace }}"
      environment: "{{ target.environment }}"
      alert_count: "{{ alert_info.count }}"
      healthy: "{{ pod_health.healthy }}"
      escalated: "{{ debug_result is defined }}"
