# Skill: Debug Production Issues
# Systematic investigation of production problems
# Enhanced with knowledge/memory/vector search integration

name: debug_prod
description: |
  Investigate production issues in Automation Analytics.
  Gathers pod status, logs, metrics, alerts, and recent deployments.
  Suggests likely causes based on patterns and learned knowledge.

  **NEW Features:**
  - Semantic search for error handlers in codebase
  - DevOps knowledge integration for deployment patterns
  - Cross-reference with recent code changes
  - Architecture-aware debugging suggestions

  Resolves namespace and paths from config.json.
version: "2.0"

inputs:
  - name: namespace
    type: string
    required: false
    description: "Namespace to investigate: 'main' or 'billing' (will ask if not provided)"

  - name: alert_name
    type: string
    required: false
    description: "Prometheus alert name if triggered by an alert"

  - name: pod_filter
    type: string
    required: false
    description: "Filter pods by name (e.g., 'fastapi', 'processor')"

  - name: time_range
    type: string
    required: false
    default: "1h"
    description: "How far back to search (15m, 1h, 6h, 24h)"

# No hardcoded constants - resolved dynamically from config.json

steps:
  # ==================== PROACTIVE ISSUE DETECTION ====================

  - name: check_k8s_known_issues
    description: "Check for known Kubernetes issues before starting"
    tool: check_known_issues
    args:
      tool_name: "kubectl_get_pods"
      error_text: ""
    output: k8s_known_issues
    on_error: continue

  - name: check_kibana_known_issues
    description: "Check for known Kibana issues before starting"
    tool: check_known_issues
    args:
      tool_name: "kibana_search_logs"
      error_text: ""
    output: kibana_known_issues
    on_error: continue

  # ==================== LOAD CONFIG ====================

  - name: load_config
    description: "Load namespace and path configuration"
    compute: |
      import os
      from scripts.common.config_loader import load_config

      config = load_config()

      # Get namespace config
      ns_config = config.get("namespaces", {}).get("production", {})
      ai_config = config.get("app_interface", {})
      repos = config.get("repositories", {})
      paths_cfg = config.get("paths", {})

      # Get app-interface repo path
      ai_repo_name = ai_config.get("repo", "app-interface")
      ai_repo_path = ai_config.get("path") or repos.get(ai_repo_name, {}).get("path", "")
      if not ai_repo_path:
          # Try workspace_roots
          workspace_roots = paths_cfg.get("workspace_roots", [])
          for root in workspace_roots:
              candidate = os.path.join(os.path.expanduser(root), "app-interface")
              if os.path.exists(candidate):
                  ai_repo_path = candidate
                  break

      # Get namespace file names from config
      ns_files = ai_config.get("namespace_files", {})

      result = {
          "namespace_main": ns_config.get("main", "tower-analytics-prod"),
          "namespace_billing": ns_config.get("billing", "tower-analytics-prod-billing"),
          "kubeconfig": ns_config.get("kubeconfig", "~/.kube/config.p"),
          "alerts_main": ai_config.get("alerts", {}).get("prod_main", "resources/insights-prod/tower-analytics-prod"),
          "alerts_billing": ai_config.get("alerts", {}).get("prod_billing", "resources/insights-prod/tower-analytics-prod-billing"),
          "saas_namespaces": ai_config.get("saas_namespaces", "data/services/insights/tower-analytics/namespaces"),
          "saas_cicd": ai_config.get("deploy_file", "data/services/insights/tower-analytics/cicd/deploy-clowder.yml").rsplit("/", 1)[0],
          "appinterface_path": ai_repo_path,
          "jira_url": config.get("jira", {}).get("url", "https://issues.redhat.com"),
          "ns_file_main": ns_files.get("prod_main", "tower-analytics-prod.yml"),
          "ns_file_billing": ns_files.get("prod_billing", "tower-analytics-prod-billing.yml"),
      }
    output: cfg

  # ==================== RESOLVE NAMESPACE ====================

  # Step 1: Determine namespace
  - name: resolve_namespace
    description: "Resolve which namespace to investigate"
    compute: |
      if inputs.namespace:
        ns_input = inputs.namespace.lower()
        if 'billing' in ns_input:
          ns = cfg["namespace_billing"]
          ns_short = "billing"
        else:
          ns = cfg["namespace_main"]
          ns_short = "main"
      else:
        # Default to main, but note we should ask
        ns = None
        ns_short = None

      result = {"namespace": ns, "short": ns_short}
    output: ns_info

  # Step 2: If no namespace, return early asking for it
  - name: check_namespace
    condition: "{{ not ns_info.namespace }}"
    compute: |
      result = {
        "ask_namespace": True,
        "message": f"Which namespace to investigate?\n\n1. **main** - {cfg['namespace_main']}\n2. **billing** - {cfg['namespace_billing']}\n\nRun again with: `skill_run(\"debug_prod\", '{{\"namespace\": \"main\"}}')`"
      }
    output: namespace_prompt

  # ==================== LOAD LEARNED PATTERNS ====================

  # Step 3: Load known patterns from memory
  - name: load_patterns
    description: "Load known error patterns from memory"
    condition: "{{ ns_info.namespace }}"
    compute: |
      from scripts.common.config_loader import load_config

      # Use shared memory helpers
      data = memory.read_memory("learned/patterns")
      patterns = data.get('error_patterns', []) if isinstance(data.get('error_patterns'), list) else []
      result = patterns
    output: known_patterns
    on_error: continue

  # ==================== CHECK PODS ====================

  # Step 4: Get pod status
  - name: get_pods
    description: "Check pod status in namespace"
    condition: "{{ ns_info.namespace }}"
    tool: kubectl_get_pods
    args:
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
    output: pod_status
    on_error: auto_heal  # K8s cluster - may need kube_login

  - name: get_pod_resources
    description: "Get CPU/memory usage for pods"
    condition: "{{ ns_info.namespace }}"
    tool: kubectl_top_pods
    args:
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
    output: pod_resources_raw
    on_error: continue

  - name: parse_pod_resources
    description: "Parse resource usage data"
    compute: |
      resources_text = str(pod_resources_raw) if 'pod_resources_raw' in dir() and pod_resources_raw else ""

      high_cpu = []
      high_memory = []

      for line in resources_text.split("\n"):
          if not line.strip() or "NAME" in line:
              continue
          parts = line.split()
          if len(parts) >= 3:
              pod_name = parts[0]
              cpu = parts[1]  # e.g., "250m"
              memory = parts[2]  # e.g., "512Mi"

              # Parse CPU (millicore)
              cpu_val = 0
              if "m" in cpu:
                  try:
                      cpu_val = int(cpu.replace("m", ""))
                  except:
                      pass
              elif cpu.isdigit():
                  cpu_val = int(cpu) * 1000

              # Parse Memory (Mi/Gi)
              mem_val = 0
              if "Gi" in memory:
                  try:
                      mem_val = float(memory.replace("Gi", "")) * 1024
                  except:
                      pass
              elif "Mi" in memory:
                  try:
                      mem_val = float(memory.replace("Mi", ""))
                  except:
                      pass

              # Flag high usage
              if cpu_val > 500:  # >500m = high CPU
                  high_cpu.append({"pod": pod_name, "cpu": cpu})
              if mem_val > 1024:  # >1Gi = high memory
                  high_memory.append({"pod": pod_name, "memory": memory})

      result = {
          "raw": resources_text[:500] if resources_text else "Could not fetch",
          "high_cpu_pods": high_cpu[:5],
          "high_memory_pods": high_memory[:5],
      }
    output: pod_resources
    on_error: continue

  # Step 5: Analyze pod issues
  - name: analyze_pods
    description: "Identify unhealthy pods"
    condition: "{{ ns_info.namespace }}"
    compute: |
      # parsers is available from skill engine safe_globals
      # Use shared parser for kubectl pods
      pods = parsers.parse_kubectl_pods(str(pod_status)) if parsers else []

      # Analyze for issues
      issues = []
      healthy = []

      for pod in pods:
        name = pod.get("name", "")
        status = pod.get("status", "")
        restarts = pod.get("restarts", "0")

        if 'CrashLoopBackOff' in status:
          issues.append({"pod": name, "issue": "CrashLoopBackOff", "severity": "critical"})
        elif 'OOMKilled' in status:
          issues.append({"pod": name, "issue": "OOMKilled", "severity": "critical"})
        elif 'Error' in status:
          issues.append({"pod": name, "issue": status, "severity": "high"})
        elif 'Pending' in status:
          issues.append({"pod": name, "issue": "Pending", "severity": "medium"})
        elif 'ImagePullBackOff' in status:
          issues.append({"pod": name, "issue": "ImagePullBackOff", "severity": "high"})
        elif int(str(restarts).split()[0] if restarts else 0) > 5:
          issues.append({"pod": name, "issue": f"High restarts: {restarts}", "severity": "medium"})
        elif pod.get("healthy", False):
          healthy.append(name)

      result = {
        "issues": issues,
        "healthy_count": len(healthy),
        "unhealthy_count": len(issues)
      }
    output: pod_analysis

  # ==================== GET EVENTS ====================

  # Step 6: Get recent events
  - name: get_events
    description: "Get recent Kubernetes events"
    condition: "{{ ns_info.namespace }}"
    tool: kubectl_get_events
    args:
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
    output: k8s_events
    on_error: auto_heal  # K8s cluster - may need kube_login

  # Step 7: Filter important events
  - name: filter_events
    condition: "{{ ns_info.namespace and k8s_events }}"
    compute: |
      events_str = str(k8s_events)
      important = []

      # Look for warning/error events
      for line in events_str.split('\n')[-30:]:  # Last 30 events
        line_lower = line.lower()
        if any(kw in line_lower for kw in ['warning', 'error', 'failed', 'killed', 'backoff', 'unhealthy']):
          important.append(line.strip()[:150])  # Truncate long lines

      result = important[:10]  # Top 10 important events
    output: important_events
    on_error: continue

  # ==================== CHECK LOGS ====================

  # Step 8: Get logs from unhealthy pods (or filtered pods)
  - name: get_pod_logs
    description: "Get recent error logs from pods"
    condition: "{{ ns_info.namespace }}"
    compute: |
      # Determine which pods to check (use pod_analysis from previous step)
      pods_to_check = []
      if pod_analysis.get('issues'):
        pods_to_check = [p['pod'] for p in pod_analysis['issues'][:3]]  # Max 3 problem pods

      result = {"pods_to_check": pods_to_check, "time_range": inputs.time_range}
    output: pods_selection

  # Step 8a2: Describe first problem pod
  - name: describe_pod_1
    description: "Get detailed info on first problem pod"
    condition: "{{ pods_selection.pods_to_check|length > 0 }}"
    tool: kubectl_describe_pod
    args:
      pod_name: "{{ pods_selection.pods_to_check[0] }}"
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
    output: pod_describe_1_raw
    on_error: auto_heal  # K8s cluster - may need kube_login

  # Step 8b: Get logs from first problem pod
  - name: get_pod_logs_1
    description: "Get logs from first problem pod"
    condition: "{{ pods_selection.pods_to_check|length > 0 }}"
    tool: kubectl_logs
    args:
      pod_name: "{{ pods_selection.pods_to_check[0] }}"
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
      tail: 500
      since: "{{ pods_selection.time_range }}"
    output: pod_logs_1_raw
    on_error: auto_heal  # K8s cluster - may need kube_login

  # Step 8c: Get logs from second problem pod
  - name: get_pod_logs_2
    description: "Get logs from second problem pod"
    condition: "{{ pods_selection.pods_to_check|length > 1 }}"
    tool: kubectl_logs
    args:
      pod_name: "{{ pods_selection.pods_to_check[1] }}"
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
      tail: 500
      since: "{{ pods_selection.time_range }}"
    output: pod_logs_2_raw
    on_error: auto_heal  # K8s cluster - may need kube_login

  # Step 8d: Get logs from third problem pod
  - name: get_pod_logs_3
    description: "Get logs from third problem pod"
    condition: "{{ pods_selection.pods_to_check|length > 2 }}"
    tool: kubectl_logs
    args:
      pod_name: "{{ pods_selection.pods_to_check[2] }}"
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
      tail: 500
      since: "{{ pods_selection.time_range }}"
    output: pod_logs_3_raw
    on_error: auto_heal  # K8s cluster - may need kube_login

  # Step 8e: Parse logs for errors
  - name: parse_pod_logs
    description: "Extract error lines from logs"
    compute: |
      logs = []

      for i, (pod_name, raw_logs) in enumerate([
        (pods_selection.get('pods_to_check', [None])[0] if pods_selection.get('pods_to_check') else None, pod_logs_1_raw if 'pod_logs_1_raw' in dir() else None),
        (pods_selection.get('pods_to_check', [None, None])[1] if len(pods_selection.get('pods_to_check', [])) > 1 else None, pod_logs_2_raw if 'pod_logs_2_raw' in dir() else None),
        (pods_selection.get('pods_to_check', [None, None, None])[2] if len(pods_selection.get('pods_to_check', [])) > 2 else None, pod_logs_3_raw if 'pod_logs_3_raw' in dir() else None),
      ]):
        if not pod_name or not raw_logs:
          continue

        log_text = str(raw_logs)
        error_lines = []
        for line in log_text.split('\n'):
          line_lower = line.lower()
          if any(kw in line_lower for kw in ['error', 'exception', 'traceback', 'warning', 'critical', 'fatal']):
            error_lines.append(line[:200])

        if error_lines:
          logs.append({
            "pod": pod_name,
            "errors": error_lines[-20:]
          })

      result = logs
    output: pod_logs

  # ==================== CHECK ALERTS ====================

  # Step 9: Get firing alerts
  - name: get_alerts
    description: "Get currently firing alerts"
    condition: "{{ ns_info.namespace }}"
    tool: alertmanager_alerts
    args:
      environment: "production"
    output: firing_alerts
    on_error: auto_heal  # Alertmanager - may need auth

  # Step 10: Get alert definition if alert_name provided
  - name: get_alert_definition
    description: "Look up alert definition"
    condition: "{{ inputs.alert_name }}"
    compute: |
      import os
      from pathlib import Path
      from scripts.common.config_loader import load_config

      alert_name = inputs.alert_name
      alert_def = None

      # Determine which path based on namespace
      ai_path = Path(cfg["appinterface_path"])
      if 'billing' in ns_info.get('short', ''):
        alert_path = ai_path / cfg["alerts_billing"]
      else:
        alert_path = ai_path / cfg["alerts_main"]

      # Search for alert in yaml files
      if alert_path.exists():
        for f in alert_path.glob('*.yaml'):
          try:
            content = f.read_text()
            if alert_name in content:
              # Extract the alert block
              lines = content.split('\n')
              in_alert = False
              alert_lines = []
              for line in lines:
                if alert_name in line:
                  in_alert = True
                if in_alert:
                  alert_lines.append(line)
                  if line.strip() and not line.startswith(' ') and len(alert_lines) > 1:
                    break
              alert_def = '\n'.join(alert_lines[:20])
              break
          except:
            pass

      result = alert_def
    output: alert_definition
    on_error: continue

  # ==================== SEARCH KIBANA LOGS ====================

  - name: search_kibana_errors
    description: "Search Kibana for recent errors"
    condition: "{{ ns_info.namespace }}"
    tool: kibana_search_logs
    args:
      query: "error OR exception OR traceback"
      environment: "production"
      namespace: "{{ ns_info.namespace }}"
      limit: 20
    output: kibana_errors_raw
    on_error: auto_heal  # Kibana - may need auth

  - name: parse_kibana_errors_initial
    description: "Initial check for Kibana auth issues"
    compute: |
      kibana_text = str(kibana_errors_raw) if 'kibana_errors_raw' in dir() and kibana_errors_raw else ""
      auth_issue = "Log In" in kibana_text or "403" in kibana_text
      result = {"auth_issue": auth_issue, "raw": kibana_text}
    output: kibana_check

  # ==================== SLACK CONTEXT ====================

  - name: check_slack_discussions
    description: "Check team Slack channel for recent relevant discussions"
    tool: slack_channel_read
    args:
      channel: "team-automation-analytics"
      limit: 30
    output: slack_messages_raw
    on_error: continue

  - name: parse_slack_context
    description: "Parse Slack messages for relevant discussion"
    compute: |
      slack_text = str(slack_messages_raw) if 'slack_messages_raw' in dir() and slack_messages_raw else ""

      # Look for keywords related to production issues
      relevant_keywords = ["prod", "production", "error", "issue", "down", "alert", "fix", "deploy", "restart"]

      relevant_messages = []
      for line in slack_text.split("\n"):
        if any(kw in line.lower() for kw in relevant_keywords):
          relevant_messages.append(line[:150])

      has_relevant = len(relevant_messages) > 0

      result = {
        "has_relevant": has_relevant,
        "messages": relevant_messages[:5],
        "preview": "\n".join(relevant_messages[:3]) if relevant_messages else "",
      }
    output: slack_context
    on_error: continue

  - name: finalize_kibana_results
    description: "Finalize Kibana results after Slack check"
    compute: |
      kibana_text = str(kibana_errors_raw) if 'kibana_errors_raw' in dir() and kibana_errors_raw else ""

      if "Log In" in kibana_text or "403" in kibana_text:
          result = {"found": False, "errors": [], "auth_issue": True, "kibana_url": None}
      else:
          lines = [l.strip() for l in kibana_text.split("\n") if l.strip()]
          error_lines = [l[:150] for l in lines if any(kw in l.lower() for kw in ["error", "exception", "traceback"])]

          kibana_url = None
          import re
          for line in lines:
              if "kibana" in line.lower() and "http" in line.lower():
                  urls = re.findall(r'https?://[^\s\)]+', line)
                  if urls:
                      kibana_url = urls[0]
                      break

          result = {"found": len(error_lines) > 0, "errors": error_lines[:10], "auth_issue": False, "kibana_url": kibana_url}
    output: kibana_results
    on_error: continue

  # ==================== CHECK SAAS DEPLOYMENTS ====================

  - name: get_saas_pipelines
    description: "Get SaaS pipeline status"
    condition: "{{ ns_info.namespace }}"
    tool: kubectl_saas_pipelines
    args:
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
    output: saas_pipelines_raw
    on_error: continue

  - name: get_saas_deployments
    description: "Get SaaS deployment status"
    condition: "{{ ns_info.namespace }}"
    tool: kubectl_saas_deployments
    args:
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
    output: saas_deployments_raw
    on_error: continue

  - name: parse_saas_info
    description: "Parse SaaS deployment info"
    compute: |
      pipelines_text = str(saas_pipelines_raw) if 'saas_pipelines_raw' in dir() and saas_pipelines_raw else ""
      deployments_text = str(saas_deployments_raw) if 'saas_deployments_raw' in dir() and saas_deployments_raw else ""

      result = {
        "pipelines_preview": pipelines_text[:400] if pipelines_text else "",
        "deployments_preview": deployments_text[:400] if deployments_text else "",
        "has_data": len(pipelines_text) > 20 or len(deployments_text) > 20,
      }
    output: saas_info
    on_error: continue

  # ==================== CHECK METRICS ====================

  # Step 11: Query error rate from Prometheus
  - name: query_error_rate
    description: "Query 5xx error rate from Prometheus"
    condition: "{{ ns_info.namespace }}"
    tool: prometheus_query
    args:
      query: "sum(rate(http_requests_total{namespace=\"{{ ns_info.namespace }}\",code=~\"5..\"}[5m]))"
      environment: "production"
    output: error_rate_raw
    on_error: auto_heal  # Prometheus - may need auth

  - name: query_error_rate_trend
    description: "Query error rate trend over time"
    condition: "{{ ns_info.namespace }}"
    tool: prometheus_query_range
    args:
      query: "sum(rate(http_requests_total{namespace=\"{{ ns_info.namespace }}\",code=~\"5..\"}[5m]))"
      start: "1h"
      end: "now"
      step: "5m"
      environment: "production"
    output: error_trend_raw
    on_error: continue

  - name: query_memory_trend
    description: "Query memory usage trend"
    condition: "{{ ns_info.namespace }}"
    tool: prometheus_query_range
    args:
      query: "sum(container_memory_working_set_bytes{namespace=\"{{ ns_info.namespace }}\"}) by (pod)"
      start: "1h"
      end: "now"
      step: "5m"
      environment: "production"
    output: memory_trend_raw
    on_error: continue

  - name: query_pod_health
    description: "Check pod health metrics"
    condition: "{{ ns_info.namespace }}"
    tool: prometheus_pod_health
    args:
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
    output: pod_health_metrics_raw
    on_error: continue

  - name: get_prometheus_rules
    description: "Get alerting rules for this namespace"
    condition: "{{ ns_info.namespace }}"
    tool: prometheus_rules
    args:
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
    output: alert_rules_raw
    on_error: continue

  - name: list_grafana_dashboards
    description: "List available Grafana dashboards"
    condition: "{{ ns_info.namespace }}"
    tool: grafana_dashboard_list
    args:
      environment: "production"
    output: grafana_dashboards_raw
    on_error: continue

  - name: get_main_dashboard
    description: "Get main AA dashboard from Grafana"
    condition: "{{ grafana_dashboards_raw }}"
    tool: grafana_dashboard_get
    args:
      dashboard_id: "tower-analytics"
      environment: "production"
    output: main_dashboard_raw
    on_error: continue

  - name: get_grafana_link
    description: "Get Grafana dashboard link"
    condition: "{{ ns_info.namespace }}"
    tool: prometheus_grafana_link
    args:
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
    output: grafana_link_raw
    on_error: continue

  - name: parse_metrics
    description: "Parse Prometheus query results"
    compute: |
      error_rate_text = str(error_rate_raw) if 'error_rate_raw' in dir() and error_rate_raw else ""
      pod_health_text = str(pod_health_metrics_raw) if 'pod_health_metrics_raw' in dir() and pod_health_metrics_raw else ""
      grafana_text = str(grafana_link_raw) if 'grafana_link_raw' in dir() and grafana_link_raw else ""
      error_trend_text = str(error_trend_raw) if 'error_trend_raw' in dir() and error_trend_raw else ""
      memory_trend_text = str(memory_trend_raw) if 'memory_trend_raw' in dir() and memory_trend_raw else ""
      rules_text = str(alert_rules_raw) if 'alert_rules_raw' in dir() and alert_rules_raw else ""
      dashboards_text = str(grafana_dashboards_raw) if 'grafana_dashboards_raw' in dir() and grafana_dashboards_raw else ""

      # Extract error rate value
      error_rate = None
      import re
      rate_match = re.search(r'(\d+\.?\d*)', error_rate_text)
      if rate_match:
          error_rate = float(rate_match.group(1))

      # Extract Grafana URL
      grafana_url = None
      urls = re.findall(r'https?://[^\s\)]+grafana[^\s\)]*', grafana_text)
      if urls:
          grafana_url = urls[0]

      # Check if error rate is trending up
      trend_up = False
      if error_trend_text:
          values = re.findall(r'(\d+\.?\d*)', error_trend_text)
          if len(values) >= 2:
              try:
                  recent = float(values[-1])
                  earlier = float(values[0])
                  trend_up = recent > earlier * 1.5  # 50% increase
              except:
                  pass

      # Count alert rules
      rule_count = rules_text.lower().count("alert") if rules_text else 0

      # Count dashboards
      dashboard_count = dashboards_text.lower().count("dashboard") if dashboards_text else 0

      result = {
          "error_rate": error_rate,
          "error_rate_high": error_rate is not None and error_rate > 0.1,
          "error_trending_up": trend_up,
          "pod_health_summary": pod_health_text[:500] if pod_health_text else "Could not query",
          "grafana_url": grafana_url,
          "alert_rules_count": rule_count,
          "dashboards_count": dashboard_count,
          "error_trend_preview": error_trend_text[:300] if error_trend_text else "",
          "memory_trend_preview": memory_trend_text[:300] if memory_trend_text else "",
      }
    output: metrics_results
    on_error: continue

  - name: check_metrics
    description: "Build metric queries for reference"
    condition: "{{ ns_info.namespace }}"
    compute: |
      ns = ns_info['namespace']

      queries = [
        f'sum(rate(http_requests_total{{namespace="{ns}",code=~"5.."}}[5m]))',
        f'sum(container_memory_working_set_bytes{{namespace="{ns}"}}) by (pod)',
        f'sum(rate(container_cpu_usage_seconds_total{{namespace="{ns}"}}[5m])) by (pod)',
      ]

      result = {"queries_to_run": queries, "namespace": ns}
    output: metric_queries
    on_error: continue

  # ==================== CHECK RECENT DEPLOYMENTS ====================

  # Step 12a: Get deployments
  - name: get_deployments
    description: "Get deployment info"
    condition: "{{ ns_info.namespace }}"
    tool: kubectl_get_deployments
    args:
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
    output: deployments_raw
    on_error: auto_heal  # K8s cluster - may need kube_login

  # Step 12b: Get replicasets
  - name: get_replicasets
    description: "Get replicasets for recent rollouts"
    condition: "{{ ns_info.namespace }}"
    tool: kubectl_get
    args:
      resource: "replicasets"
      namespace: "{{ ns_info.namespace }}"
      environment: "production"
      output_format: "wide"
    output: replicasets_raw
    on_error: auto_heal  # K8s cluster - may need kube_login

  # Step 12c: Parse deployment info
  - name: parse_deployment_info
    description: "Combine deployment and replicaset info"
    compute: |
      deployments = str(deployments_raw)[:1000] if 'deployments_raw' in dir() and deployments_raw else "Could not fetch deployments"

      # Parse replicasets
      rs_text = str(replicasets_raw) if 'replicasets_raw' in dir() and replicasets_raw else ""
      rs_lines = rs_text.split('\n')
      # Get last 5 replicasets (plus header)
      recent_rs = rs_lines[:1] + rs_lines[-5:] if len(rs_lines) > 5 else rs_lines

      result = {
        "deployments": deployments,
        "recent_rollouts": '\n'.join(recent_rs)[:500]
      }
    output: deployment_info

  # Step 13: Check deployed SHA from app-interface CICD config
  - name: check_deployed_sha
    description: "Get currently deployed SHA from app-interface config"
    condition: "{{ ns_info.namespace }}"
    compute: |
      import yaml
      from pathlib import Path
      from scripts.common.config_loader import load_config

      cicd_file = Path(cfg["appinterface_path"]) / cfg["saas_cicd"] / "deploy-clowder.yml"
      deployed_info = {"sha": None, "namespace_config": None}

      if cicd_file.exists():
        try:
          with open(cicd_file) as f:
            content = f.read()

          # Parse to find the ref for this namespace
          data = yaml.safe_load(content)

          # Target namespace file based on which we're debugging (from config)
          if 'billing' in ns_info.get('short', ''):
            target_ns = cfg.get("ns_file_billing", "tower-analytics-prod-billing.yml")
          else:
            target_ns = cfg.get("ns_file_main", "tower-analytics-prod.yml")

          # Find the ref for this namespace in resourceTemplates
          for template in data.get('resourceTemplates', []):
            for target in template.get('targets', []):
              ns_ref = target.get('namespace', {}).get('$ref', '')
              if target_ns in ns_ref:
                deployed_info['sha'] = target.get('ref', 'unknown')
                deployed_info['namespace_config'] = ns_ref
                break
        except Exception as e:
          deployed_info['error'] = str(e)

      result = deployed_info
    output: deployed_sha
    on_error: continue

  # Step 14: Check namespace config
  - name: check_namespace_config
    description: "Get namespace configuration from app-interface"
    condition: "{{ ns_info.namespace }}"
    compute: |
      import yaml
      from pathlib import Path
      from scripts.common.config_loader import load_config

      ns_path = Path(cfg["appinterface_path"]) / cfg["saas_namespaces"]
      config_info = {}

      # Use namespace file from config
      if 'billing' in ns_info.get('short', ''):
        ns_file = ns_path / cfg.get("ns_file_billing", "tower-analytics-prod-billing.yml")
      else:
        ns_file = ns_path / cfg.get("ns_file_main", "tower-analytics-prod.yml")

      if ns_file.exists():
        try:
          with open(ns_file) as f:
            data = yaml.safe_load(f)

          # Extract key config info
          config_info = {
            "name": data.get('name', ''),
            "cluster": data.get('cluster', {}).get('$ref', '').split('/')[-1] if data.get('cluster') else '',
            "description": data.get('description', '')[:100] if data.get('description') else ''
          }
        except Exception as e:
          config_info['error'] = str(e)

      result = config_info
    output: namespace_config
    on_error: continue

  # ==================== ANALYZE & SUGGEST ====================

  # Step 13: Match against known patterns
  - name: match_patterns
    description: "Match issues against known patterns"
    condition: "{{ ns_info.namespace }}"
    compute: |
      matches = []

      # Check pod issues against known patterns
      for pod_issue in pod_analysis.get('issues', []):
        issue_type = pod_issue.get('issue', '')

        for pattern in (known_patterns or []):
          if isinstance(pattern, dict):
            pattern_text = pattern.get('pattern', '')
            if pattern_text.lower() in issue_type.lower():
              matches.append({
                "issue": issue_type,
                "pattern": pattern_text,
                "meaning": pattern.get('meaning', ''),
                "fix": pattern.get('fix', ''),
                "commands": pattern.get('commands', [])
              })

      # Add generic suggestions for unmatched issues
      for pod_issue in pod_analysis.get('issues', []):
        issue_type = pod_issue.get('issue', '')
        matched = any(m['issue'] == issue_type for m in matches)

        if not matched:
          if 'OOMKilled' in issue_type:
            matches.append({
              "issue": issue_type,
              "pattern": "OOMKilled",
              "meaning": "Container exceeded memory limits",
              "fix": "Increase memory limits or investigate memory leak",
              "commands": ["kubectl describe pod <pod>", "kubectl top pod -n <ns>"]
            })
          elif 'CrashLoopBackOff' in issue_type:
            matches.append({
              "issue": issue_type,
              "pattern": "CrashLoopBackOff",
              "meaning": "Container crashes repeatedly on startup",
              "fix": "Check logs for startup errors, config issues, or missing deps",
              "commands": ["kubectl logs <pod> --previous", "kubectl describe pod <pod>"]
            })
          elif 'ImagePullBackOff' in issue_type:
            matches.append({
              "issue": issue_type,
              "pattern": "ImagePullBackOff",
              "meaning": "Cannot pull container image",
              "fix": "Check image name, tag exists in Quay, registry credentials",
              "commands": ["kubectl describe pod <pod>"]
            })

      result = matches
    output: pattern_matches

  # ==================== KNOWLEDGE & VECTOR SEARCH ====================

  # Search for error handlers in the codebase
  - name: search_error_handlers
    description: "Find error handling code for observed errors"
    condition: "pod_logs and len(pod_logs) > 0"
    tool: code_search
    args:
      query: "{{ pod_logs[0].errors[0][:50] if pod_logs and pod_logs[0].errors else 'exception handler' }} error handling"
      project: "automation-analytics-backend"
      limit: 5
    output: error_handler_raw
    on_error: continue

  - name: parse_error_handlers
    description: "Parse error handler search results"
    compute: |
      handlers = []
      if 'error_handler_raw' in dir() and error_handler_raw:
          raw_text = str(error_handler_raw)
          for line in raw_text.split('\n'):
              if line.strip() and ':' in line:
                  handlers.append(line.strip()[:120])

      result = {
          "found": len(handlers) > 0,
          "handlers": handlers[:5],
          "count": len(handlers),
      }
    output: error_handlers
    on_error: continue

  # Load DevOps knowledge for deployment patterns
  - name: load_devops_knowledge
    description: "Load DevOps knowledge for debugging context"
    tool: knowledge_query
    args:
      project: "automation-analytics-backend"
      persona: "devops"
      section: "patterns.deployment"
    output: devops_patterns_raw
    on_error: continue

  - name: parse_devops_patterns
    description: "Parse DevOps deployment patterns"
    compute: |
      patterns = []
      if 'devops_patterns_raw' in dir() and devops_patterns_raw:
          raw_text = str(devops_patterns_raw)
          import re
          for match in re.finditer(r'-\s*(.+?)(?=\n-|\n\n|$)', raw_text, re.DOTALL):
              pattern = match.group(1).strip()[:150]
              if pattern and len(pattern) > 10:
                  patterns.append(pattern)

      result = {
          "found": len(patterns) > 0,
          "patterns": patterns[:5],
      }
    output: devops_patterns
    on_error: continue

  # Load production gotchas
  - name: load_prod_gotchas
    description: "Load production-specific gotchas"
    tool: knowledge_query
    args:
      project: "automation-analytics-backend"
      persona: "devops"
      section: "gotchas"
    output: prod_gotchas_raw
    on_error: continue

  - name: parse_prod_gotchas
    description: "Parse production gotchas"
    compute: |
      gotchas = []
      if 'prod_gotchas_raw' in dir() and prod_gotchas_raw:
          raw_text = str(prod_gotchas_raw)
          import re
          for match in re.finditer(r'-\s*(.+?)(?=\n-|\n\n|$)', raw_text, re.DOTALL):
              gotcha = match.group(1).strip()[:150]
              if gotcha and len(gotcha) > 10:
                  gotchas.append(gotcha)

      result = {
          "found": len(gotchas) > 0,
          "gotchas": gotchas[:5],
      }
    output: prod_gotchas
    on_error: continue

  # Search for code related to the deployed SHA
  - name: search_deployed_code
    description: "Search for code changes in deployed version"
    condition: "deployed_sha and deployed_sha.sha"
    tool: code_search
    args:
      query: "{{ ns_info.namespace }} deployment configuration"
      project: "automation-analytics-backend"
      limit: 3
    output: deployed_code_raw
    on_error: continue

  - name: parse_deployed_code
    description: "Parse deployed code search"
    compute: |
      code = []
      if 'deployed_code_raw' in dir() and deployed_code_raw:
          raw_text = str(deployed_code_raw)
          for line in raw_text.split('\n'):
              if line.strip() and ':' in line:
                  code.append(line.strip()[:100])

      result = {
          "found": len(code) > 0,
          "code": code[:3],
      }
    output: deployed_code
    on_error: continue

  # ==================== BUILD REPORT ====================

  # Step 16: Compile investigation report
  - name: build_report
    condition: "{{ ns_info.namespace }}"
    compute: |
      lines = []

      # Header
      lines.append(f"## ðŸ” Production Debug: {ns_info['namespace']}")
      lines.append(f"**Time range:** {inputs.time_range}")
      if inputs.alert_name:
        lines.append(f"**Alert:** {inputs.alert_name}")

      # Deployed SHA
      if deployed_sha and deployed_sha.get('sha'):
        lines.append(f"**Deployed SHA:** `{deployed_sha['sha'][:12]}`")
      lines.append("")

      # === POD STATUS ===
      lines.append("### ðŸ“¦ Pod Status")
      if pod_analysis['unhealthy_count'] > 0:
        lines.append(f"**âš ï¸ {pod_analysis['unhealthy_count']} unhealthy pods**")
        for issue in pod_analysis['issues'][:5]:
          severity_icon = "ðŸ”´" if issue['severity'] == 'critical' else "ðŸŸ " if issue['severity'] == 'high' else "ðŸŸ¡"
          lines.append(f"- {severity_icon} `{issue['pod']}`: {issue['issue']}")
      else:
        lines.append(f"âœ… All {pod_analysis['healthy_count']} pods healthy")
      lines.append("")

      # === RESOURCE USAGE ===
      if pod_resources and (pod_resources.get('high_cpu_pods') or pod_resources.get('high_memory_pods')):
        lines.append("### ðŸ“Š Resource Usage (High)")
        if pod_resources.get('high_cpu_pods'):
          lines.append("**High CPU:**")
          for p in pod_resources['high_cpu_pods'][:3]:
            lines.append(f"- `{p['pod']}`: {p['cpu']}")
        if pod_resources.get('high_memory_pods'):
          lines.append("**High Memory:**")
          for p in pod_resources['high_memory_pods'][:3]:
            lines.append(f"- `{p['pod']}`: {p['memory']}")
        lines.append("")

      # === POD DETAILS ===
      if 'pod_describe_1_raw' in dir() and pod_describe_1_raw:
        desc_text = str(pod_describe_1_raw)
        # Extract key info from describe
        conditions = []
        events = []
        for line in desc_text.split('\n'):
          if 'Conditions:' in line or ('Type' in line and 'Status' in line):
            conditions.append(line.strip()[:80])
          if 'Warning' in line or 'Error' in line:
            events.append(line.strip()[:100])

        if conditions or events:
          lines.append("### ðŸ”Ž Pod Details")
          pod_name = pods_selection.get('pods_to_check', [''])[0] if pods_selection else ''
          lines.append(f"**Pod:** `{pod_name}`")
          if events:
            lines.append("**Warning Events:**")
            for e in events[:3]:
              lines.append(f"- {e}")
          lines.append("")

      # === EVENTS ===
      if important_events:
        lines.append("### âš¡ Recent Events")
        for event in important_events[:5]:
          lines.append(f"- {event[:120]}")
        lines.append("")

      # === LOGS ===
      if pod_logs:
        lines.append("### ðŸ“‹ Error Logs")
        for log in pod_logs[:2]:  # Max 2 pods to avoid context overflow
          lines.append(f"**{log['pod']}:**")
          lines.append("```")
          for err in log['errors'][-5:]:  # Last 5 errors per pod
            lines.append(err[:150])
          lines.append("```")
        lines.append("")

      # === KIBANA LOGS ===
      if kibana_results and kibana_results.get('found'):
        lines.append("### ðŸ“‹ Kibana Log Errors")
        lines.append(f"Found **{len(kibana_results.get('errors', []))}** errors in logs:")
        for err in kibana_results.get('errors', [])[:5]:
          lines.append(f"- `{err[:100]}`")
        if kibana_results.get('kibana_url'):
          lines.append(f"\nðŸ”— [Open in Kibana]({kibana_results['kibana_url']})")
        lines.append("")
      elif kibana_results and kibana_results.get('auth_issue'):
        lines.append("### ðŸ“‹ Kibana Logs")
        lines.append("âš ï¸ Kibana auth required - open in browser first")
        lines.append("")

      # === METRICS ===
      if metrics_results:
        lines.append("### ðŸ“Š Metrics Summary")
        if metrics_results.get('error_rate') is not None:
          rate = metrics_results['error_rate']
          status = "ðŸ”´ HIGH" if rate > 0.1 else ("ðŸŸ¡ ELEVATED" if rate > 0.01 else "ðŸŸ¢ OK")
          lines.append(f"- **5xx Error Rate:** {rate:.4f}/sec {status}")
        if metrics_results.get('pod_health_summary'):
          lines.append(f"- **Pod Health:** {metrics_results['pod_health_summary'][:200]}")
        if metrics_results.get('grafana_url'):
          lines.append(f"\nðŸ“ˆ [Open Grafana Dashboard]({metrics_results['grafana_url']})")
        lines.append("")

      # === ALERTS ===
      if firing_alerts and 'No alerts' not in str(firing_alerts):
        lines.append("### ðŸš¨ Firing Alerts")
        alerts_str = str(firing_alerts)[:500]
        lines.append(f"```\n{alerts_str}\n```")
        lines.append("")

      # === ALERT DEFINITION ===
      if alert_definition:
        lines.append("### ðŸ“– Alert Definition")
        lines.append(f"```yaml\n{alert_definition[:300]}\n```")
        lines.append("")

      # === DEPLOYMENTS ===
      if deployment_info:
        lines.append("### ðŸš€ Recent Deployments")
        lines.append(f"```\n{deployment_info.get('recent_rollouts', 'N/A')[:400]}\n```")
        lines.append("")

      # === PATTERN MATCHES ===
      if pattern_matches:
        lines.append("### ðŸ’¡ Likely Causes")
        for match in pattern_matches[:3]:
          lines.append(f"**{match['pattern']}**")
          lines.append(f"- *Meaning:* {match['meaning']}")
          lines.append(f"- *Fix:* {match['fix']}")
          if match.get('commands'):
            lines.append(f"- *Commands:* `{match['commands'][0]}`")
        lines.append("")

      # === ERROR HANDLERS (from code search) ===
      if 'error_handlers' in dir() and error_handlers and error_handlers.get('found'):
        lines.append("### ðŸ” Related Error Handlers (Code Search)")
        lines.append(f"Found **{error_handlers['count']}** relevant code locations:")
        for handler in error_handlers.get('handlers', [])[:3]:
          lines.append(f"- `{handler}`")
        lines.append("")

      # === DEVOPS PATTERNS ===
      if 'devops_patterns' in dir() and devops_patterns and devops_patterns.get('found'):
        lines.append("### ðŸ“š DevOps Deployment Patterns")
        for pattern in devops_patterns.get('patterns', [])[:3]:
          lines.append(f"- {pattern}")
        lines.append("")

      # === PRODUCTION GOTCHAS ===
      if 'prod_gotchas' in dir() and prod_gotchas and prod_gotchas.get('found'):
        lines.append("### âš ï¸ Production Gotchas")
        for gotcha in prod_gotchas.get('gotchas', [])[:3]:
          lines.append(f"- {gotcha}")
        lines.append("")

      # === DEPLOYED CODE CONTEXT ===
      if 'deployed_code' in dir() and deployed_code and deployed_code.get('found'):
        lines.append("### ðŸ“ Related Deployment Code")
        for code in deployed_code.get('code', []):
          lines.append(f"- `{code}`")
        lines.append("")

      # === DASHBOARDS ===
      lines.append("### ðŸ”— Dashboards")
      if metrics_results and metrics_results.get('grafana_url'):
        lines.append(f"- ðŸ“ˆ [Grafana]({metrics_results['grafana_url']})")
      if kibana_results and kibana_results.get('kibana_url'):
        lines.append(f"- ðŸ“‹ [Kibana]({kibana_results['kibana_url']})")
      lines.append("")

      # === SLACK CONTEXT ===
      if 'slack_context' in dir() and slack_context and slack_context.get('has_relevant'):
        lines.append("### ðŸ’¬ Recent Team Discussion")
        lines.append("*From #team-automation-analytics:*")
        for msg in slack_context.get('messages', [])[:3]:
          lines.append(f"> {msg}")
        lines.append("")

      # === SUGGESTED QUERIES ===
      lines.append("### ðŸ“Š Additional Prometheus Queries")
      lines.append("Run these for more details:")
      for q in metric_queries.get('queries_to_run', [])[:3]:
        lines.append(f"```\n{q}\n```")

      result = '\n'.join(lines)
    output: report

  # ==================== MEMORY INTEGRATION ====================

  - name: build_memory_context
    description: "Build context for memory updates"
    condition: "not namespace_prompt"
    compute: |
      from datetime import datetime

      # Summarize findings
      issues_found = []
      if pod_issues and pod_issues.get("unhealthy_pods"):
          issues_found.append(f"{len(pod_issues['unhealthy_pods'])} unhealthy pods")
      if log_analysis and log_analysis.get("error_count", 0) > 0:
          issues_found.append(f"{log_analysis['error_count']} errors in logs")
      if pattern_matches:
          issues_found.append(f"{len(pattern_matches)} pattern matches")

      result = {
          "timestamp": datetime.now().isoformat(),
          "namespace": ns_info.get("namespace", "unknown") if ns_info else "unknown",
          "environment": ns_info.get("environment", "production") if ns_info else "production",
          "issues_summary": ", ".join(issues_found) if issues_found else "No major issues",
      }
    output: memory_context

  - name: log_session_debug
    description: "Log debug session"
    condition: "not namespace_prompt"
    tool: memory_session_log
    args:
      action: "Debugged {{ memory_context.environment }}/{{ memory_context.namespace }}"
      details: "{{ memory_context.issues_summary }}"
    on_error: continue

  - name: update_environment_after_debug
    description: "Update environment status after debugging"
    condition: "not namespace_prompt"
    compute: |
      # Use shared memory helpers
      env_key = "stage" if memory_context.get("environment") == "stage" else "production"

      # Determine status based on findings
      status = "healthy"
      if pod_issues and pod_issues.get("unhealthy_pods"):
          status = "issues"
      elif log_analysis and log_analysis.get("error_count", 0) > 10:
          status = "issues"

      memory.update_field("state/environments", f"environments.{env_key}.last_check", memory_context["timestamp"])
      memory.update_field("state/environments", f"environments.{env_key}.notes", f"Debug: {memory_context['issues_summary']}")
      memory.update_field("state/environments", f"environments.{env_key}.status", status)
      result = "environment updated"
    output: env_update_result
    on_error: continue

  # ==================== LEARNING FROM FAILURES ====================

  - name: detect_debug_failures
    description: "Detect failure patterns from debug operations"
    compute: |
      errors_detected = []

      # Check kubectl failures
      pod_status_text = str(pod_status) if 'pod_status' in dir() and pod_status else ""
      events_text = str(k8s_events) if 'k8s_events' in dir() and k8s_events else ""
      combined_k8s = pod_status_text + events_text

      if "unauthorized" in combined_k8s.lower() or "forbidden" in combined_k8s.lower():
          errors_detected.append({
              "tool": "kubectl_get_pods",
              "pattern": "unauthorized",
              "cause": "Kubernetes auth expired for production cluster",
              "fix": "Run kube_login(cluster='prod') to refresh credentials"
          })
      if "no route to host" in combined_k8s.lower():
          errors_detected.append({
              "tool": "kubectl_get_pods",
              "pattern": "no route to host",
              "cause": "VPN not connected - cannot reach production cluster",
              "fix": "Run vpn_connect() to connect to Red Hat VPN"
          })

      # Check Prometheus failures
      error_rate_text = str(error_rate_raw) if 'error_rate_raw' in dir() and error_rate_raw else ""
      if "connection refused" in error_rate_text.lower():
          errors_detected.append({
              "tool": "prometheus_query",
              "pattern": "connection refused",
              "cause": "Cannot connect to Prometheus - VPN or auth issue",
              "fix": "Check VPN connection and Prometheus access"
          })

      # Check Kibana failures
      kibana_text = str(kibana_errors_raw) if 'kibana_errors_raw' in dir() and kibana_errors_raw else ""
      if "unauthorized" in kibana_text.lower() or "401" in kibana_text:
          errors_detected.append({
              "tool": "kibana_search_logs",
              "pattern": "unauthorized",
              "cause": "Kibana authentication failed",
              "fix": "Check Kibana credentials in config.json"
          })

      # Check Slack failures
      slack_text = str(slack_messages_raw) if 'slack_messages_raw' in dir() and slack_messages_raw else ""
      if "invalid_auth" in slack_text.lower() or "not_authed" in slack_text.lower():
          errors_detected.append({
              "tool": "slack_search_messages",
              "pattern": "invalid_auth",
              "cause": "Slack authentication failed or token expired",
              "fix": "Refresh Slack token in config.json"
          })

      result = errors_detected
    output: debug_errors_detected
    on_error: continue

  - name: learn_prod_auth_failure
    description: "Learn from production cluster auth failures"
    condition: "debug_errors_detected and any(e.get('tool') == 'kubectl_get_pods' and e.get('pattern') == 'unauthorized' for e in debug_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "kubectl_get_pods"
      error_pattern: "unauthorized"
      root_cause: "Kubernetes auth expired for production cluster"
      fix_description: "Run kube_login(cluster='prod') to refresh credentials"
    output: prod_auth_fix_learned
    on_error: continue

  - name: learn_prod_vpn_failure
    description: "Learn from production VPN failures"
    condition: "debug_errors_detected and any(e.get('pattern') == 'no route to host' for e in debug_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "kubectl_get_pods"
      error_pattern: "no route to host"
      root_cause: "VPN not connected - cannot reach production cluster"
      fix_description: "Run vpn_connect() to connect to Red Hat VPN"
    output: prod_vpn_fix_learned
    on_error: continue

  - name: learn_prometheus_failure
    description: "Learn from Prometheus connection failures"
    condition: "debug_errors_detected and any(e.get('tool') == 'prometheus_query' for e in debug_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "prometheus_query"
      error_pattern: "connection refused"
      root_cause: "Cannot connect to Prometheus - VPN or auth issue"
      fix_description: "Check VPN connection and Prometheus access"
    output: prometheus_fix_learned
    on_error: continue

  - name: learn_kibana_failure
    description: "Learn from Kibana auth failures"
    condition: "debug_errors_detected and any(e.get('tool') == 'kibana_search_logs' for e in debug_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "kibana_search_logs"
      error_pattern: "unauthorized"
      root_cause: "Kibana authentication failed"
      fix_description: "Check Kibana credentials in config.json"
    output: kibana_fix_learned
    on_error: continue

outputs:
  - name: summary
    value: |
      {% if namespace_prompt %}
      {{ namespace_prompt.message }}
      {% else %}
      {{ report }}

      ---

      ### What's Next?

      1. **Get more logs:** `kubectl_logs(pod_name='<pod>', namespace='{{ ns_info.namespace }}', environment='production')`
      2. **Describe pod:** `kubectl_describe_pod(pod_name='<pod>', namespace='{{ ns_info.namespace }}', environment='production')`
      3. **Query metrics:** `prometheus_query(query='...', environment='production')`
      4. **Check Kibana:** `kibana_search_logs(namespace='{{ ns_info.namespace }}', query='error')`
      5. **View git history since deployment:** `git_log(repo='automation-analytics-backend', since='<deployed_sha>')`

      {% if pattern_matches %}
      ### ðŸ“ Remember This?

      If the suggested fix worked (or didn't), let me know and I'll update the learned patterns.
      {% endif %}
      {% endif %}

  - name: context
    value:
      namespace: "{{ ns_info.namespace }}"
      deployed_sha: "{{ deployed_sha.sha if deployed_sha else None }}"
      unhealthy_pods: "{{ pod_analysis.unhealthy_count if pod_analysis else 0 }}"
      has_alerts: "{{ firing_alerts is not none }}"
      pattern_matches: "{{ pattern_matches | length if pattern_matches else 0 }}"
