# Skill: Test MR in Ephemeral Environment
# Deploy PR/MR image to ephemeral namespace and run pytest validation
#
# IMPORTANT: This skill uses MCP tools (aa_bonfire, aa_quay). DO NOT run raw bonfire commands.
# The skill handles: kubeconfig, component names, image tags, and bonfire syntax automatically.

name: test_mr_ephemeral
description: |
  Deploy an MR's image to an ephemeral namespace for testing.

  ## How It Works
  1. Gets commit SHA from MR (via GitLab MCP)
  2. Checks if Konflux has built the image (via Quay MCP) - STOPS if not ready
  3. Reserves ephemeral namespace (via bonfire MCP)
  4. Deploys using full SHA image tag (via bonfire_deploy_aa)
  5. Runs pytest against ephemeral DB (optional)

  ## Prerequisites
  - Konflux must have built the image (check quay.io/redhat-user-workloads/aap-aa-tenant)
  - User must be logged into ephemeral cluster (run `kube e` once)

  ## NEVER Do These Things
  - DO NOT copy kubeconfig files (cp ~/.kube/config.e ~/.kube/config)
  - DO NOT run raw `bonfire deploy` without --set-image-tag with FULL SHA
  - DO NOT use short SHA (8 chars like 8d23cab) - images are tagged with FULL 40-char SHA
  - DO NOT truncate the SHA - Quay only has images tagged with full 40-char commit SHA

  ## Why Full SHA Matters
  Konflux tags images with the FULL 40-char git commit SHA, not short form.
  - WRONG: quay.io/.../image:8d23cab (manifest unknown!)
  - RIGHT: quay.io/.../image:8d23cab1234567890abcdef1234567890abcdef12

  ## Key Config Values (from config.json)
  - App: tower-analytics
  - Component: tower-analytics-clowdapp (main) or tower-analytics-billing-clowdapp (billing)
  - Image base: quay.io/redhat-user-workloads/aap-aa-tenant/aap-aa-main/automation-analytics-backend-main

  ## ITS Deploy Pattern (what bonfire needs)
  - template_ref: Full 40-char git commit SHA
  - IMAGE: quay.io/.../image@sha256 (base + @sha256 suffix)
  - IMAGE_TAG: 64-char sha256 digest from Quay (NOT the git SHA!)

  The skill automatically extracts the sha256 digest from Quay after checking the image exists.

  ## STOP Conditions
  - If image not in Quay: STOP, tell user to wait for Konflux build
  - If namespace reservation fails: STOP, check cluster login

  DO NOT fall back to raw bonfire commands - always use the MCP tools.
version: "1.4"

inputs:
  - name: mr_id
    type: integer
    required: false
    description: "GitLab MR ID (will find the image from Konflux)"

  - name: commit_sha
    type: string
    required: false
    description: "Specific commit SHA to test (alternative to mr_id)"

  - name: duration
    type: string
    required: false
    default: "2h"
    description: "How long to reserve namespace (e.g., 1h, 2h, 4h)"

  - name: run_tests
    type: boolean
    required: false
    default: true
    description: "Run pytest against ephemeral environment"

  - name: billing
    type: boolean
    required: false
    default: null
    description: |
      Which ClowdApp to deploy:
      - null (default): AUTO-DETECT from Jira issue and commit diff
      - false: Force tower-analytics-clowdapp (main app)
      - true: Force tower-analytics-billing-clowdapp (billing features)

      Auto-detection checks:
      1. Jira issue key in commit → search for "billing" in issue
      2. Commit modifies aap_billing_controller/ files
      3. Commit modifies test/processor/aap_billing_controller/ files

  - name: cleanup_on_failure
    type: boolean
    required: false
    default: true
    description: "Release namespace if deployment/tests fail"

  - name: cleanup_on_success
    type: boolean
    required: false
    default: false
    description: "Release namespace after successful tests (default: keep for manual testing)"

# No hardcoded constants - resolved from config.json

steps:
  # ==================== LOAD CONFIG ====================

  - name: load_config
    description: "Load Konflux, Quay, and GitLab configuration"
    compute: |
      from pathlib import Path
      from scripts.common.config_loader import load_config, get_config_section

      config = load_config()
      repos = config.get("repositories", {})
      quay_config = get_config_section("quay", {})
      bonfire_config = get_config_section("bonfire", {})

      # Get automation-analytics-backend config
      aa_config = repos.get("automation-analytics-backend", {})

      # Resolve repo path (for git operations)
      repo_path = aa_config.get("path", "")
      if not repo_path:
        # Fallback: try common locations
        for candidate in [
          Path.home() / "src" / "automation-analytics-backend",
          Path.home() / "src" / "aa-backend",
          Path("/home/daoneill/src/automation-analytics-backend"),
        ]:
          if candidate.exists():
            repo_path = str(candidate)
            break

      result = {
          "gitlab_project": aa_config.get("gitlab", "automation-analytics/automation-analytics-backend"),
          "konflux_namespace": aa_config.get("konflux_namespace", "aap-aa-tenant"),
          # Full Quay path: namespace/org/app/component
          "quay_pr_repo": f"{quay_config.get('default_namespace', 'redhat-user-workloads')}/aap-aa-tenant/aap-aa-main/automation-analytics-backend-main",
          "quay_pr_namespace": quay_config.get("default_namespace", "redhat-user-workloads"),
          "quay_release_namespace": "redhat-services-prod",
          "ephemeral_kubeconfig": bonfire_config.get("kubeconfig", str(Path.home() / ".kube/config.e")),
          "repo_path": repo_path,
      }
    output: cfg

  # ==================== PRE-FLIGHT CHECKS ====================

  # Step 0: Check required tools
  - name: check_tools
    description: "Verify bonfire and kubectl are available"
    compute: |
      import shutil

      required = ["bonfire", "kubectl", "oc"]
      missing = []
      available = []

      for tool in required:
          if shutil.which(tool):
              available.append(tool)
          else:
              missing.append(tool)

      # bonfire is required, kubectl/oc need at least one
      if "bonfire" in missing:
          raise ValueError(
              "bonfire not installed. Install with: pip install crc-bonfire"
          )

      if "kubectl" in missing and "oc" in missing:
          raise ValueError(
              "kubectl or oc not installed. Install OpenShift CLI."
          )

      result = {"available": available, "missing": missing}
    output: tools_check

  # ==================== GET COMMIT SHA ====================

  # Step 1: Get full 40-char SHA directly from GitLab API (if mr_id provided)
  - name: get_mr_sha
    description: "Get full 40-char commit SHA from GitLab MR"
    condition: "{{ inputs.get('mr_id') and not inputs.get('commit_sha') }}"
    tool: gitlab_mr_sha
    args:
      project: "{{ cfg.gitlab_project }}"
      mr_id: "{{ inputs.mr_id }}"
    output: mr_sha_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Step 1b: Parse SHA from gitlab_mr_sha output
  - name: parse_mr_sha
    description: "Extract SHA from gitlab_mr_sha response"
    condition: "{{ inputs.mr_id and not inputs.commit_sha }}"
    compute: |
      import re

      sha_text = str(mr_sha_raw) if 'mr_sha_raw' in dir() and mr_sha_raw else ""

      # Check for error
      if sha_text.startswith("❌"):
        raise ValueError(f"Failed to get MR SHA: {sha_text}")

      # Extract 40-char SHA from output
      sha_match = re.search(r'\*\*SHA:\*\*\s*`([a-f0-9]{40})`', sha_text)
      if sha_match:
        sha = sha_match.group(1)
      else:
        # Try to find any 40-char hex string
        hex_match = re.search(r'([a-f0-9]{40})', sha_text)
        if hex_match:
          sha = hex_match.group(1)
        else:
          raise ValueError(f"Could not extract 40-char SHA from: {sha_text[:200]}")

      # Extract branch if present
      branch_match = re.search(r'\*\*Branch:\*\*\s*`([^`]+)`', sha_text)
      branch = branch_match.group(1) if branch_match else ""

      result = {"sha": sha, "branch": branch}
    output: mr_info

  # Step 1c: Check GitLab CI pipeline status
  - name: check_gitlab_ci
    description: "Check GitLab CI pipeline status for MR"
    condition: "{{ inputs.mr_id }}"
    tool: gitlab_ci_status
    args:
      project: "{{ cfg.gitlab_project }}"
      mr_id: "{{ inputs.mr_id }}"
    output: gitlab_ci_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  - name: parse_gitlab_ci
    description: "Parse GitLab CI status"
    condition: "{{ gitlab_ci_raw }}"
    compute: |
      ci_text = str(gitlab_ci_raw) if gitlab_ci_raw else ""

      status = "unknown"
      pipeline_id = None

      import re
      if "failed" in ci_text.lower():
          status = "failed"
      elif "success" in ci_text.lower() or "passed" in ci_text.lower():
          status = "success"
      elif "running" in ci_text.lower():
          status = "running"

      id_match = re.search(r'pipeline[:\s#]*(\d+)', ci_text, re.I)
      if id_match:
          pipeline_id = id_match.group(1)

      result = {
          "status": status,
          "pipeline_id": pipeline_id,
          "can_retry": status == "failed" and pipeline_id is not None,
      }
    output: gitlab_ci_status
    on_error: continue

  # Step 2: Determine final commit SHA
  - name: resolve_commit
    compute: |
      if inputs.get('commit_sha'):
        sha = inputs.get('commit_sha')
      elif mr_info and mr_info.get('sha'):
        sha = mr_info['sha']
      else:
        sha = None
      result = sha
    output: commit_sha

  # Step 3: Validate we have a commit
  - name: validate_commit
    compute: |
      if not commit_sha:
        raise Exception("Could not determine commit SHA. Provide either mr_id or commit_sha.")
      result = f"Using commit: {commit_sha[:12]}"
    output: commit_status

  # ==================== AUTO-DETECT CLOWDAPP ====================
  # Determine if we should deploy billing or main ClowdApp

  # Step 3a: Get commit message using git_show
  - name: get_commit_message
    description: "Get commit message for Jira key extraction"
    condition: "{{ cfg.repo_path }}"
    tool: git_show
    args:
      repo: "{{ cfg.repo_path }}"
      commit: "{{ commit_sha }}"
      format: "%s%n%b"
    output: commit_message_raw
    on_error: continue

  # Step 3b: Parse commit info
  - name: parse_commit_info
    description: "Extract Jira key and billing keywords from commit using shared parser"
    compute: |
      from scripts.common.parsers import extract_jira_key

      commit_message = str(commit_message_raw).strip() if 'commit_message_raw' in dir() and commit_message_raw else ""

      # Remove error prefix if present
      if commit_message.startswith("❌"):
        commit_message = ""

      # Extract Jira issue key using shared parser
      jira_key = extract_jira_key(commit_message)
      if jira_key:
        jira_key = jira_key.upper()

      # Check for billing keywords in commit message itself
      billing_keywords = ['billing', 'subscription', 'vcpu', 'host_count', 'infra_usage']
      commit_has_billing = any(kw in commit_message.lower() for kw in billing_keywords)

      result = {
        "message": commit_message[:200],
        "jira_key": jira_key,
        "commit_mentions_billing": commit_has_billing
      }
    output: commit_info

  # Step 3b: Check Jira issue for billing indicators
  - name: check_jira_billing
    description: "Look for billing indicators in Jira issue"
    condition: "{{ commit_info.get('jira_key') }}"
    tool: jira_get_issue
    args:
      issue_key: "{{ commit_info.jira_key }}"
    output: jira_issue
    on_error: auto_heal  # Jira API - may need auth refresh

  # Step 3c: Analyze Jira issue for billing signals
  - name: analyze_jira_billing
    description: "Check if Jira issue mentions billing"
    compute: |
      jira_text = str(jira_issue).lower() if jira_issue else ""

      billing_signals = [
        'billing',
        'subscription watch',
        'subscription-watch',
        'vcpu',
        'host count',
        'metering',
        'rhsm',
        'red hat subscription',
        'aap-billing',
        'billing-controller',
        'billing controller',
        'billing exporter',
        'swatch',
      ]

      found_signals = [s for s in billing_signals if s in jira_text]

      result = {
        "is_billing": len(found_signals) > 0,
        "signals": found_signals[:5],  # Top 5 matches
        "jira_checked": bool(jira_issue)
      }
    output: jira_billing_check

  # Step 3d: Get files changed in commit
  - name: get_changed_files
    description: "Get list of files changed in this commit"
    condition: "{{ cfg.repo_path }}"
    tool: git_diff_tree
    args:
      repo: "{{ cfg.repo_path }}"
      commit: "{{ commit_sha }}"
      name_only: true
    output: changed_files_raw
    on_error: continue

  # Step 3e: Check for billing file changes
  - name: check_billing_files
    description: "Check if commit modifies billing-related files"
    compute: |
      changed_files_text = str(changed_files_raw) if 'changed_files_raw' in dir() and changed_files_raw else ""

      # Skip if error
      if changed_files_text.startswith("❌"):
        changed_files = []
      else:
        changed_files = [f.strip() for f in changed_files_text.strip().split('\n') if f.strip()]

      # Billing-related paths
      billing_paths = [
        'tower_analytics_report/processor/aap_billing_controller',
        'test/processor/aap_billing_controller',
        'aap_billing',
        'billing',
        'subscription_watch',
      ]

      billing_files = []
      for f in changed_files:
        for bp in billing_paths:
          if bp in f.lower():
            billing_files.append(f)
            break

      result = {
        "total_files_changed": len(changed_files),
        "billing_files_changed": billing_files,
        "has_billing_changes": len(billing_files) > 0
      }
    output: diff_billing_check

  # Step 3e: Determine final ClowdApp selection
  - name: determine_clowdapp
    description: "Decide whether to deploy billing or main ClowdApp"
    compute: |
      # If user explicitly specified, use that
      if inputs.get('billing') is not None:
        use_billing = inputs.get('billing')
        reason = "User explicitly specified"
      else:
        # Auto-detect based on signals
        signals = []

        # Signal 1: Commit message mentions billing (safe access)
        commit_info_safe = commit_info if 'commit_info' in dir() and commit_info else {}
        if commit_info_safe.get('commit_mentions_billing'):
          signals.append("commit_message")

        # Signal 2: Jira issue mentions billing (safe access - may not exist)
        jira_billing_safe = jira_billing_check if 'jira_billing_check' in dir() and jira_billing_check else {}
        if jira_billing_safe.get('is_billing'):
          signals.append(f"jira_issue({', '.join(jira_billing_safe.get('signals', [])[:2])})")

        # Signal 3: Commit changes billing files (safe access)
        diff_billing_safe = diff_billing_check if 'diff_billing_check' in dir() and diff_billing_check else {}
        if diff_billing_safe.get('has_billing_changes'):
          signals.append(f"files({len(diff_billing_safe.get('billing_files_changed', []))} billing files)")

        use_billing = len(signals) > 0
        reason = f"Auto-detected: {', '.join(signals)}" if signals else "No billing signals found, using main"

      # Select ClowdApp
      if use_billing:
        clowdapp = "tower-analytics-billing-clowdapp"
        template = "clowderapp-billing.yaml"
      else:
        clowdapp = "tower-analytics-clowdapp"
        template = "clowderapp.yaml"

      result = {
        "use_billing": use_billing,
        "clowdapp": clowdapp,
        "template": template,
        "reason": reason,
        "jira_key": commit_info_safe.get('jira_key') if 'commit_info_safe' in dir() else None,
        "billing_files": diff_billing_safe.get('billing_files_changed', [])[:5] if 'diff_billing_safe' in dir() else []
      }
    output: clowdapp_selection

  # ==================== CHECK IMAGE EXISTS ====================

  # Step 4: Check if image exists and get sha256 digest using skopeo
  - name: check_quay_image
    description: "Verify the image was built and get sha256 digest for bonfire"
    tool: skopeo_get_digest
    args:
      repository: "{{ cfg.quay_pr_repo }}"
      tag: "{{ commit_sha }}"
      namespace: "{{ cfg.quay_pr_namespace }}"
    output: quay_result
    on_error: auto_heal  # Quay API - may need auth/network

  # Step 5: If image not found, check Konflux build status
  - name: check_konflux_build
    description: "Check Konflux build status for this commit"
    condition: "{{ not quay_result or 'not found' in str(quay_result).lower() }}"
    tool: konflux_list_builds
    args:
      namespace: "{{ cfg.konflux_namespace }}"
      limit: 10
    output: konflux_builds

  # Step 5b: Get Tekton pipeline runs for this commit
  - name: list_tekton_pipelines
    description: "List Tekton pipeline runs to find this commit's build"
    condition: "{{ not quay_result or 'not found' in str(quay_result).lower() }}"
    tool: tkn_pipelinerun_list
    args:
      namespace: "{{ cfg.konflux_namespace }}"
      limit: 20
    output: tekton_pipelines_raw
    on_error: auto_heal  # Konflux cluster - may need kube_login

  - name: list_snapshots
    description: "List recent snapshots to find deployment candidate"
    condition: "{{ quay_result and 'found' in str(quay_result).lower() }}"
    tool: konflux_list_snapshots
    args:
      namespace: "{{ cfg.konflux_namespace }}"
    output: snapshots_raw
    on_error: auto_heal  # Konflux cluster - may need kube_login

  - name: get_snapshot_for_sha
    description: "Get snapshot details for this commit"
    condition: "snapshots_raw"
    tool: konflux_get_snapshot
    args:
      name: "{{ git_sha[:8] }}"
      namespace: "{{ cfg.konflux_namespace }}"
    output: snapshot_details_raw
    on_error: auto_heal  # Konflux cluster - may need kube_login

  - name: find_commit_pipeline
    description: "Find pipeline run for this specific commit"
    condition: "{{ not quay_result or 'not found' in str(quay_result).lower() }}"
    compute: |
      # Try both Konflux API and Tekton output
      builds_text = str(konflux_builds) if konflux_builds else ""
      tekton_text = str(tekton_pipelines_raw) if 'tekton_pipelines_raw' in dir() and tekton_pipelines_raw else ""
      combined = builds_text + "\n" + tekton_text

      sha_short = commit_sha[:12] if commit_sha else ""

      # Look for our commit in the build list
      pipeline_name = None
      build_status = "unknown"

      for line in combined.split("\n"):
          if sha_short in line:
              # Extract pipeline name (usually first column)
              parts = line.split()
              if parts:
                  pipeline_name = parts[0]
              # Check for status keywords
              if "running" in line.lower():
                  build_status = "running"
              elif "succeeded" in line.lower() or "complete" in line.lower():
                  build_status = "succeeded"
              elif "failed" in line.lower():
                  build_status = "failed"
              break

      result = {
          "pipeline_name": pipeline_name,
          "build_status": build_status,
          "commit_found": pipeline_name is not None,
      }
    output: commit_pipeline

  # Step 5b2: Get detailed pipeline run status if found
  - name: describe_pipeline_run
    description: "Get detailed status of the pipeline run"
    condition: "{{ commit_pipeline and commit_pipeline.get('pipeline_name') }}"
    tool: tkn_pipelinerun_describe
    args:
      run_name: "{{ commit_pipeline.pipeline_name }}"
      namespace: "{{ cfg.konflux_namespace }}"
    output: pipeline_describe_raw
    on_error: auto_heal  # Tekton/Konflux - may need kube_login

  - name: parse_pipeline_details
    description: "Extract detailed pipeline information"
    condition: "{{ pipeline_describe_raw }}"
    compute: |
      desc_text = str(pipeline_describe_raw) if pipeline_describe_raw else ""

      # Extract task statuses
      import re
      tasks = []
      for line in desc_text.split("\n"):
          if "TaskRun" in line or "Task " in line:
              tasks.append(line.strip()[:100])

      # Check for specific failure reasons
      failure_reason = None
      if "failed" in desc_text.lower():
          reason_match = re.search(r'reason[:\s]+(.+)', desc_text, re.I)
          if reason_match:
              failure_reason = reason_match.group(1)[:100]

      result = {
          "tasks": tasks[:10],
          "failure_reason": failure_reason,
          "raw_preview": desc_text[:1000] if desc_text else "",
      }
    output: pipeline_details
    on_error: continue

  # Step 5b3: Get Tekton logs if pipeline failed or running
  - name: get_tekton_pipeline_logs
    description: "Get Tekton pipeline run logs"
    condition: "{{ commit_pipeline and commit_pipeline.get('pipeline_name') and commit_pipeline.get('build_status') in ['failed', 'running'] }}"
    tool: tkn_pipelinerun_logs
    args:
      run_name: "{{ commit_pipeline.pipeline_name }}"
      namespace: "{{ cfg.konflux_namespace }}"
      follow: false
      all_tasks: true
    output: tekton_logs_raw
    on_error: auto_heal  # Tekton/Konflux - may need kube_login

  # Step 5c: Get build logs if pipeline failed
  - name: get_failed_build_logs
    description: "Get logs from failed build"
    condition: "{{ commit_pipeline and commit_pipeline.get('build_status') == 'failed' and commit_pipeline.get('pipeline_name') }}"
    tool: konflux_get_build_logs
    args:
      build_name: "{{ commit_pipeline.pipeline_name }}"
      namespace: "{{ cfg.konflux_namespace }}"
      tail: 100
    output: build_logs_raw
    on_error: auto_heal  # Konflux cluster - may need kube_login

  - name: parse_build_failure
    description: "Parse build failure details from Konflux and Tekton logs"
    condition: "{{ commit_pipeline and commit_pipeline.get('build_status') == 'failed' }}"
    compute: |
      # Combine Konflux and Tekton logs
      logs_text = str(build_logs_raw) if 'build_logs_raw' in dir() and build_logs_raw else ""
      tekton_logs = str(tekton_logs_raw) if 'tekton_logs_raw' in dir() and tekton_logs_raw else ""
      combined_logs = logs_text + "\n" + tekton_logs

      # Look for error indicators
      error_lines = []
      for line in combined_logs.split("\n"):
          line_lower = line.lower()
          if any(kw in line_lower for kw in ["error", "failed", "fatal", "exception", "traceback"]):
              error_lines.append(line.strip()[:150])

      # Get failure reason from pipeline details if available
      failure_reason = None
      if 'pipeline_details' in dir() and pipeline_details:
          failure_reason = pipeline_details.get('failure_reason')

      result = {
          "has_logs": bool(combined_logs.strip()),
          "errors": error_lines[:10],
          "full_logs": combined_logs[-2000:] if combined_logs else "",
          "failure_reason": failure_reason,
          "tasks": pipeline_details.get('tasks', []) if 'pipeline_details' in dir() and pipeline_details else [],
      }
    output: build_failure_info
    on_error: continue

  # Step 6: Validate image availability and extract sha256 digest
  - name: validate_image
    compute: |
      import re

      quay_output = str(quay_result) if 'quay_result' in dir() and quay_result else ""

      # Check for success indicator from skopeo_get_digest
      if "✅ Image Digest" in quay_output:
        # Extract the 64-char sha256 hash (without sha256: prefix)
        # Look for the IMAGE_TAG line which has just the hash
        hash_match = re.search(r'For bonfire deploy.*?\n```\n([a-f0-9]{64})\n```', quay_output, re.DOTALL)
        if hash_match:
          digest_hash = hash_match.group(1)
        else:
          # Fallback: extract from full digest line
          digest_match = re.search(r'\*\*Digest:\*\*\s*`sha256:([a-f0-9]{64})`', quay_output)
          digest_hash = digest_match.group(1) if digest_match else None

        if digest_hash:
          result = {
            "available": True,
            "status": "Image ready in Quay",
            "sha256_digest": f"sha256:{digest_hash}"
          }
        else:
          result = {"available": False, "status": "Could not parse digest from Quay response"}
      elif "❌" in quay_output:
        result = {"available": False, "status": "Image not found - build may be in progress"}
      else:
        result = {"available": False, "status": "Unknown Quay response"}
    output: image_status

  # Step 6b: HARD STOP if image not available
  # DO NOT proceed to namespace reservation or deployment if image doesn't exist
  - name: check_image_ready
    description: "STOP if image not built yet"
    compute: |
      if not image_status.get("available", False):
        # Return early with clear message - do NOT try to deploy
        result = {
          "error": True,
          "message": f"STOP: Image for commit {commit_sha[:12] if commit_sha else 'unknown'} not found in Quay.",
          "action": "Wait for Konflux to build the image, then retry.",
          "check_command": f"quay_get_tag(repository='automation-analytics-backend-main', tag='{commit_sha}', namespace='aap-aa-tenant')"
        }
      else:
        result = {"error": False, "message": "Image available, proceeding with deployment"}
    output: image_check

  # ==================== RESERVE NAMESPACE ====================
  # Only proceed if image is available (image_check.error == False)

  # Step 7: Reserve ephemeral namespace
  - name: reserve_namespace
    description: "Reserve an ephemeral namespace"
    condition: "{{ image_status.get('available', False) and not image_check.get('error', True) }}"
    tool: bonfire_namespace_reserve
    args:
      duration: "{{ inputs.duration }}"
      pool: "default"
      timeout: 600
      force: true
    output: namespace_result
    on_error: auto_heal  # CRITICAL: Ephemeral cluster - needs kube_login(ephemeral)

  # Step 8: Extract namespace name
  - name: get_namespace_name
    condition: "{{ image_status.get('available', False) and not image_check.get('error', True) }}"
    compute: |
      from scripts.common.parsers import extract_ephemeral_namespace

      # Use retry result if available, otherwise use original
      ns_result = namespace_result_retry if 'namespace_result_retry' in dir() and namespace_result_retry else namespace_result
      result = extract_ephemeral_namespace(str(ns_result)) if ns_result else None
    output: namespace_name

  # ==================== DEPLOY ====================

  # Step 9: Deploy to ephemeral
  # Uses ITS pattern: template_ref = git SHA, image_tag = sha256 digest
  # ClowdApp is auto-detected from Jira/commit or explicitly specified
  - name: deploy_app
    description: "Deploy AA to ephemeral namespace using {{ clowdapp_selection.clowdapp }}"
    condition: "{{ namespace_name and image_status.get('sha256_digest') }}"
    tool: bonfire_deploy_aa
    args:
      namespace: "{{ namespace_name }}"
      template_ref: "{{ commit_sha }}"
      image_tag: "{{ image_status.sha256_digest }}"
      billing: "{{ clowdapp_selection.use_billing }}"
      timeout: 900
    output: deploy_result

  # Step 10: Wait for pods to be ready
  - name: wait_for_ready
    description: "Wait for deployment to be ready"
    condition: "{{ namespace_name and deploy_result }}"
    tool: bonfire_namespace_wait
    args:
      namespace: "{{ namespace_name }}"
      timeout: 300
    output: wait_result
    on_error: auto_heal  # Ephemeral cluster - may need kube_login

  # Step 10b: Get deployment events for diagnostics
  - name: get_deployment_events
    description: "Get Kubernetes events for deployment troubleshooting"
    condition: "{{ namespace_name }}"
    tool: kubectl_get_events
    args:
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
    output: deploy_events_raw
    on_error: auto_heal  # Ephemeral cluster - may need kube_login

  - name: parse_deploy_events
    description: "Parse deployment events for issues"
    compute: |
      events_text = str(deploy_events_raw) if 'deploy_events_raw' in dir() and deploy_events_raw else ""

      warnings = []
      errors = []
      for line in events_text.split("\n"):
          line_lower = line.lower()
          if "warning" in line_lower or "failed" in line_lower:
              warnings.append(line.strip()[:120])
          if "error" in line_lower or "backoff" in line_lower:
              errors.append(line.strip()[:120])

      result = {
          "has_issues": len(errors) > 0,
          "warnings": warnings[:5],
          "errors": errors[:5],
          "preview": events_text[:500] if events_text else "",
      }
    output: deploy_events
    on_error: continue

  # ==================== GET DB CREDENTIALS ====================

  # Step 11: Get postgres connection info from automation-analytics-db secret
  # Using individual tool calls for each secret key

  - name: get_db_host
    description: "Get DB host from secret"
    condition: "{{ namespace_name and inputs.run_tests }}"
    tool: kubectl_get_secret_value
    args:
      secret_name: "automation-analytics-db"
      key: "db.host"
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
      decode: true
    output: db_host_raw
    on_error: auto_heal  # Ephemeral cluster - DB secrets

  - name: get_db_port
    description: "Get DB port from secret"
    condition: "{{ namespace_name and inputs.run_tests }}"
    tool: kubectl_get_secret_value
    args:
      secret_name: "automation-analytics-db"
      key: "db.port"
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
      decode: true
    output: db_port_raw
    on_error: auto_heal  # Ephemeral cluster - DB secrets

  - name: get_db_user
    description: "Get DB user from secret"
    condition: "{{ namespace_name and inputs.run_tests }}"
    tool: kubectl_get_secret_value
    args:
      secret_name: "automation-analytics-db"
      key: "db.user"
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
      decode: true
    output: db_user_raw
    on_error: auto_heal  # Ephemeral cluster - DB secrets

  - name: get_db_password
    description: "Get DB password from secret"
    condition: "{{ namespace_name and inputs.run_tests }}"
    tool: kubectl_get_secret_value
    args:
      secret_name: "automation-analytics-db"
      key: "db.password"
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
      decode: true
    output: db_password_raw
    on_error: auto_heal  # Ephemeral cluster - DB secrets

  - name: get_db_name
    description: "Get DB name from secret"
    condition: "{{ namespace_name and inputs.run_tests }}"
    tool: kubectl_get_secret_value
    args:
      secret_name: "automation-analytics-db"
      key: "db.name"
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
      decode: true
    output: db_name_raw
    on_error: auto_heal  # Ephemeral cluster - DB secrets

  - name: build_db_credentials
    description: "Build DB credentials object from secret values"
    condition: "{{ namespace_name and inputs.run_tests }}"
    compute: |
      def clean_value(raw, default=""):
        if not raw or str(raw).startswith("❌"):
          return default
        return str(raw).strip()

      db_host = clean_value(db_host_raw if 'db_host_raw' in dir() else None)
      db_port = clean_value(db_port_raw if 'db_port_raw' in dir() else None, "5432")
      db_user = clean_value(db_user_raw if 'db_user_raw' in dir() else None)
      db_password = clean_value(db_password_raw if 'db_password_raw' in dir() else None)
      db_name = clean_value(db_name_raw if 'db_name_raw' in dir() else None)

      ns = namespace_name

      result = {
        "host": db_host or f"automation-analytics-db.{ns}.svc",
        "port": db_port,
        "user": db_user or "postgres",
        "password": db_password or "",
        "database": db_name or "tower-analytics",
        "found": bool(db_host and db_user and db_password),
        "connection_string": f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}" if all([db_user, db_password, db_host, db_name]) else None
      }
    output: db_creds

  # Step 12: Check pod status
  - name: check_pods
    description: "Get pod status in namespace"
    condition: "{{ namespace_name }}"
    tool: kubectl_get_pods
    args:
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
    output: pod_status
    on_error: auto_heal  # Ephemeral cluster - pod status

  - name: identify_failing_pods
    description: "Identify any failing pods for detailed diagnosis"
    compute: |
      pod_text = str(pod_status) if 'pod_status' in dir() and pod_status else ""

      failing_pods = []
      for line in pod_text.split("\n"):
        parts = line.split()
        if len(parts) >= 3:
          name = parts[0]
          ready = parts[1]
          status = parts[2]
          # Check for non-Running statuses
          if status not in ["Running", "Completed", "NAME"] and name != "NAME":
            failing_pods.append({"name": name, "ready": ready, "status": status})

      result = {
        "failing": failing_pods[:3],
        "first_failing": failing_pods[0]["name"] if failing_pods else None,
        "has_failures": len(failing_pods) > 0,
      }
    output: failing_pod_check
    on_error: continue

  - name: describe_failing_pod
    description: "Get detailed info on first failing pod"
    condition: "failing_pod_check and failing_pod_check.has_failures"
    tool: kubectl_describe_pod
    args:
      pod_name: "{{ failing_pod_check.first_failing }}"
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
    output: failing_pod_description
    on_error: auto_heal  # Ephemeral cluster - pod details

  - name: parse_pod_issue
    description: "Parse pod description for issue details"
    condition: "failing_pod_description"
    compute: |
      desc_text = str(failing_pod_description) if 'failing_pod_description' in dir() else ""

      # Look for common issues
      issues = []
      if "ImagePullBackOff" in desc_text:
        issues.append("ImagePullBackOff - Image cannot be pulled")
      if "CrashLoopBackOff" in desc_text:
        issues.append("CrashLoopBackOff - Container is crashing repeatedly")
      if "OOMKilled" in desc_text:
        issues.append("OOMKilled - Container ran out of memory")
      if "Pending" in desc_text:
        issues.append("Pending - Pod cannot be scheduled")

      # Extract events
      import re
      events = []
      event_matches = re.findall(r'(Warning|Normal)\s+\w+\s+[^\n]+', desc_text)
      events = event_matches[-5:] if event_matches else []

      result = {
        "issues": issues,
        "events": events,
        "preview": desc_text[-500:] if desc_text else "",
      }
    output: pod_issue_analysis
    on_error: continue

  # Step 13: Get FastAPI pod name
  # Using kubectl_get_pods tool and then parsing for fastapi pattern
  - name: get_all_pods
    description: "Get all pods in namespace"
    condition: "{{ namespace_name and inputs.run_tests }}"
    tool: kubectl_get_pods
    args:
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
    output: all_pods_raw
    on_error: auto_heal  # Ephemeral cluster - pod list for tests

  - name: find_fastapi_pod
    description: "Find the FastAPI pod from pod listing"
    condition: "{{ namespace_name and inputs.run_tests and all_pods_raw }}"
    compute: |
      pods_text = str(all_pods_raw) if 'all_pods_raw' in dir() else ""

      fastapi_pod = None
      for line in pods_text.split('\n'):
        # Skip header and empty lines
        if not line.strip() or 'NAME' in line:
          continue

        parts = line.split()
        if len(parts) >= 3:
          pod_name = parts[0]
          status = parts[2] if len(parts) > 2 else ""

          # Find fastapi pod that's Running
          if 'fastapi' in pod_name.lower() and 'Running' in status:
            fastapi_pod = pod_name
            break

      result = fastapi_pod
    output: fastapi_pod

  # ==================== RUN TESTS ====================

  # Step 14: Run smoke tests (not full suite - that takes 90 min!)
  # LEARNINGS:
  # - LOG_LEVEL env var causes conflicts (some code expects int, some string) - UNSET it
  # - DATABASE_PREFIX is required by test helpers
  # - Full test suite takes ~90 minutes - only run smoke tests inline
  # - Test directories: test/test_*.py, test/v1/, test/processor/ (no test/restapi/)
  # Step 14a: Create test script locally
  - name: create_test_script
    description: "Create smoke test script with DB credentials"
    condition: "{{ fastapi_pod and db_creds }}"
    compute: |
      import os
      import textwrap

      db = db_creds
      ns = namespace_name

      # Extract credentials
      db_host = db.get('host', f'automation-analytics-db.{ns}.svc')
      db_port = db.get('port', '5432')
      db_user = db.get('user', 'postgres')
      db_password = db.get('password', '')
      db_name = db.get('database', 'tower-analytics')

      # Build DATABASE_PREFIX (required by test helpers)
      database_prefix = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}"
      database_url = f"{database_prefix}/{db_name}"

      # Create smoke test script
      test_script = textwrap.dedent(f"""
          #!/bin/bash

          # Database connection from automation-analytics-db secret
          export POSTGRESQL_USER="{db_user}"
          export POSTGRESQL_PASSWORD="{db_password}"
          export POSTGRESQL_HOST="{db_host}"
          export POSTGRESQL_PORT="{db_port}"
          export POSTGRESQL_DATABASE="{db_name}"

          # SQLAlchemy-style URLs (required by app and tests)
          export DATABASE_URL="{database_url}"
          export DATABASE_PREFIX="{database_prefix}"

          # App settings
          export SECRET_KEY="test-secret-key-12345"

          # CRITICAL: Unset LOG_LEVEL - app has conflicting code
          unset LOG_LEVEL

          cd /opt/app-root/src

          echo "=== Smoke Tests (full suite = 90min, run separately) ==="
          echo "DB: {db_host}:{db_port}/{db_name}"
          echo ""

          # Run quick smoke tests only
          pytest test/test_hello.py test/test_db.py test/test_configurator.py test/test_liveness_check.py -v --tb=short 2>&1

          echo ""
          echo "=== Smoke tests complete ==="
      """).strip()

      # Write script to temp file
      script_path = "/tmp/ephemeral_smoke_test.sh"
      with open(script_path, 'w') as f:
        f.write(test_script)
      os.chmod(script_path, 0o755)

      result = script_path
    output: test_script_path

  # Step 14b: Copy test script to pod
  - name: copy_test_script
    description: "Copy smoke test script to FastAPI pod"
    condition: "{{ fastapi_pod and test_script_path }}"
    tool: kubectl_cp
    args:
      source: "{{ test_script_path }}"
      destination: "{{ fastapi_pod }}:/tmp/smoke_test.sh"
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
      container: "automation-analytics-api-fastapi-v2"
      to_pod: true
    output: copy_result
    on_error: continue

  # Step 14c: Execute smoke tests
  - name: run_smoke_tests
    description: "Execute smoke tests in FastAPI pod"
    condition: "{{ fastapi_pod and copy_result }}"
    tool: kubectl_exec
    args:
      pod_name: "{{ fastapi_pod }}"
      command: "bash /tmp/smoke_test.sh"
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
      container: "automation-analytics-api-fastapi-v2"
      timeout: 120
    output: test_exec_result
    on_error: continue

  # Step 14d: Parse test results
  - name: parse_test_results
    description: "Parse smoke test output"
    compute: |
      output = str(test_exec_result) if 'test_exec_result' in dir() and test_exec_result else ""

      if len(output) > 3000:
        output = output[-3000:]

      # Check for pytest success indicators
      passed = "passed" in output.lower() and "failed" not in output.lower()

      result = {
        "passed": passed,
        "output": output,
        "smoke_only": True,
        "full_suite_time": "~90 minutes"
      }
    output: test_result

  # ==================== CLEANUP ====================

  # Step 14: Cleanup on failure
  - name: cleanup_on_failure
    description: "Release namespace if deployment or tests failed"
    condition: "namespace_name and inputs.cleanup_on_failure and ((deploy_result and 'fail' in str(deploy_result).lower()) or (test_result and not test_result.get('passed')))"
    tool: bonfire_namespace_release
    args:
      namespace: "{{ namespace_name }}"
    output: cleanup_failure_result
    on_error: continue

  # Step 15: Cleanup on success (if requested)
  - name: cleanup_on_success
    description: "Release namespace after successful tests"
    condition: "namespace_name and inputs.cleanup_on_success and test_result and test_result.get('passed')"
    tool: bonfire_namespace_release
    args:
      namespace: "{{ namespace_name }}"
    output: cleanup_success_result
    on_error: continue

  # Step 16: Determine final cleanup status
  - name: cleanup_status
    description: "Report cleanup status"
    compute: |
      if cleanup_failure_result:
        result = {"cleaned_up": True, "reason": "failure"}
      elif cleanup_success_result:
        result = {"cleaned_up": True, "reason": "success"}
      else:
        result = {"cleaned_up": False, "reason": "kept"}
    output: cleanup_info

  # Step 17: Emit ephemeral hooks
  - name: emit_ephemeral_hooks
    description: "Notify about ephemeral environment status"
    compute: |
      # emit_event is available from skill engine safe_globals
      if emit_event:
          # Emit ready notification when deployed
          if namespace_name and deploy_result and 'fail' not in str(deploy_result).lower():
              emit_event("ephemeral_ready", {
                  "namespace": namespace_name,
                  "mr_id": str(inputs.get('mr_id', '')),
                  "author": "",  # self
              })
              result = "ephemeral_ready hook sent"

          # Emit test failure notification
          if test_result and not test_result.get('passed'):
              emit_event("ephemeral_tests_failed", {
                  "mr_id": str(inputs.get('mr_id', '')),
                  "namespace": namespace_name or "",
                  "author": "",  # self
              })
              result = "ephemeral_tests_failed hook sent"
          else:
              result = "no failure hook needed"
      else:
          result = "hook skipped: emit_event not available"
    output: ephemeral_hook_result
    on_error: continue

  # ==================== KNOWLEDGE INTEGRATION ====================

  - name: search_related_tests
    description: "Find test files related to the changed code"
    condition: "{{ commit_info and commit_info.get('message') }}"
    tool: code_search
    args:
      query: "test {{ commit_info.message[:100] }}"
      project: "automation-analytics-backend"
      limit: 5
      min_score: 0.5
    output: related_tests_raw
    on_error: continue

  - name: parse_related_tests
    description: "Parse related test results"
    compute: |
      tests_result = related_tests_raw if 'related_tests_raw' in dir() and related_tests_raw else {}

      related_tests = []
      if isinstance(tests_result, dict) and tests_result.get('results'):
          for r in tests_result.get('results', [])[:5]:
              if 'test' in r.get('file_path', '').lower():
                  related_tests.append({
                      'file': r.get('file_path', ''),
                      'score': r.get('score', 0),
                  })

      result = {
          'tests': related_tests,
          'count': len(related_tests),
          'has_related_tests': len(related_tests) > 0,
      }
    output: related_tests
    on_error: continue

  - name: get_testing_gotchas
    description: "Get testing-specific gotchas from knowledge"
    tool: knowledge_query
    args:
      project: "automation-analytics-backend"
      persona: "developer"
      section: "gotchas"
    output: testing_gotchas_raw
    on_error: continue

  - name: parse_testing_gotchas
    description: "Parse testing gotchas"
    compute: |
      gotchas_result = testing_gotchas_raw if 'testing_gotchas_raw' in dir() and testing_gotchas_raw else {}

      testing_gotchas = []
      if isinstance(gotchas_result, dict) and gotchas_result.get('found'):
          content = gotchas_result.get('content', [])
          if isinstance(content, list):
              # Filter for test-related gotchas
              for g in content:
                  g_str = str(g).lower()
                  if any(kw in g_str for kw in ['test', 'pytest', 'mock', 'fixture', 'assert']):
                      testing_gotchas.append(g)

      result = {
          'gotchas': testing_gotchas[:5],
          'has_gotchas': len(testing_gotchas) > 0,
      }
    output: testing_gotchas
    on_error: continue

  - name: check_ephemeral_known_issues
    description: "Check for known ephemeral/bonfire issues"
    compute: |
      # Check known issues for bonfire and ephemeral
      bonfire_issues = memory.check_known_issues("bonfire", "") or {}
      quay_issues = memory.check_known_issues("quay", "") or {}
      konflux_issues = memory.check_known_issues("konflux", "") or {}

      all_issues = []
      for issues in [bonfire_issues, quay_issues, konflux_issues]:
          if issues and issues.get("matches"):
              all_issues.extend(issues.get("matches", [])[:2])

      result = {
          "has_known_issues": len(all_issues) > 0,
          "issues": all_issues[:5],
      }
    output: ephemeral_known_issues
    on_error: continue

  # ==================== MEMORY INTEGRATION ====================

  - name: build_memory_context
    description: "Build context for memory updates"
    compute: |
      from datetime import datetime

      result = {
          "timestamp": datetime.now().isoformat(),
          "mr_id": str(inputs.get('mr_id', '')) if inputs.get('mr_id') else '',
          "commit_short": commit_sha[:12] if commit_sha else 'unknown',
      }
    output: memory_context

  - name: log_session_ephemeral
    description: "Log ephemeral deployment to session"
    condition: "namespace_name"
    tool: memory_session_log
    args:
      action: "Deployed to ephemeral {{ namespace_name }}"
      details: "MR: !{{ memory_context.mr_id }}, Commit: {{ memory_context.commit_short }}, Duration: {{ inputs.duration }}"
    on_error: continue

  - name: track_ephemeral_namespace
    description: "Track ephemeral namespace in environment memory"
    condition: "namespace_name"
    compute: |
      # Use shared memory helpers
      from datetime import timedelta

      # Parse duration for expiry calculation
      duration_str = inputs.get("duration", "2h")
      hours = int(duration_str.replace("h", "")) if "h" in duration_str else 2
      expiry = (datetime.now() + timedelta(hours=hours)).isoformat()

      # Read current environments
      data = memory.read_memory("state/environments")
      if "environments" not in data:
          data["environments"] = {}
      if "ephemeral" not in data["environments"]:
          data["environments"]["ephemeral"] = {"active_namespaces": []}

      eph = data["environments"]["ephemeral"]
      eph["status"] = "active"
      eph["last_check"] = memory_context["timestamp"]

      # Add this namespace to active list
      active = eph.get("active_namespaces", [])
      ns_entry = {
          "name": namespace_name,
          "mr_id": memory_context["mr_id"],
          "commit": memory_context["commit_short"],
          "created": memory_context["timestamp"],
          "expires": expiry,
          "tests_passed": test_result.get("passed") if test_result else None,
      }

      # Remove old entry for same namespace if exists
      active = [n for n in active if n.get("name") != namespace_name]
      active.append(ns_entry)
      eph["active_namespaces"] = active[:20]  # Keep last 20

      memory.write_memory("state/environments", data)
      result = "ephemeral namespace tracked"
    output: ephemeral_tracking_result
    on_error: continue

  # ==================== LEARNING FROM FAILURES ====================

  - name: detect_failure_patterns
    description: "Detect failure patterns from tool outputs"
    compute: |
      # Collect error patterns from various tool outputs
      errors_detected = []

      # Check GitLab failures
      mr_sha_text = str(mr_sha_raw) if 'mr_sha_raw' in dir() and mr_sha_raw else ""
      if "no such host" in mr_sha_text.lower() or "dial tcp" in mr_sha_text.lower():
          errors_detected.append({
              "tool": "gitlab_mr_sha",
              "pattern": "no such host",
              "cause": "VPN not connected - internal GitLab not reachable",
              "fix": "Run vpn_connect() to connect to Red Hat VPN"
          })

      # Check Quay failures
      quay_text = str(quay_result) if 'quay_result' in dir() and quay_result else ""
      if "manifest unknown" in quay_text.lower():
          errors_detected.append({
              "tool": "quay_get_tag",
              "pattern": "manifest unknown",
              "cause": "Image not built yet or wrong SHA - Konflux may not have completed build",
              "fix": "Wait for Konflux build to complete, check with konflux_list_builds()"
          })
      if "unauthorized" in quay_text.lower():
          errors_detected.append({
              "tool": "quay_get_tag",
              "pattern": "unauthorized",
              "cause": "Quay authentication failed or image is private",
              "fix": "Check Quay credentials and image visibility"
          })

      # Check bonfire/namespace failures
      ns_text = str(namespace_result) if 'namespace_result' in dir() and namespace_result else ""
      if "no pools available" in ns_text.lower():
          errors_detected.append({
              "tool": "bonfire_namespace_reserve",
              "pattern": "no pools available",
              "cause": "All ephemeral namespace pools are exhausted",
              "fix": "Wait for pools to free up or release unused namespaces with bonfire_namespace_list(mine=True)"
          })
      if "unauthorized" in ns_text.lower() or "forbidden" in ns_text.lower():
          errors_detected.append({
              "tool": "bonfire_namespace_reserve",
              "pattern": "unauthorized",
              "cause": "Kubernetes auth expired for ephemeral cluster",
              "fix": "Run kube_login(cluster='ephemeral') to refresh credentials"
          })

      # Check Konflux/Tekton failures
      tekton_text = str(tekton_pipelines_raw) if 'tekton_pipelines_raw' in dir() and tekton_pipelines_raw else ""
      if "unauthorized" in tekton_text.lower():
          errors_detected.append({
              "tool": "tkn_pipelinerun_list",
              "pattern": "unauthorized",
              "cause": "Kubernetes auth expired for Konflux cluster",
              "fix": "Run kube_login(cluster='konflux') to refresh credentials"
          })

      result = errors_detected
    output: detected_errors
    on_error: continue

  - name: learn_gitlab_failure
    description: "Learn from GitLab failures"
    condition: "detected_errors and any(e.get('tool') == 'gitlab_mr_sha' for e in detected_errors)"
    tool: learn_tool_fix
    args:
      tool_name: "gitlab_mr_sha"
      error_pattern: "no such host"
      root_cause: "VPN not connected - internal GitLab not reachable"
      fix_description: "Run vpn_connect() to connect to Red Hat VPN"
    output: gitlab_fix_learned
    on_error: continue

  - name: learn_quay_manifest_failure
    description: "Learn from Quay manifest unknown failures"
    condition: "detected_errors and any(e.get('pattern') == 'manifest unknown' for e in detected_errors)"
    tool: learn_tool_fix
    args:
      tool_name: "quay_get_tag"
      error_pattern: "manifest unknown"
      root_cause: "Image not built yet or wrong SHA - Konflux may not have completed build"
      fix_description: "Wait for Konflux build to complete, check with konflux_list_builds()"
    output: quay_manifest_fix_learned
    on_error: continue

  - name: learn_bonfire_pool_failure
    description: "Learn from bonfire pool exhaustion"
    condition: "detected_errors and any(e.get('pattern') == 'no pools available' for e in detected_errors)"
    tool: learn_tool_fix
    args:
      tool_name: "bonfire_namespace_reserve"
      error_pattern: "no pools available"
      root_cause: "All ephemeral namespace pools are exhausted"
      fix_description: "Wait for pools to free up or release unused namespaces with bonfire_namespace_list(mine=True)"
    output: bonfire_pool_fix_learned
    on_error: continue

  - name: learn_k8s_auth_failure
    description: "Learn from Kubernetes auth failures"
    condition: "detected_errors and any(e.get('pattern') == 'unauthorized' and 'bonfire' in e.get('tool', '') for e in detected_errors)"
    tool: learn_tool_fix
    args:
      tool_name: "bonfire_namespace_reserve"
      error_pattern: "unauthorized"
      root_cause: "Kubernetes auth expired for ephemeral cluster"
      fix_description: "Run kube_login(cluster='ephemeral') to refresh credentials"
    output: k8s_auth_fix_learned
    on_error: continue

outputs:
  - name: summary
    value: |
      ## 🧪 Ephemeral Test Environment

      {% if image_check and image_check.get('error') %}
      ### 🛑 STOPPED: Image Not Ready

      **Commit:** `{{ commit_sha[:12] if commit_sha else 'unknown' }}`
      **Message:** {{ image_check.message }}

      **Action Required:** {{ image_check.action }}

      #### Check Image Status
      ```python
      # Via MCP tool:
      {{ image_check.check_command }}

      # Or check Quay directly:
      # https://quay.io/repository/redhat-user-workloads/aap-aa-tenant/aap-aa-main/automation-analytics-backend-main?tag={{ commit_sha }}
      ```

      #### Konflux Build Status
      {% if commit_pipeline and commit_pipeline.commit_found %}
      **Pipeline:** `{{ commit_pipeline.pipeline_name }}`
      **Status:** {{ "🔄 Running" if commit_pipeline.build_status == "running" else ("❌ Failed" if commit_pipeline.build_status == "failed" else "⏳ " + commit_pipeline.build_status) }}

      {% if build_failure_info %}
      {% if build_failure_info.failure_reason %}
      **Failure Reason:** {{ build_failure_info.failure_reason }}
      {% endif %}

      {% if build_failure_info.tasks %}
      **Pipeline Tasks:**
      {% for task in build_failure_info.tasks[:5] %}
      - {{ task }}
      {% endfor %}
      {% endif %}

      {% if build_failure_info.errors %}
      **Build Errors:**
      {% for err in build_failure_info.errors[:5] %}
      - `{{ err }}`
      {% endfor %}
      {% endif %}
      {% endif %}
      {% else %}
      **No build found for this commit yet.** Konflux may not have started the build.
      {% endif %}

      #### Tekton Commands
      ```python
      # List recent pipeline runs
      tkn_pipelinerun_list(namespace='{{ cfg.konflux_namespace }}', limit=10)

      # Get pipeline run details
      tkn_pipelinerun_describe(run_name='PIPELINE_NAME', namespace='{{ cfg.konflux_namespace }}')

      # Get pipeline logs
      tkn_pipelinerun_logs(run_name='PIPELINE_NAME', namespace='{{ cfg.konflux_namespace }}')
      ```

      #### Check Build Status
      ```python
      konflux_list_builds(namespace='{{ cfg.konflux_namespace }}')
      ```

      **DO NOT** try to deploy manually - wait for the image to be built first.

      {% elif not image_status.available %}
      ### ❌ Image Not Available

      **Commit:** `{{ commit_sha[:12] if commit_sha else 'unknown' }}`
      **Status:** {{ image_status.status }}

      The image hasn't been built yet. Check Konflux:
      ```
      konflux_list_builds(namespace='{{ cfg.konflux_namespace }}')
      ```

      **Quay (PR images):** `quay.io/{{ cfg.quay_pr_namespace }}/{{ cfg.quay_pr_repo }}`

      {% else %}
      **Commit:** `{{ commit_sha[:12] }}`
      **Namespace:** `{{ namespace_name }}`
      **Duration:** {{ inputs.duration }}

      ### ClowdApp Selection
      **Component:** `{{ clowdapp_selection.clowdapp }}`
      **Reason:** {{ clowdapp_selection.reason }}
      {% if clowdapp_selection.jira_key %}**Jira:** [{{ clowdapp_selection.jira_key }}]({{ cfg.jira.url }}/browse/{{ clowdapp_selection.jira_key }}){% endif %}
      {% if clowdapp_selection.billing_files %}
      **Billing files changed:**
      {% for f in clowdapp_selection.billing_files %}
      - `{{ f }}`
      {% endfor %}
      {% endif %}

      ### Deployment Status
      {% if deploy_result and '✅' in str(deploy_result) %}
      ✅ Deployed successfully
      {% else %}
      ⚠️ Check deployment status below
      {% endif %}

      ### Pod Status
      ```
      {{ pod_status[:600] if pod_status else 'Checking...' }}
      ```

      {% if failing_pod_check and failing_pod_check.has_failures %}
      ### ⚠️ Pod Issues Detected

      {% for pod in failing_pod_check.failing %}
      - **{{ pod.name }}**: {{ pod.status }} ({{ pod.ready }} ready)
      {% endfor %}

      {% if pod_issue_analysis and pod_issue_analysis.issues %}
      **Diagnosis:**
      {% for issue in pod_issue_analysis.issues %}
      - {{ issue }}
      {% endfor %}
      {% endif %}

      {% if pod_issue_analysis and pod_issue_analysis.events %}
      **Recent Events:**
      {% for event in pod_issue_analysis.events %}
      - {{ event }}
      {% endfor %}
      {% endif %}

      {% endif %}

      {% if db_creds %}
      ### Database Connection
      - **Host:** `{{ db_creds.host }}`
      - **Port:** `{{ db_creds.port }}`
      - **Database:** `{{ db_creds.database }}`
      - **User:** `{{ db_creds.user }}`
      - **Credentials found:** {{ '✅' if db_creds.found else '⚠️ Using defaults' }}

      {% if db_creds.found %}
      **Connection String:**
      ```
      {{ db_creds.connection_string }}
      ```
      {% endif %}
      {% endif %}

      {% if inputs.run_tests and test_result %}
      ### 🧪 Test Results
      {% if test_result.passed %}
      ✅ **Tests passed**
      {% else %}
      ❌ **Tests failed**
      {% endif %}

      ```
      {{ test_result.output[:1000] }}
      ```
      {% endif %}

      {% if related_tests and related_tests.has_related_tests %}
      ---

      ### 🔍 Related Test Files (from semantic search)

      {% for test in related_tests.tests[:5] %}
      - `{{ test.file }}` (similarity: {{ "%.0f"|format(test.score * 100) }}%)
      {% endfor %}
      {% endif %}

      {% if testing_gotchas and testing_gotchas.has_gotchas %}
      ---

      ### ⚠️ Testing Gotchas

      {% for gotcha in testing_gotchas.gotchas[:3] %}
      - {{ gotcha }}
      {% endfor %}
      {% endif %}

      {% if ephemeral_known_issues and ephemeral_known_issues.has_known_issues %}
      ---

      ### 💡 Known Issues to Watch For

      {% for issue in ephemeral_known_issues.issues[:3] %}
      - {{ issue.pattern if issue.pattern else issue }}
      {% endfor %}
      {% endif %}

      ---

      ### Useful Commands

      **Check pods:**
      ```
      kubectl_get_pods(namespace='{{ namespace_name }}', environment='ephemeral')
      ```

      **Get logs:**
      ```
      kubectl_logs(pod_name='{{ fastapi_pod }}', namespace='{{ namespace_name }}', environment='ephemeral')
      ```

      **Extend time:**
      ```
      bonfire_namespace_extend(namespace='{{ namespace_name }}', duration='1h')
      ```

      **Release when done:**
      ```
      bonfire_namespace_release(namespace='{{ namespace_name }}')
      ```
      {% endif %}

  - name: context
    value:
      commit_sha: "{{ commit_sha }}"
      namespace: "{{ namespace_name }}"
      image_available: "{{ image_status.available }}"
      deployed: "{{ deploy_result is not none }}"
      tests_passed: "{{ test_result.passed if test_result else 'not run' }}"
      fastapi_pod: "{{ fastapi_pod }}"
      db_host: "{{ db_creds.host if db_creds else none }}"
