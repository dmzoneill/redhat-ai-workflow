# Skill: Test MR in Ephemeral Environment
# Deploy PR/MR image to ephemeral namespace and run pytest validation
#
# IMPORTANT: This skill uses MCP tools (aa-bonfire, aa-quay). DO NOT run raw bonfire commands.
# The skill handles: kubeconfig, component names, image tags, and bonfire syntax automatically.

name: test_mr_ephemeral
description: |
  Deploy an MR's image to an ephemeral namespace for testing.
  
  ## How It Works
  1. Gets commit SHA from MR (via GitLab MCP)
  2. Checks if Konflux has built the image (via Quay MCP) - STOPS if not ready
  3. Reserves ephemeral namespace (via bonfire MCP)
  4. Deploys using full SHA image tag (via bonfire_deploy_aa)
  5. Runs pytest against ephemeral DB (optional)
  
  ## Prerequisites
  - Konflux must have built the image (check quay.io/redhat-user-workloads/aap-aa-tenant)
  - User must be logged into ephemeral cluster (run `kube e` once)
  
  ## NEVER Do These Things
  - DO NOT copy kubeconfig files (cp ~/.kube/config.e ~/.kube/config)
  - DO NOT run raw `bonfire deploy` without --set-image-tag with FULL SHA
  - DO NOT use short SHA (8 chars like 8d23cab) - images are tagged with FULL 40-char SHA
  - DO NOT truncate the SHA - Quay only has images tagged with full 40-char commit SHA
  
  ## Why Full SHA Matters
  Konflux tags images with the FULL 40-char git commit SHA, not short form.
  - WRONG: quay.io/.../image:8d23cab (manifest unknown!)
  - RIGHT: quay.io/.../image:8d23cab1234567890abcdef1234567890abcdef12
  
  ## Key Config Values (from config.json)
  - App: tower-analytics
  - Component: tower-analytics-clowdapp (main) or tower-analytics-billing-clowdapp (billing)
  - Image base: quay.io/redhat-user-workloads/aap-aa-tenant/aap-aa-main/automation-analytics-backend-main
  
  ## ITS Deploy Pattern (what bonfire needs)
  - template_ref: Full 40-char git commit SHA
  - IMAGE: quay.io/.../image@sha256 (base + @sha256 suffix)
  - IMAGE_TAG: 64-char sha256 digest from Quay (NOT the git SHA!)
  
  The skill automatically extracts the sha256 digest from Quay after checking the image exists.
  
  ## STOP Conditions
  - If image not in Quay: STOP, tell user to wait for Konflux build
  - If namespace reservation fails: STOP, check cluster login
  
  DO NOT fall back to raw bonfire commands - always use the MCP tools.
version: "1.3"

inputs:
  - name: mr_id
    type: integer
    required: false
    description: "GitLab MR ID (will find the image from Konflux)"
  
  - name: commit_sha
    type: string
    required: false
    description: "Specific commit SHA to test (alternative to mr_id)"
  
  - name: duration
    type: string
    required: false
    default: "2h"
    description: "How long to reserve namespace (e.g., 1h, 2h, 4h)"
  
  - name: run_tests
    type: boolean
    required: false
    default: true
    description: "Run pytest against ephemeral environment"
  
  - name: billing
    type: boolean
    required: false
    default: null
    description: |
      Which ClowdApp to deploy:
      - null (default): AUTO-DETECT from Jira issue and commit diff
      - false: Force tower-analytics-clowdapp (main app)
      - true: Force tower-analytics-billing-clowdapp (billing features)
      
      Auto-detection checks:
      1. Jira issue key in commit ‚Üí search for "billing" in issue
      2. Commit modifies aap_billing_controller/ files
      3. Commit modifies test/processor/aap_billing_controller/ files

  - name: cleanup_on_failure
    type: boolean
    required: false
    default: true
    description: "Release namespace if deployment/tests fail"

  - name: cleanup_on_success
    type: boolean
    required: false
    default: false
    description: "Release namespace after successful tests (default: keep for manual testing)"

# No hardcoded constants - resolved from config.json

steps:
  # ==================== LOAD CONFIG ====================

  - name: load_config
    description: "Load Konflux, Quay, and GitLab configuration"
    compute: |
      import json
      from pathlib import Path

      config_paths = [
          Path.cwd() / "config.json",
          Path.home() / "src/redhat-ai-workflow/config.json",
      ]
      config = {}
      for p in config_paths:
          if p.exists():
              with open(p) as f:
                  config = json.load(f)
              break

      repos = config.get("repositories", {})
      quay_config = config.get("quay", {})
      bonfire_config = config.get("bonfire", {})

      # Get automation-analytics-backend config
      aa_config = repos.get("automation-analytics-backend", {})

      result = {
          "gitlab_project": aa_config.get("gitlab", "automation-analytics/automation-analytics-backend"),
          "konflux_namespace": aa_config.get("konflux_namespace", "aap-aa-tenant"),
          "quay_pr_repo": quay_config.get("repositories", {}).get("automation-analytics", "aap-aa-tenant/aap-aa-main/automation-analytics-backend-main"),
          "quay_pr_namespace": quay_config.get("default_namespace", "redhat-user-workloads"),
          "quay_release_namespace": "redhat-services-prod",
          "ephemeral_kubeconfig": bonfire_config.get("kubeconfig", str(Path.home() / ".kube/config.e")),
      }
    output: cfg

  # ==================== PRE-FLIGHT CHECKS ====================

  # Step 0: Check required tools
  - name: check_tools
    description: "Verify bonfire and kubectl are available"
    compute: |
      import shutil

      required = ["bonfire", "kubectl", "oc"]
      missing = []
      available = []

      for tool in required:
          if shutil.which(tool):
              available.append(tool)
          else:
              missing.append(tool)

      # bonfire is required, kubectl/oc need at least one
      if "bonfire" in missing:
          raise ValueError(
              "bonfire not installed. Install with: pip install crc-bonfire"
          )

      if "kubectl" in missing and "oc" in missing:
          raise ValueError(
              "kubectl or oc not installed. Install OpenShift CLI."
          )

      result = {"available": available, "missing": missing}
    output: tools_check

  # ==================== GET COMMIT SHA ====================

  # Step 1: Get commit SHA from MR (if mr_id provided)
  # Gets short SHA from GitLab, then expands to full 40-char using local git
  - name: get_mr_commit
    description: "Get the FULL commit SHA from the MR (40 chars)"
    condition: "{{ inputs.mr_id and not inputs.commit_sha }}"
    compute: |
      import subprocess
      import json
      
      # Get MR info from GitLab (may return short SHA)
      api_result = subprocess.run(
        ["glab", "api", f"projects/:id/merge_requests/{inputs.mr_id}"],
        capture_output=True,
        text=True
      )
      
      short_sha = None
      source_branch = None
      
      if api_result.returncode == 0:
        try:
          mr_data = json.loads(api_result.stdout)
          short_sha = mr_data.get("sha", "")
          source_branch = mr_data.get("source_branch", "")
        except:
          pass
      
      if not short_sha:
        raise ValueError(f"Could not get SHA for MR {inputs.mr_id}")
      
      # Expand to full SHA using local git repo
      expand_result = subprocess.run(
        ["git", "rev-parse", short_sha],
        capture_output=True,
        text=True
      )
      
      if expand_result.returncode != 0:
        # Maybe need to fetch first
        subprocess.run(["git", "fetch", "origin"], capture_output=True)
        expand_result = subprocess.run(
          ["git", "rev-parse", short_sha],
          capture_output=True,
          text=True
        )
      
      full_sha = expand_result.stdout.strip() if expand_result.returncode == 0 else None
      
      if not full_sha or len(full_sha) != 40:
        raise ValueError(f"Could not expand SHA '{short_sha}' to full 40-char. Run 'git fetch origin' and retry.")
      
      result = {"sha": full_sha, "branch": source_branch}
    output: mr_info

  # Step 2: Determine final commit SHA
  - name: resolve_commit
    compute: |
      if inputs.commit_sha:
        sha = inputs.commit_sha
      elif mr_info and mr_info.get('sha'):
        sha = mr_info['sha']
      else:
        sha = None
      result = sha
    output: commit_sha

  # Step 3: Validate we have a commit
  - name: validate_commit
    compute: |
      if not commit_sha:
        raise Exception("Could not determine commit SHA. Provide either mr_id or commit_sha.")
      result = f"Using commit: {commit_sha[:12]}"
    output: commit_status

  # ==================== AUTO-DETECT CLOWDAPP ====================
  # Determine if we should deploy billing or main ClowdApp
  
  # Step 3a: Get commit message and extract Jira issue key
  - name: get_commit_info
    description: "Get commit message to extract Jira issue key"
    compute: |
      import subprocess
      import re
      
      # Get commit message
      result_msg = subprocess.run(
        ["git", "log", "-1", "--format=%s%n%b", commit_sha],
        capture_output=True,
        text=True
      )
      
      commit_message = result_msg.stdout.strip() if result_msg.returncode == 0 else ""
      
      # Extract Jira issue key (AAP-XXXXX pattern)
      jira_match = re.search(r'(AAP-\d+)', commit_message, re.IGNORECASE)
      jira_key = jira_match.group(1).upper() if jira_match else None
      
      # Check for billing keywords in commit message itself
      billing_keywords = ['billing', 'subscription', 'vcpu', 'host_count', 'infra_usage']
      commit_has_billing = any(kw in commit_message.lower() for kw in billing_keywords)
      
      result = {
        "message": commit_message[:200],
        "jira_key": jira_key,
        "commit_mentions_billing": commit_has_billing
      }
    output: commit_info
  
  # Step 3b: Check Jira issue for billing indicators
  - name: check_jira_billing
    description: "Look for billing indicators in Jira issue"
    condition: "{{ commit_info.get('jira_key') }}"
    tool: jira_get_issue
    args:
      issue_key: "{{ commit_info.jira_key }}"
    output: jira_issue
    on_error: continue
  
  # Step 3c: Analyze Jira issue for billing signals
  - name: analyze_jira_billing
    description: "Check if Jira issue mentions billing"
    compute: |
      jira_text = str(jira_issue).lower() if jira_issue else ""
      
      billing_signals = [
        'billing',
        'subscription watch',
        'subscription-watch',
        'vcpu',
        'host count',
        'metering',
        'rhsm',
        'red hat subscription',
        'aap-billing',
        'billing-controller',
        'billing controller',
        'billing exporter',
        'swatch',
      ]
      
      found_signals = [s for s in billing_signals if s in jira_text]
      
      result = {
        "is_billing": len(found_signals) > 0,
        "signals": found_signals[:5],  # Top 5 matches
        "jira_checked": bool(jira_issue)
      }
    output: jira_billing_check
  
  # Step 3d: Check commit diff for billing file changes
  - name: check_commit_diff
    description: "Check if commit modifies billing-related files"
    compute: |
      import subprocess
      
      # Get list of files changed in this commit
      diff_result = subprocess.run(
        ["git", "diff-tree", "--no-commit-id", "--name-only", "-r", commit_sha],
        capture_output=True,
        text=True
      )
      
      changed_files = diff_result.stdout.strip().split('\n') if diff_result.returncode == 0 else []
      
      # Billing-related paths
      billing_paths = [
        'tower_analytics_report/processor/aap_billing_controller',
        'test/processor/aap_billing_controller',
        'aap_billing',
        'billing',
        'subscription_watch',
      ]
      
      billing_files = []
      for f in changed_files:
        for bp in billing_paths:
          if bp in f.lower():
            billing_files.append(f)
            break
      
      result = {
        "total_files_changed": len(changed_files),
        "billing_files_changed": billing_files,
        "has_billing_changes": len(billing_files) > 0
      }
    output: diff_billing_check
  
  # Step 3e: Determine final ClowdApp selection
  - name: determine_clowdapp
    description: "Decide whether to deploy billing or main ClowdApp"
    compute: |
      # If user explicitly specified, use that
      if inputs.billing is not None:
        use_billing = inputs.billing
        reason = "User explicitly specified"
      else:
        # Auto-detect based on signals
        signals = []
        
        # Signal 1: Commit message mentions billing
        if commit_info.get('commit_mentions_billing'):
          signals.append("commit_message")
        
        # Signal 2: Jira issue mentions billing
        if jira_billing_check.get('is_billing'):
          signals.append(f"jira_issue({', '.join(jira_billing_check.get('signals', [])[:2])})")
        
        # Signal 3: Commit changes billing files
        if diff_billing_check.get('has_billing_changes'):
          signals.append(f"files({len(diff_billing_check.get('billing_files_changed', []))} billing files)")
        
        use_billing = len(signals) > 0
        reason = f"Auto-detected: {', '.join(signals)}" if signals else "No billing signals found, using main"
      
      # Select ClowdApp
      if use_billing:
        clowdapp = "tower-analytics-billing-clowdapp"
        template = "clowderapp-billing.yaml"
      else:
        clowdapp = "tower-analytics-clowdapp"
        template = "clowderapp.yaml"
      
      result = {
        "use_billing": use_billing,
        "clowdapp": clowdapp,
        "template": template,
        "reason": reason,
        "jira_key": commit_info.get('jira_key'),
        "billing_files": diff_billing_check.get('billing_files_changed', [])[:5]
      }
    output: clowdapp_selection

  # ==================== CHECK IMAGE EXISTS ====================

  # Step 4: Check if image exists in PR repo (redhat-user-workloads)
  - name: check_quay_image
    description: "Verify the image was built and pushed to Quay (PR repo)"
    tool: quay_get_tag
    args:
      repository: "{{ cfg.quay_pr_repo }}"
      tag: "{{ commit_sha }}"
      namespace: "{{ cfg.quay_pr_namespace }}"
    output: quay_result
    on_error: continue

  # Step 5: If image not found, check Konflux build status
  - name: check_konflux_build
    description: "Check Konflux build status for this commit"
    condition: "{{ not quay_result or 'not found' in str(quay_result).lower() }}"
    tool: konflux_list_builds
    args:
      namespace: "{{ cfg.konflux_namespace }}"
      limit: 10
    output: konflux_builds

  # Step 6: Validate image availability and extract sha256 digest
  - name: validate_image
    compute: |
      import re
      
      quay_output = str(quay_result) if quay_result else ""
      
      if quay_output and 'not found' not in quay_output.lower():
        # Extract the sha256 digest from Manifest Digest line
        # Format: **Manifest Digest:** `sha256:abc123...`
        digest_match = re.search(r'sha256:([a-f0-9]{64})', quay_output)
        if digest_match:
          sha256_digest = digest_match.group(1)  # Just the 64-char hex part
          result = {
            "available": True, 
            "status": "Image ready in Quay",
            "sha256_digest": sha256_digest
          }
        else:
          result = {"available": False, "status": "Could not extract sha256 digest from Quay response"}
      else:
        result = {"available": False, "status": "Image not found - build may be in progress"}
    output: image_status

  # Step 6b: HARD STOP if image not available
  # DO NOT proceed to namespace reservation or deployment if image doesn't exist
  - name: check_image_ready
    description: "STOP if image not built yet"
    compute: |
      if not image_status.get("available", False):
        # Return early with clear message - do NOT try to deploy
        result = {
          "error": True,
          "message": f"STOP: Image for commit {commit_sha[:12] if commit_sha else 'unknown'} not found in Quay.",
          "action": "Wait for Konflux to build the image, then retry.",
          "check_command": f"quay_get_tag(repository='automation-analytics-backend-main', tag='{commit_sha}', namespace='aap-aa-tenant')"
        }
      else:
        result = {"error": False, "message": "Image available, proceeding with deployment"}
    output: image_check

  # ==================== RESERVE NAMESPACE ====================
  # Only proceed if image is available (image_check.error == False)

  # Step 7: Reserve ephemeral namespace
  - name: reserve_namespace
    description: "Reserve an ephemeral namespace"
    condition: "{{ image_status.get('available', False) and not image_check.get('error', True) }}"
    tool: bonfire_namespace_reserve
    args:
      duration: "{{ inputs.duration }}"
      pool: "default"
      timeout: 600
      force: true
    output: namespace_result

  # Step 8: Extract namespace name
  - name: get_namespace_name
    condition: "{{ image_status.get('available', False) and not image_check.get('error', True) }}"
    compute: |
      import re
      
      ns_output = str(namespace_result)
      match = re.search(r'(ephemeral-[a-z0-9]+)', ns_output.lower())
      if match:
        result = match.group(1)
      else:
        result = None
    output: namespace_name

  # ==================== DEPLOY ====================

  # Step 9: Deploy to ephemeral
  # Uses ITS pattern: template_ref = git SHA, image_tag = sha256 digest
  # ClowdApp is auto-detected from Jira/commit or explicitly specified
  - name: deploy_app
    description: "Deploy AA to ephemeral namespace using {{ clowdapp_selection.clowdapp }}"
    condition: "{{ namespace_name and image_status.get('sha256_digest') }}"
    tool: bonfire_deploy_aa
    args:
      namespace: "{{ namespace_name }}"
      template_ref: "{{ commit_sha }}"
      image_tag: "{{ image_status.sha256_digest }}"
      billing: "{{ clowdapp_selection.use_billing }}"
      timeout: 900
    output: deploy_result

  # Step 10: Wait for pods to be ready
  - name: wait_for_ready
    description: "Wait for deployment to be ready"
    condition: "{{ namespace_name and deploy_result }}"
    tool: bonfire_namespace_wait
    args:
      namespace: "{{ namespace_name }}"
      timeout: 300
    output: wait_result
    on_error: continue

  # ==================== GET DB CREDENTIALS ====================

  # Step 11: Get postgres connection info from automation-analytics-db secret
  # The secret contains base64-encoded values that must be decoded
  - name: get_db_credentials
    description: "Get PostgreSQL connection details from automation-analytics-db secret"
    condition: "{{ namespace_name and inputs.run_tests }}"
    compute: |
      import subprocess
      import json
      import base64
      import os
      
      ns = namespace_name
      kubeconfig = cfg.get("ephemeral_kubeconfig", os.path.expanduser("~/.kube/config.e"))
      
      # Get the automation-analytics-db secret directly
      # Secret keys: db.host, db.name, db.password, db.port, db.user
      secret_result = subprocess.run(
        ["kubectl", f"--kubeconfig={kubeconfig}", "get", "secret", 
         "automation-analytics-db", "-n", ns, "-ojson"],
        capture_output=True,
        text=True
      )
      
      db_host = None
      db_port = "5432"
      db_user = None
      db_password = None
      db_name = None
      
      if secret_result.returncode == 0:
        try:
          secret = json.loads(secret_result.stdout)
          data = secret.get("data", {})
          
          # Decode base64 values
          if "db.host" in data:
            db_host = base64.b64decode(data["db.host"]).decode("utf-8")
          elif "hostname" in data:
            db_host = base64.b64decode(data["hostname"]).decode("utf-8")
          
          if "db.port" in data:
            db_port = base64.b64decode(data["db.port"]).decode("utf-8")
          elif "port" in data:
            db_port = base64.b64decode(data["port"]).decode("utf-8")
          
          if "db.user" in data:
            db_user = base64.b64decode(data["db.user"]).decode("utf-8")
          elif "username" in data:
            db_user = base64.b64decode(data["username"]).decode("utf-8")
          
          if "db.password" in data:
            db_password = base64.b64decode(data["db.password"]).decode("utf-8")
          elif "password" in data:
            db_password = base64.b64decode(data["password"]).decode("utf-8")
          
          if "db.name" in data:
            db_name = base64.b64decode(data["db.name"]).decode("utf-8")
          elif "name" in data:
            db_name = base64.b64decode(data["name"]).decode("utf-8")
        except Exception as e:
          pass
      
      result = {
        "host": db_host or f"automation-analytics-db.{ns}.svc",
        "port": db_port,
        "user": db_user or "postgres",
        "password": db_password or "",
        "database": db_name or "tower-analytics",
        "found": bool(db_host and db_user and db_password),
        "connection_string": f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}" if all([db_user, db_password, db_host, db_name]) else None
      }
    output: db_creds
    on_error: continue

  # Step 12: Check pod status
  - name: check_pods
    description: "Get pod status in namespace"
    condition: "{{ namespace_name }}"
    tool: kubectl_get_pods
    args:
      namespace: "{{ namespace_name }}"
      environment: "ephemeral"
    output: pod_status
    on_error: continue

  # Step 13: Get FastAPI pod name
  # Note: Label selector app=automation-analytics-api-fastapi-v2 doesn't work,
  # so we find the pod by name pattern from pod listing
  - name: get_fastapi_pod
    description: "Find the FastAPI pod for running tests"
    condition: "{{ namespace_name and inputs.run_tests }}"
    compute: |
      import subprocess
      import os
      
      ns = namespace_name
      kubeconfig = cfg.get("ephemeral_kubeconfig", os.path.expanduser("~/.kube/config.e"))
      
      # Get pods and find fastapi by name pattern (more reliable than labels)
      result_pods = subprocess.run(
        ["kubectl", f"--kubeconfig={kubeconfig}", "get", "pods", "-n", ns, 
         "-o", "jsonpath={.items[*].metadata.name}"],
        capture_output=True,
        text=True
      )
      
      fastapi_pod = None
      if result_pods.returncode == 0:
        pods = result_pods.stdout.split()
        for pod in pods:
          if 'fastapi' in pod.lower():
            # Verify it's running
            status_check = subprocess.run(
              ["kubectl", f"--kubeconfig={kubeconfig}", "get", "pod", pod, "-n", ns,
               "-o", "jsonpath={.status.phase}"],
              capture_output=True,
              text=True
            )
            if status_check.stdout.strip() == "Running":
              fastapi_pod = pod
              break
      
      result = fastapi_pod
    output: fastapi_pod

  # ==================== RUN TESTS ====================

  # Step 14: Run smoke tests (not full suite - that takes 90 min!)
  # LEARNINGS:
  # - LOG_LEVEL env var causes conflicts (some code expects int, some string) - UNSET it
  # - DATABASE_PREFIX is required by test helpers
  # - Full test suite takes ~90 minutes - only run smoke tests inline
  # - Test directories: test/test_*.py, test/v1/, test/processor/ (no test/restapi/)
  - name: run_pytest
    description: "Run smoke tests in FastAPI pod (full suite = 90min, run separately)"
    condition: "{{ fastapi_pod and db_creds }}"
    compute: |
      import subprocess
      import os
      import textwrap
      
      pod = fastapi_pod
      ns = namespace_name
      db = db_creds
      
      # Extract credentials
      db_host = db.get('host', f'automation-analytics-db.{ns}.svc')
      db_port = db.get('port', '5432')
      db_user = db.get('user', 'postgres')
      db_password = db.get('password', '')
      db_name = db.get('database', 'tower-analytics')
      
      # Build DATABASE_PREFIX (required by test helpers)
      database_prefix = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}"
      database_url = f"{database_prefix}/{db_name}"
      
      # Create smoke test script
      # IMPORTANT: unset LOG_LEVEL to avoid int/string conflict in app code
      test_script = textwrap.dedent(f"""
          #!/bin/bash
          
          # Database connection from automation-analytics-db secret
          export POSTGRESQL_USER="{db_user}"
          export POSTGRESQL_PASSWORD="{db_password}"
          export POSTGRESQL_HOST="{db_host}"
          export POSTGRESQL_PORT="{db_port}"
          export POSTGRESQL_DATABASE="{db_name}"
          
          # SQLAlchemy-style URLs (required by app and tests)
          export DATABASE_URL="{database_url}"
          export DATABASE_PREFIX="{database_prefix}"
          
          # App settings
          export SECRET_KEY="test-secret-key-12345"
          
          # CRITICAL: Unset LOG_LEVEL - app has conflicting code that expects
          # both int and string values, causing ValueError
          unset LOG_LEVEL
          
          cd /opt/app-root/src
          
          echo "=== Smoke Tests (full suite = 90min, run separately) ==="
          echo "DB: {db_host}:{db_port}/{db_name}"
          echo ""
          
          # Run quick smoke tests only (DB, config, basic functionality)
          # These complete in ~10 seconds
          pytest test/test_hello.py test/test_db.py test/test_configurator.py test/test_liveness_check.py -v --tb=short 2>&1
          
          echo ""
          echo "=== Smoke tests complete ==="
          echo "To run full suite (90 min), exec into pod and run:"
          echo "  pytest test/ -v --tb=short"
      """).strip()
      
      # Write script to temp file
      script_path = "/tmp/ephemeral_smoke_test.sh"
      with open(script_path, 'w') as f:
        f.write(test_script)
      os.chmod(script_path, 0o755)
      
      kubeconfig = cfg.get("ephemeral_kubeconfig", os.path.expanduser("~/.kube/config.e"))
      
      # Copy to pod (use -c to specify container in multi-container pod)
      subprocess.run(
        ["kubectl", f"--kubeconfig={kubeconfig}", "cp", script_path, 
         f"{ns}/{pod}:/tmp/smoke_test.sh", "-c", "automation-analytics-api-fastapi-v2"],
        capture_output=True
      )
      
      # Execute with timeout (smoke tests should complete in <30s)
      test_result = subprocess.run(
        ["kubectl", f"--kubeconfig={kubeconfig}", "exec", "-n", ns, pod,
         "-c", "automation-analytics-api-fastapi-v2", "--", "bash", "/tmp/smoke_test.sh"],
        capture_output=True,
        text=True,
        timeout=120  # 2 min timeout for smoke tests
      )
      
      output = test_result.stdout + test_result.stderr
      if len(output) > 3000:
        output = output[-3000:]
      
      # Check for pytest success indicators
      passed = test_result.returncode == 0 or "passed" in output.lower()
      
      result = {
        "passed": passed,
        "output": output,
        "smoke_only": True,
        "full_suite_time": "~90 minutes"
      }
    output: test_result
    on_error: continue

  # ==================== CLEANUP ====================

  # Step 14: Cleanup on failure
  - name: cleanup_on_failure
    description: "Release namespace if deployment or tests failed"
    condition: "namespace_name and inputs.cleanup_on_failure and ((deploy_result and 'fail' in str(deploy_result).lower()) or (test_result and not test_result.get('passed')))"
    tool: bonfire_namespace_release
    args:
      namespace: "{{ namespace_name }}"
    output: cleanup_failure_result
    on_error: continue

  # Step 15: Cleanup on success (if requested)
  - name: cleanup_on_success
    description: "Release namespace after successful tests"
    condition: "namespace_name and inputs.cleanup_on_success and test_result and test_result.get('passed')"
    tool: bonfire_namespace_release
    args:
      namespace: "{{ namespace_name }}"
    output: cleanup_success_result
    on_error: continue

  # Step 16: Determine final cleanup status
  - name: cleanup_status
    description: "Report cleanup status"
    compute: |
      if cleanup_failure_result:
        result = {"cleaned_up": True, "reason": "failure"}
      elif cleanup_success_result:
        result = {"cleaned_up": True, "reason": "success"}
      else:
        result = {"cleaned_up": False, "reason": "kept"}
    output: cleanup_info

  # Step 17: Emit ephemeral hooks
  - name: emit_ephemeral_hooks
    description: "Notify about ephemeral environment status"
    compute: |
      import asyncio
      import sys
      from pathlib import Path
      sys.path.insert(0, str(Path.home() / "src/redhat-ai-workflow"))
      
      try:
          from scripts.skill_hooks import emit_event
          
          # Emit ready notification when deployed
          if namespace_name and deploy_result and 'fail' not in str(deploy_result).lower():
              asyncio.run(emit_event("ephemeral_ready", {
                  "namespace": namespace_name,
                  "mr_id": str(inputs.get('mr_id', '')),
                  "author": "",  # self
              }))
              result = "ephemeral_ready hook sent"
          
          # Emit test failure notification
          if test_result and not test_result.get('passed'):
              asyncio.run(emit_event("ephemeral_tests_failed", {
                  "mr_id": str(inputs.get('mr_id', '')),
                  "namespace": namespace_name or "",
                  "author": "",  # self
              }))
              result = "ephemeral_tests_failed hook sent"
          else:
              result = "no failure hook needed"
      except Exception as e:
          result = f"hook skipped: {e}"
    output: ephemeral_hook_result
    on_error: continue

outputs:
  - name: summary
    value: |
      ## üß™ Ephemeral Test Environment
      
      {% if image_check and image_check.get('error') %}
      ### üõë STOPPED: Image Not Ready
      
      **Commit:** `{{ commit_sha[:12] if commit_sha else 'unknown' }}`
      **Message:** {{ image_check.message }}
      
      **Action Required:** {{ image_check.action }}
      
      #### Check Image Status
      ```python
      # Via MCP tool:
      {{ image_check.check_command }}
      
      # Or check Quay directly:
      # https://quay.io/repository/redhat-user-workloads/aap-aa-tenant/aap-aa-main/automation-analytics-backend-main?tag={{ commit_sha }}
      ```
      
      #### Check Konflux Build
      ```python
      konflux_list_builds(namespace='{{ cfg.konflux_namespace }}')
      ```
      
      **DO NOT** try to deploy manually - wait for the image to be built first.
      
      {% elif not image_status.available %}
      ### ‚ùå Image Not Available
      
      **Commit:** `{{ commit_sha[:12] if commit_sha else 'unknown' }}`
      **Status:** {{ image_status.status }}
      
      The image hasn't been built yet. Check Konflux:
      ```
      konflux_list_builds(namespace='{{ cfg.konflux_namespace }}')
      ```
      
      **Quay (PR images):** `quay.io/{{ cfg.quay_pr_namespace }}/{{ cfg.quay_pr_repo }}`
      
      {% else %}
      **Commit:** `{{ commit_sha[:12] }}`
      **Namespace:** `{{ namespace_name }}`
      **Duration:** {{ inputs.duration }}
      
      ### ClowdApp Selection
      **Component:** `{{ clowdapp_selection.clowdapp }}`
      **Reason:** {{ clowdapp_selection.reason }}
      {% if clowdapp_selection.jira_key %}**Jira:** [{{ clowdapp_selection.jira_key }}](https://issues.redhat.com/browse/{{ clowdapp_selection.jira_key }}){% endif %}
      {% if clowdapp_selection.billing_files %}
      **Billing files changed:**
      {% for f in clowdapp_selection.billing_files %}
      - `{{ f }}`
      {% endfor %}
      {% endif %}
      
      ### Deployment Status
      {% if deploy_result and '‚úÖ' in str(deploy_result) %}
      ‚úÖ Deployed successfully
      {% else %}
      ‚ö†Ô∏è Check deployment status below
      {% endif %}
      
      ### Pod Status
      ```
      {{ pod_status[:600] if pod_status else 'Checking...' }}
      ```
      
      {% if db_creds %}
      ### Database Connection
      - **Host:** `{{ db_creds.host }}`
      - **Port:** `{{ db_creds.port }}`
      - **Database:** `{{ db_creds.database }}`
      - **User:** `{{ db_creds.user }}`
      - **Credentials found:** {{ '‚úÖ' if db_creds.found else '‚ö†Ô∏è Using defaults' }}
      
      {% if db_creds.found %}
      **Connection String:**
      ```
      {{ db_creds.connection_string }}
      ```
      {% endif %}
      {% endif %}
      
      {% if inputs.run_tests and test_result %}
      ### üß™ Test Results
      {% if test_result.passed %}
      ‚úÖ **Tests passed**
      {% else %}
      ‚ùå **Tests failed**
      {% endif %}
      
      ```
      {{ test_result.output[:1000] }}
      ```
      {% endif %}
      
      ---
      
      ### Useful Commands
      
      **Check pods:**
      ```
      kubectl_get_pods(namespace='{{ namespace_name }}', environment='ephemeral')
      ```
      
      **Get logs:**
      ```
      kubectl_logs(pod_name='{{ fastapi_pod }}', namespace='{{ namespace_name }}', environment='ephemeral')
      ```
      
      **Extend time:**
      ```
      bonfire_namespace_extend(namespace='{{ namespace_name }}', duration='1h')
      ```
      
      **Release when done:**
      ```
      bonfire_namespace_release(namespace='{{ namespace_name }}')
      ```
      {% endif %}
  
  - name: context
    value:
      commit_sha: "{{ commit_sha }}"
      namespace: "{{ namespace_name }}"
      image_available: "{{ image_status.available }}"
      deployed: "{{ deploy_result is not none }}"
      tests_passed: "{{ test_result.passed if test_result else 'not run' }}"
      fastapi_pod: "{{ fastapi_pod }}"
      db_host: "{{ db_creds.host if db_creds else none }}"
