# Skill: Morning Coffee Briefing
# Your daily standup assistant - everything you need to start the day
# Enhanced with knowledge and vector search stats

name: coffee
description: |
  Morning briefing - everything you need to know at the start of your work day.

  This skill gathers and summarizes:
  - üìÖ Calendar: Today's meetings with Meet links
  - üìß Email: Unread emails categorized (people vs newsletters)
  - üîÄ PRs: Your open PRs, feedback waiting, failed pipelines
  - üëÄ Reviews: PRs assigned to you for review
  - üìã Jira: Sprint activity for last day/week
  - üöÄ Merges: Recent merged code in aa-backend
  - üß™ Ephemeral: Your active test environments with expiry
  - üìù Yesterday: Your commits (for standup prep)
  - üö® Alerts: Any firing or recent alerts
  - üéØ Actions: Suggested next steps
  - **NEW:** üß† Knowledge: Project knowledge stats and index freshness
  - **NEW:** üîç Vector Search: Index health and search stats

  Requires: Gmail API access (same OAuth as Calendar)
version: "2.0"

inputs:
  - name: full_email_scan
    type: boolean
    required: false
    default: false
    description: "Process all unread emails (vs just summary)"

  - name: auto_archive_email
    type: boolean
    required: false
    default: false
    description: "Automatically archive processed emails"

  - name: days_back
    type: integer
    required: false
    default: 1
    description: "Days to look back for activity (default: 1)"

  - name: slack_format
    type: boolean
    required: false
    default: false
    description: "Use Slack's <URL|Text> link format instead of standard markdown"

steps:
  # ==================== CONFIGURATION ====================

  - name: load_config
    description: "Load configuration using shared loader"
    compute: |
      from datetime import datetime
      from zoneinfo import ZoneInfo
      from scripts.common.config_loader import load_config, get_timezone

      config = load_config()
      tz = ZoneInfo(get_timezone())
      now = datetime.now(tz)

      result = {
        "config": config,
        "now": now.isoformat(),
        "today": now.strftime("%Y-%m-%d"),
        "day_name": now.strftime("%A"),
        "time": now.strftime("%H:%M"),
        "greeting": "Good morning" if now.hour < 12 else "Good afternoon" if now.hour < 18 else "Good evening",
      }
    output: ctx

  # ==================== MEMORY CONTEXT ====================

  - name: load_current_work
    description: "Load current work state from memory"
    compute: |
      # Use shared memory helpers
      current_work = memory.read_memory("state/current_work")
      if not current_work:
          current_work = {"active_issues": [], "open_mrs": [], "follow_ups": []}

      result = {
          "active_issues": current_work.get("active_issues", []),
          "open_mrs": current_work.get("open_mrs", []),
          "follow_ups": current_work.get("follow_ups", []),
      }
    output: memory_work

  - name: load_yesterday_session
    description: "Load yesterday's session log for context"
    compute: |
      from datetime import datetime, timedelta

      yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
      session_log = memory.read_memory(f"sessions/{yesterday}")

      result = {
          "has_session": bool(session_log),
          "actions": session_log.get("actions", [])[-10:] if session_log else [],
      }
    output: yesterday_session

  - name: check_gitlab_known_issues
    description: "Check for known GitLab tool issues"
    tool: check_known_issues
    args:
      tool_name: "gitlab_mr_list"
    output: gitlab_known_issues
    on_error: continue

  - name: check_jira_known_issues
    description: "Check for known Jira tool issues"
    tool: check_known_issues
    args:
      tool_name: "jira_search"
    output: jira_known_issues
    on_error: continue

  - name: check_bonfire_known_issues
    description: "Check for known Bonfire tool issues"
    tool: check_known_issues
    args:
      tool_name: "bonfire"
    output: bonfire_known_issues
    on_error: continue

  - name: aggregate_known_issues
    description: "Aggregate all known issues"
    compute: |
      all_issues = []

      # Parse each known issues result
      for raw in [gitlab_known_issues, jira_known_issues, bonfire_known_issues]:
          if raw:
              raw_text = str(raw)
              # Check if there are matches in the output
              if "Known fixes" in raw_text or "pattern" in raw_text.lower():
                  all_issues.append(raw_text[:200])

      result = {
          "has_known_issues": len(all_issues) > 0,
          "issues": all_issues[:5],
      }
    output: known_issues
    on_error: continue

  # ==================== KNOWLEDGE & VECTOR STATS ====================

  - name: get_vector_stats
    description: "Get vector index statistics"
    compute: |
      # Vector stats are optional - only available if code_search module is loaded
      result = None
      try:
          # Check if vector index exists by looking for metadata file
          from pathlib import Path
          vector_db_path = Path.home() / ".cache" / "aa-workflow" / "vectors" / "automation-analytics-backend"
          metadata_file = vector_db_path / "metadata.json"

          if metadata_file.exists():
              import json
              with open(metadata_file) as f:
                  metadata = json.load(f)

              # Get stats from nested structure
              stats = metadata.get('stats', {})
              search_stats = metadata.get('search_stats', {})

              files = stats.get('files_indexed', 0)
              chunks = stats.get('chunks_created', 0)
              searches = search_stats.get('total_searches', 0)

              result = f"files: {files}, chunks: {chunks}, searches: {searches}"

              # Check index age
              indexed_at = metadata.get('indexed_at')
              if indexed_at:
                  from datetime import datetime, timezone
                  try:
                      indexed_time = datetime.fromisoformat(indexed_at.replace('Z', '+00:00'))
                      if indexed_time.tzinfo is None:
                          indexed_time = indexed_time.replace(tzinfo=timezone.utc)
                      now = datetime.now(timezone.utc)
                      age_hours = (now - indexed_time).total_seconds() / 3600
                      result += f", {age_hours:.1f} hours ago"
                  except Exception:
                      pass

              # Check if file watcher is running (look for PID file)
              watcher_pid_file = Path.home() / ".cache" / "aa-workflow" / "watcher.pid"
              if watcher_pid_file.exists():
                  try:
                      import os
                      pid = int(watcher_pid_file.read_text().strip())
                      # Check if process is running
                      os.kill(pid, 0)  # Signal 0 just checks if process exists
                      result += ", watcher running"
                  except (ProcessLookupError, ValueError, PermissionError):
                      pass
          else:
              result = "not indexed"
      except Exception as e:
          result = f"error: {e}"
    output: vector_stats_raw
    on_error: continue

  - name: parse_vector_stats
    description: "Parse vector index stats"
    compute: |
      import re

      stats = {
          "indexed": False,
          "files": 0,
          "chunks": 0,
          "age_hours": None,
          "searches": 0,
          "watcher_running": False,
      }

      if 'vector_stats_raw' in dir() and vector_stats_raw:
          raw_text = str(vector_stats_raw)

          # Check if indexed
          if "files" in raw_text.lower() and "not found" not in raw_text.lower():
              stats["indexed"] = True

              # Extract stats
              files_match = re.search(r'(\d+)\s*files', raw_text)
              chunks_match = re.search(r'(\d+)\s*chunks', raw_text)
              age_match = re.search(r'(\d+\.?\d*)\s*hours?\s*ago', raw_text)
              searches_match = re.search(r'(\d+)\s*searches', raw_text)

              if files_match:
                  stats["files"] = int(files_match.group(1))
              if chunks_match:
                  stats["chunks"] = int(chunks_match.group(1))
              if age_match:
                  stats["age_hours"] = float(age_match.group(1))
              if searches_match:
                  stats["searches"] = int(searches_match.group(1))

              # Check watcher status
              if "watcher" in raw_text.lower() and "running" in raw_text.lower():
                  stats["watcher_running"] = True

      vector_stats = stats
    output: vector_stats
    on_error: continue

  - name: get_knowledge_stats
    description: "Get knowledge base statistics"
    tool: knowledge_query
    args:
      project: "automation-analytics-backend"
      section: "metadata"
    output: knowledge_stats_raw
    on_error: continue

  - name: parse_knowledge_stats
    description: "Parse knowledge stats"
    compute: |
      import re

      stats = {
          "has_knowledge": False,
          "confidence": 0,
          "gotchas_count": 0,
          "patterns_count": 0,
      }

      if 'knowledge_stats_raw' in dir() and knowledge_stats_raw:
          raw_text = str(knowledge_stats_raw)

          if "confidence" in raw_text.lower() or "gotchas" in raw_text.lower():
              stats["has_knowledge"] = True

              # Extract confidence (handles both int and float like 0.82)
              conf_match = re.search(r'confidence[:\s]+([\d.]+)', raw_text, re.I)
              if conf_match:
                  conf_val = float(conf_match.group(1))
                  # Convert 0-1 scale to percentage if needed
                  if conf_val <= 1:
                      stats["confidence"] = int(conf_val * 100)
                  else:
                      stats["confidence"] = int(conf_val)

              # Count gotchas
              gotchas_count = raw_text.lower().count("gotcha")
              stats["gotchas_count"] = gotchas_count

      knowledge_stats = stats
    output: knowledge_stats
    on_error: continue

  # ==================== CALENDAR ====================

  - name: get_todays_calendar
    description: "Fetch today's calendar events"
    compute: |
      from pathlib import Path
      from datetime import datetime, timedelta
      from zoneinfo import ZoneInfo
      import os

      # Read config_dir from config.json, fallback to default
      gc_config = ctx.get('config', {}).get('google_calendar', {})
      config_dir = gc_config.get('config_dir', '~/.config/google_calendar')
      CONFIG_DIR = Path(os.path.expanduser(config_dir))
      TOKEN_FILE = CONFIG_DIR / "token.json"
      TIMEZONE = gc_config.get('timezone', 'Europe/Dublin')
      tz = ZoneInfo(TIMEZONE)

      events_today = []

      if TOKEN_FILE.exists():
        try:
          from google.oauth2.credentials import Credentials
          from google.auth.transport.requests import Request
          from googleapiclient.discovery import build

          # Load token with its original scopes (don't override)
          creds = Credentials.from_authorized_user_file(str(TOKEN_FILE))

          # Refresh if expired
          if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
            # Save refreshed token
            with open(TOKEN_FILE, 'w') as f:
              f.write(creds.to_json())

          service = build('calendar', 'v3', credentials=creds)

          now = datetime.now(tz)
          day_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
          day_end = day_start + timedelta(days=1)

          events_result = service.events().list(
            calendarId='primary',
            timeMin=day_start.isoformat(),
            timeMax=day_end.isoformat(),
            singleEvents=True,
            orderBy='startTime',
            timeZone=TIMEZONE,
          ).execute()

          for event in events_result.get('items', []):
            start = event['start'].get('dateTime', event['start'].get('date'))
            try:
              if 'T' in start:
                dt = datetime.fromisoformat(start.replace('Z', '+00:00')).astimezone(tz)
                time_str = dt.strftime('%H:%M')
              else:
                time_str = "All day"
            except:
              time_str = start

            # Check for Meet link
            meet_link = ""
            if event.get('conferenceData', {}).get('entryPoints'):
              for entry in event['conferenceData']['entryPoints']:
                if entry.get('entryPointType') == 'video':
                  meet_link = entry.get('uri', '')
                  break

            events_today.append({
              "time": time_str,
              "title": event.get('summary', 'No title'),
              "meet_link": meet_link,
            })
        except Exception as e:
          events_today = [{"error": str(e)}]
      else:
        events_today = [{"error": "Calendar not configured"}]

      result = events_today
    output: calendar_events

  # ==================== EMAIL (Gmail) with Smart Triage ====================

  - name: get_email_summary
    description: "Fetch, analyze, and optionally triage unread emails"
    compute: |
      from pathlib import Path
      import os

      # Read config_dir from config.json, fallback to default
      gc_config = ctx.get('config', {}).get('google_calendar', {})
      config_dir = gc_config.get('config_dir', '~/.config/google_calendar')
      CONFIG_DIR = Path(os.path.expanduser(config_dir))
      TOKEN_FILE = CONFIG_DIR / "token.json"

      # Get user info for "directed at me" detection
      user_email = ctx.get('config', {}).get('user', {}).get('email', '')
      user_name = ctx.get('config', {}).get('user', {}).get('full_name', '')
      first_name = user_name.split()[0].lower() if user_name else ''

      # Should we auto-archive?
      do_archive = inputs.get('auto_archive_email', False)

      email_summary = {
        "unread_count": 0,
        "needs_attention": [],      # Emails directed at me
        "archived": [],             # Emails we auto-archived
        "newsletters": 0,
        "notifications": 0,
        "cc_only": 0,
        "error": None,
      }

      def is_directed_at_me(headers, user_email, first_name):
        """Determine if email is specifically directed at the user."""
        to_field = headers.get('To', '').lower()
        cc_field = headers.get('Cc', '').lower()
        from_field = headers.get('From', '').lower()
        subject = headers.get('Subject', '').lower()

        # Check if I'm in TO (not just CC)
        in_to = user_email.lower() in to_field if user_email else False
        in_cc_only = not in_to and (user_email.lower() in cc_field if user_email else False)

        # Automated sender patterns
        auto_senders = ['noreply', 'no-reply', 'notifications@', 'system@',
                        'mailer-daemon', 'postmaster', 'donotreply', 'automated']
        is_automated = any(pattern in from_field for pattern in auto_senders)

        # Newsletter patterns
        newsletter_patterns = ['newsletter', 'digest', 'weekly', 'daily update',
                               'unsubscribe', 'mailing list', 'marketing']
        is_newsletter = any(pattern in from_field or pattern in subject
                           for pattern in newsletter_patterns)

        # Jira notifications - keep ones assigned to me
        is_jira = 'jira' in from_field
        jira_assigned = is_jira and ('assigned' in subject or first_name in subject)

        # GitLab notifications - keep ones mentioning me or my MRs
        is_gitlab = 'gitlab' in from_field
        gitlab_important = is_gitlab and (first_name in subject or 'mentioned' in subject
                                          or 'assigned' in subject or 'approved' in subject)

        # Decision logic
        if is_newsletter:
          return False, "newsletter"
        elif is_automated and not jira_assigned and not gitlab_important:
          return False, "notification"
        elif in_cc_only and not (first_name and first_name in subject):
          return False, "cc_only"
        else:
          return True, "direct"

      if TOKEN_FILE.exists():
        try:
          from google.oauth2.credentials import Credentials
          from google.auth.transport.requests import Request
          from googleapiclient.discovery import build

          creds = Credentials.from_authorized_user_file(str(TOKEN_FILE))

          if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
            with open(TOKEN_FILE, 'w') as f:
              f.write(creds.to_json())

          service = build('gmail', 'v1', credentials=creds)

          # Get unread messages
          results = service.users().messages().list(
            userId='me',
            q='is:unread in:inbox',
            maxResults=50
          ).execute()

          messages = results.get('messages', [])
          email_summary["unread_count"] = len(messages)

          # Process each message
          for msg in messages[:30]:  # Process up to 30
            msg_data = service.users().messages().get(
              userId='me',
              id=msg['id'],
              format='metadata',
              metadataHeaders=['Subject', 'From', 'To', 'Cc', 'List-Unsubscribe']
            ).execute()

            headers = {h['name']: h['value'] for h in msg_data.get('payload', {}).get('headers', [])}
            subject = headers.get('Subject', 'No subject')
            sender = headers.get('From', 'Unknown')
            msg_id = msg['id']

            # Check if email is directed at me
            is_important, category = is_directed_at_me(headers, user_email, first_name)

            email_info = {
              "id": msg_id,
              "subject": subject[:60],
              "from": sender.split('<')[0].strip()[:30],
              "category": category,
            }

            if is_important:
              email_summary["needs_attention"].append(email_info)
            else:
              # Track by category
              if category == "newsletter":
                email_summary["newsletters"] += 1
              elif category == "notification":
                email_summary["notifications"] += 1
              elif category == "cc_only":
                email_summary["cc_only"] += 1

              # Auto-archive if enabled
              if do_archive:
                try:
                  # Mark as read and archive (remove INBOX label)
                  service.users().messages().modify(
                    userId='me',
                    id=msg_id,
                    body={'removeLabelIds': ['UNREAD', 'INBOX']}
                  ).execute()
                  email_summary["archived"].append(email_info)
                except Exception:
                  pass  # Skip if archive fails

        except Exception as e:
          error_str = str(e)
          if "gmail" in error_str.lower() or "scope" in error_str.lower():
            email_summary["error"] = "Gmail API not enabled. Run /setup-gmail to configure."
          else:
            email_summary["error"] = error_str
      else:
        email_summary["error"] = "Google OAuth not configured"

      result = email_summary
    output: email_summary
    on_error: continue

  # ==================== OPEN PRs (using MCP tools) ====================

  - name: get_open_prs
    description: "Get all open PRs in the repo"
    tool: gitlab_mr_list
    args:
      project: "automation-analytics/automation-analytics-backend"
    output: all_open_prs_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  - name: get_my_prs
    description: "Get PRs authored by me"
    tool: gitlab_mr_list
    args:
      project: "automation-analytics/automation-analytics-backend"
      author: "@me"
    output: my_prs_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  - name: parse_my_prs
    description: "Parse MR list output and detect network errors"
    compute: |
      from scripts.common.parsers import parse_mr_list

      # Check for network errors in the raw output
      network_error = False
      raw_text = str(my_prs_raw) if my_prs_raw else ""
      if "no such host" in raw_text.lower() or "dial tcp" in raw_text.lower():
          network_error = True

      # Store network error state for later use
      ctx['network_error'] = network_error

      result = parse_mr_list(my_prs_raw) if my_prs_raw and not network_error else []
    output: my_prs

  # Get MR IDs for comments check
  - name: get_pr_ids_for_feedback
    description: "Extract MR IDs for feedback check"
    compute: |
      # Get up to 5 MR IDs to check
      pr_ids = [pr['id'] for pr in (my_prs or [])[:5]]
      result = pr_ids
    output: pr_ids_to_check

  # Get comments for MR 1
  - name: get_pr1_comments
    description: "Get comments for first PR"
    condition: "{{ pr_ids_to_check|length > 0 }}"
    tool: gitlab_mr_comments
    args:
      project: "automation-analytics/automation-analytics-backend"
      mr_id: "{{ pr_ids_to_check[0] }}"
    output: pr1_comments_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Get comments for MR 2
  - name: get_pr2_comments
    description: "Get comments for second PR"
    condition: "{{ pr_ids_to_check|length > 1 }}"
    tool: gitlab_mr_comments
    args:
      project: "automation-analytics/automation-analytics-backend"
      mr_id: "{{ pr_ids_to_check[1] }}"
    output: pr2_comments_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Get comments for MR 3
  - name: get_pr3_comments
    description: "Get comments for third PR"
    condition: "{{ pr_ids_to_check|length > 2 }}"
    tool: gitlab_mr_comments
    args:
      project: "automation-analytics/automation-analytics-backend"
      mr_id: "{{ pr_ids_to_check[2] }}"
    output: pr3_comments_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Parse all comments for feedback
  - name: parse_pr_feedback
    description: "Parse all PR comments for feedback"
    compute: |
      # parsers is available from skill engine safe_globals
      feedback = []
      current_user = ctx.get('config', {}).get('user', {}).get('username', os.getenv('USER', 'unknown'))

      # Process each MR's comments
      comments_data = [
        (pr_ids_to_check[0] if pr_ids_to_check else None, pr1_comments_raw if 'pr1_comments_raw' in dir() else None),
        (pr_ids_to_check[1] if len(pr_ids_to_check) > 1 else None, pr2_comments_raw if 'pr2_comments_raw' in dir() else None),
        (pr_ids_to_check[2] if len(pr_ids_to_check) > 2 else None, pr3_comments_raw if 'pr3_comments_raw' in dir() else None),
      ]

      # Map MR IDs back to titles
      mr_titles = {pr['id']: pr.get('title', '')[:40] for pr in (my_prs or [])}

      for mr_id, comments_raw in comments_data:
        if not mr_id or not comments_raw:
          continue

        comments_text = str(comments_raw)

        # Find human comments
        for line in comments_text.split('\n'):
          if ' commented ' in line and current_user.lower() not in line.lower():
            if not parsers.is_bot_comment(line, ''):
              match = re.match(r'^(\w+) commented', line)
              if match:
                feedback.append({
                  "mr_id": mr_id,
                  "author": match.group(1),
                  "title": mr_titles.get(mr_id, '')
                })
                break

      result = feedback
    output: pr_feedback

  # ==================== JIRA ACTIVITY (using MCP tool) ====================

  - name: build_jira_jql
    description: "Build JQL query with proper days_back value"
    compute: |
      days = inputs.get('days_back', 1) if inputs else 1
      # Use labels instead of component (component 'Automation Analytics' doesn't exist)
      result = f"project = AAP AND updated >= -{days}d AND labels = 'automation-analytics' ORDER BY updated DESC"
    output: jira_jql

  - name: get_sprint_activity
    description: "Get Jira activity for the sprint"
    tool: jira_search
    args:
      jql: "{{ jira_jql }}"
      max_results: 20
    output: jira_activity_raw
    on_error: auto_heal  # Jira API - may need auth refresh

  - name: parse_jira_activity
    description: "Parse Jira search results"
    compute: |
      from scripts.common.parsers import parse_jira_issues

      recent_issues = parse_jira_issues(jira_activity_raw) if jira_activity_raw else []

      result = {
        "count": len(recent_issues),
        "issues": recent_issues[:10]
      }
    output: jira_activity
    on_error: continue

  # ==================== MERGED CODE (using MCP tool) ====================

  - name: get_recent_merges
    description: "Get recently merged MRs"
    tool: gitlab_mr_list
    args:
      project: "automation-analytics/automation-analytics-backend"
      state: "merged"
    output: recent_merges_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  - name: parse_recent_merges
    description: "Parse merged MRs"
    compute: |
      from scripts.common.parsers import parse_mr_list

      merged_mrs = parse_mr_list(recent_merges_raw) if recent_merges_raw else []
      result = merged_mrs[:5]  # Last 5
    output: recent_merges
    on_error: continue

  # ==================== ALERTS (using Alertmanager MCP tools) ====================

  - name: check_alerts_stage
    description: "Check for Automation Analytics alerts in stage"
    tool: alertmanager_alerts
    args:
      environment: "stage"
      filter_name: "Automation Analytics"
      silenced: false
    output: alerts_stage_raw
    on_error: auto_heal  # Alertmanager - may need auth

  - name: check_alerts_prod
    description: "Check for Automation Analytics alerts in production"
    tool: alertmanager_alerts
    args:
      environment: "production"
      filter_name: "Automation Analytics"
      silenced: false
    output: alerts_prod_raw
    on_error: auto_heal  # Alertmanager - may need auth

  - name: parse_alerts
    description: "Parse alert results"
    compute: |
      import re

      alerts = {
        "stage": [],
        "production": [],
        "error": None
      }

      # Parse stage alerts
      stage_text = str(alerts_stage_raw) if alerts_stage_raw else ""
      if "No active alerts" in stage_text or not stage_text:
        alerts["stage"] = []
      elif "Failed to get alerts" in stage_text:
        alerts["error"] = "Could not connect to Alertmanager (stage)"
      else:
        # Extract alert names from the output
        for line in stage_text.split('\n'):
          if line.startswith('üî¥') or line.startswith('üü°') or line.startswith('üîµ'):
            match = re.search(r'\*\*(.+?)\*\*', line)
            if match:
              alerts["stage"].append(match.group(1))

      # Parse production alerts
      prod_text = str(alerts_prod_raw) if alerts_prod_raw else ""
      if "No active alerts" in prod_text or not prod_text:
        alerts["production"] = []
      elif "Failed to get alerts" in prod_text:
        if not alerts["error"]:
          alerts["error"] = "Could not connect to Alertmanager (prod)"
      else:
        for line in prod_text.split('\n'):
          if line.startswith('üî¥') or line.startswith('üü°') or line.startswith('üîµ'):
            match = re.search(r'\*\*(.+?)\*\*', line)
            if match:
              alerts["production"].append(match.group(1))

      result = alerts
    output: alerts
    on_error: continue

  # ==================== FAILED PIPELINES ====================

  # Parse .gitlab-ci.yml to identify jobs with allow_failure: true
  - name: get_allow_failure_jobs
    description: "Parse .gitlab-ci.yml to find jobs that allow failure"
    compute: |
      from pathlib import Path
      import yaml

      allow_failure_jobs = set()

      # Get repo path from config
      repo_path = ctx.get('config', {}).get('repositories', {}).get('automation-analytics-backend', {}).get('path', '')
      if not repo_path:
        repo_path = str(Path.home() / "src/automation-analytics-backend")

      ci_file = Path(repo_path) / ".gitlab-ci.yml"

      if ci_file.exists():
        try:
          with open(ci_file) as f:
            ci_config = yaml.safe_load(f)

          # Find all jobs with allow_failure: true
          for job_name, job_config in ci_config.items():
            # Skip non-job keys (stages, default, include, etc.)
            if job_name in ('stages', 'default', 'include', 'variables', 'workflow'):
              continue
            if isinstance(job_config, dict) and job_config.get('allow_failure', False):
              allow_failure_jobs.add(job_name.lower())
        except Exception:
          pass

      result = list(allow_failure_jobs)
    output: allow_failure_jobs

  # Get branch names for CI status check
  - name: get_mr_branches
    description: "Extract branch names from MRs for CI status check"
    compute: |
      # Map MR IDs to branch names (from the parsed MR list)
      mr_branches = {}
      for pr in (my_prs or [])[:5]:
        mr_id = pr.get('id')
        branch = pr.get('branch', '')
        if mr_id and branch:
          mr_branches[mr_id] = branch
      result = mr_branches
    output: mr_branches

  # Get CI status for MR 1
  - name: get_mr1_ci_status
    description: "Get CI status for first MR"
    condition: "{{ pr_ids_to_check|length > 0 and mr_branches.get(pr_ids_to_check[0]) }}"
    tool: gitlab_ci_status
    args:
      project: "automation-analytics/automation-analytics-backend"
      branch: "{{ mr_branches.get(pr_ids_to_check[0], '') }}"
    output: mr1_ci_status_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Get CI status for MR 2
  - name: get_mr2_ci_status
    description: "Get CI status for second MR"
    condition: "{{ pr_ids_to_check|length > 1 and mr_branches.get(pr_ids_to_check[1]) }}"
    tool: gitlab_ci_status
    args:
      project: "automation-analytics/automation-analytics-backend"
      branch: "{{ mr_branches.get(pr_ids_to_check[1], '') }}"
    output: mr2_ci_status_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  # Get CI status for MR 3
  - name: get_mr3_ci_status
    description: "Get CI status for third MR"
    condition: "{{ pr_ids_to_check|length > 2 and mr_branches.get(pr_ids_to_check[2]) }}"
    tool: gitlab_ci_status
    args:
      project: "automation-analytics/automation-analytics-backend"
      branch: "{{ mr_branches.get(pr_ids_to_check[2], '') }}"
    output: mr3_ci_status_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  - name: check_failed_pipelines
    description: "Parse CI status for failed pipelines, excluding allow_failure jobs"
    compute: |
      import re

      failed_pipelines = []

      # Map MR IDs to titles
      mr_titles = {pr['id']: pr.get('title', '')[:40] for pr in (my_prs or [])}

      # Get allow_failure jobs (lowercase for comparison)
      allow_failure_set = set(j.lower() for j in (allow_failure_jobs or []))

      # Check each MR's CI status
      ci_status_list = [
        (pr_ids_to_check[0] if pr_ids_to_check else None, mr1_ci_status_raw if 'mr1_ci_status_raw' in dir() else None),
        (pr_ids_to_check[1] if len(pr_ids_to_check) > 1 else None, mr2_ci_status_raw if 'mr2_ci_status_raw' in dir() else None),
        (pr_ids_to_check[2] if len(pr_ids_to_check) > 2 else None, mr3_ci_status_raw if 'mr3_ci_status_raw' in dir() else None),
      ]

      for mr_id, ci_status_raw in ci_status_list:
        if not mr_id or not ci_status_raw:
          continue

        output = str(ci_status_raw)

        # Parse job statuses from glab ci status output
        # Format: "(failed) ‚Ä¢ 00m 28s	extra		safety"
        # or "(success) ‚Ä¢ 00m 48s	required		black"
        failed_jobs = []
        for line in output.split('\n'):
          # Match lines like "(failed) ‚Ä¢ 00m 28s	extra		jobname"
          match = re.search(r'\((failed|canceled)\)[^a-z]*\w+\s+(\w+)$', line.strip(), re.IGNORECASE)
          if match:
            status = match.group(1).lower()
            job_name = match.group(2).lower()

            # Skip jobs that have allow_failure: true
            if job_name not in allow_failure_set:
              failed_jobs.append(job_name)

        # Only report if there are real (non-allow_failure) failures
        if failed_jobs:
          failed_pipelines.append({
            "mr_id": mr_id,
            "title": mr_titles.get(mr_id, ''),
            "status": "failed",
            "failed_jobs": failed_jobs
          })

      result = failed_pipelines
    output: failed_pipelines

  # ==================== EPHEMERAL NAMESPACES (using MCP tool) ====================

  - name: check_ephemeral_namespaces
    description: "List your active ephemeral environments"
    tool: bonfire_namespace_list
    args:
      mine: true
    output: ephemeral_namespaces_raw
    on_error: auto_heal  # Ephemeral cluster - may need kube_login

  - name: parse_ephemeral_namespaces
    description: "Parse namespace list"
    compute: |
      from scripts.common.parsers import parse_namespaces

      result = parse_namespaces(ephemeral_namespaces_raw) if ephemeral_namespaces_raw else []
    output: ephemeral_namespaces
    on_error: continue

  # ==================== YESTERDAY'S WORK (using MCP tool) ====================

  - name: get_yesterdays_commits_raw
    description: "Get your commits from yesterday (for standup)"
    tool: git_log
    args:
      repo: "automation-analytics-backend"
      limit: 10
      author: "{{ ctx.config.user.username }}"
      since: "yesterday"
      until: "today"
    output: yesterdays_commits_raw
    on_error: continue

  - name: parse_yesterdays_commits
    description: "Parse commit output"
    compute: |
      import re

      commits = []
      raw = str(yesterdays_commits_raw) if yesterdays_commits_raw else ""

      for line in raw.split('\n'):
        # Look for lines like "- `abc1234 commit message`"
        match = re.search(r'`([a-f0-9]{7,})\s+(.+?)`', line)
        if match:
          commits.append({
            "sha": match.group(1)[:7],
            "message": match.group(2)[:60]
          })

      result = commits
    output: yesterdays_commits
    on_error: continue

  # ==================== REVIEW REQUESTS (using MCP tool) ====================

  - name: get_review_requests
    description: "Get PRs where you're assigned as reviewer"
    tool: gitlab_mr_list
    args:
      project: "automation-analytics/automation-analytics-backend"
      reviewer: "@me"
    output: review_requests_raw
    on_error: auto_heal  # GitLab API - may need auth refresh

  - name: parse_review_requests
    description: "Parse review request list"
    compute: |
      from scripts.common.parsers import parse_mr_list

      review_requests = parse_mr_list(review_requests_raw) if review_requests_raw else []
      result = review_requests[:5]  # Limit to 5
    output: review_requests
    on_error: continue

  # ==================== SUMMARY ====================

  - name: format_briefing
    description: "Create the morning briefing"
    compute: |
      import re

      lines = []

      # Get user's first name from config
      full_name = ctx.get('config', {}).get('user', {}).get('full_name', 'there')
      first_name = full_name.split()[0] if full_name else 'there'

      # Base URLs for hyperlinks (inlined to avoid closure issues in sandbox)
      # GitLab: https://gitlab.cee.redhat.com/automation-analytics/automation-analytics-backend/-/merge_requests/{id}
      # Import shared linkify function
      from scripts.common.parsers import linkify_jira_keys

      # Slack vs Markdown link helper
      is_slack = inputs.get('slack_format', True)

      def build_link(url, text):
          if is_slack:
              return f"<{url}|{text}>"
          return f"[{text}]({url})"

      # Header
      lines.append(f"# ‚òï {ctx['greeting']}, {first_name}!")
      lines.append(f"")
      lines.append(f"**{ctx['day_name']}, {ctx['today']}** | {ctx['time']} Irish time")
      lines.append("")
      lines.append("---")
      lines.append("")

      # Current Work from Memory (if any)
      active_issues = memory_work.get('active_issues', []) if memory_work else []
      tracked_mrs = memory_work.get('open_mrs', []) if memory_work else []
      follow_ups = memory_work.get('follow_ups', []) if memory_work else []

      if active_issues or follow_ups:
        lines.append("## üéØ Current Work (from memory)")
        if active_issues:
          for issue in active_issues[:3]:
            key = issue.get('key', 'Unknown')
            summary = issue.get('summary', '')[:40]
            branch = issue.get('branch', '')
            jira_url = f"https://issues.redhat.com/browse/{key}"
            lines.append(f"- **{build_link(jira_url, key)}**: {summary}")
            if branch:
              lines.append(f"  - Branch: `{branch}`")
        if follow_ups:
          lines.append("**Follow-ups:**")
          for fu in follow_ups[:3]:
            task = fu.get('task', 'Unknown task')
            priority = fu.get('priority', 'normal')
            emoji = "üî¥" if priority == "high" else "üü°" if priority == "medium" else "‚ö™"
            lines.append(f"- {emoji} {task}")
        lines.append("")

      # Calendar
      lines.append("## üìÖ Today's Calendar")
      if calendar_events and not any('error' in e for e in calendar_events):
        if calendar_events:
          for event in calendar_events:
            meet_icon = "üìπ" if event.get('meet_link') else ""
            lines.append(f"- **{event['time']}** - {event['title']} {meet_icon}")
        else:
          lines.append("- No meetings scheduled! üéâ")
      else:
        lines.append("- ‚ö†Ô∏è Calendar not accessible")
      lines.append("")

      # Email with smart triage
      lines.append("## üìß Email")
      if email_summary and not email_summary.get('error'):
        needs_attention = email_summary.get('needs_attention', [])
        archived = email_summary.get('archived', [])
        newsletters = email_summary.get('newsletters', 0)
        notifications = email_summary.get('notifications', 0)
        cc_only = email_summary.get('cc_only', 0)

        # Show emails needing attention
        if needs_attention:
          lines.append(f"### ‚ö†Ô∏è Needs Your Attention ({len(needs_attention)})")
          for email in needs_attention[:8]:
            lines.append(f"- **{email['from']}**: *{email['subject']}*")
          if len(needs_attention) > 8:
            lines.append(f"- *...and {len(needs_attention) - 8} more*")
          lines.append("")

        # Show what was archived (if auto-archive was enabled)
        if archived:
          lines.append(f"### ‚úÖ Auto-Archived ({len(archived)})")
          # Group by category
          by_cat = {}
          for email in archived:
            cat = email.get('category', 'other')
            if cat not in by_cat:
              by_cat[cat] = []
            by_cat[cat].append(email)

          for cat, emails in by_cat.items():
            cat_name = {'newsletter': 'üì∞ Newsletters', 'notification': 'üîî Notifications',
                        'cc_only': 'üìã CC-only'}.get(cat, cat)
            lines.append(f"- {cat_name}: {len(emails)}")
          lines.append("")

        # Show what WOULD be archived (if not auto-archiving)
        if not archived and (newsletters or notifications or cc_only):
          skippable = newsletters + notifications + cc_only
          lines.append(f"### üí° Could Auto-Archive ({skippable})")
          if newsletters:
            lines.append(f"- üì∞ Newsletters: {newsletters}")
          if notifications:
            lines.append(f"- üîî Notifications: {notifications}")
          if cc_only:
            lines.append(f"- üìã CC-only: {cc_only}")
          lines.append("")
          lines.append("*Tip: Run with `auto_archive_email: true` to clean these up*")
          lines.append("")

        if not needs_attention and not archived and not newsletters:
          lines.append("‚ú® Inbox is clean!")
          lines.append("")
      else:
        error = email_summary.get('error', 'Unknown') if email_summary else 'Not configured'
        lines.append(f"- ‚ö†Ô∏è {error}")
        lines.append("")

      # PRs needing attention
      lines.append("## üîÄ Your PRs")
      lines.append(f"")
      lines.append(f"**Open PRs:** {len(my_prs) if my_prs else 0}")
      if my_prs:
        for pr in my_prs[:5]:
          title_with_links = linkify_jira_keys(pr['title'], slack_format=is_slack)
          mr_url = f"https://gitlab.cee.redhat.com/automation-analytics/automation-analytics-backend/-/merge_requests/{pr['id']}"
          lines.append(f"- {build_link(mr_url, '!' + str(pr['id']))} - {title_with_links}")
      lines.append("")

      if pr_feedback:
        lines.append(f"**‚ö†Ô∏è Feedback Waiting ({len(pr_feedback)}):**")
        for fb in pr_feedback:
          mr_url = f"https://gitlab.cee.redhat.com/automation-analytics/automation-analytics-backend/-/merge_requests/{fb['mr_id']}"
          lines.append(f"- {build_link(mr_url, '!' + str(fb['mr_id']))} - Comment from **{fb['author']}**")
      else:
        lines.append("‚úÖ No pending feedback on your PRs")
      lines.append("")

      # Failed pipelines (excluding allow_failure jobs)
      if failed_pipelines:
        lines.append(f"**üî¥ Failed Pipelines ({len(failed_pipelines)}):**")
        for fp in failed_pipelines:
          mr_url = f"https://gitlab.cee.redhat.com/automation-analytics/automation-analytics-backend/-/merge_requests/{fp['mr_id']}"
          failed_jobs_str = ", ".join(fp.get('failed_jobs', [])[:3])
          if len(fp.get('failed_jobs', [])) > 3:
            failed_jobs_str += f" +{len(fp['failed_jobs']) - 3} more"
          lines.append(f"- {build_link(mr_url, '!' + str(fp['mr_id']))} - {failed_jobs_str or fp['status'].upper()}")
        lines.append("")
      else:
        # Show that we're filtering allow_failure jobs
        if allow_failure_jobs:
          lines.append(f"‚úÖ No blocking pipeline failures (ignoring {len(allow_failure_jobs)} optional jobs)")
          lines.append("")

      # Review requests - PRs assigned to YOU for review
      lines.append("## üëÄ Review Requests")
      if review_requests:
        lines.append(f"**Assigned to you:** {len(review_requests)}")
        for rr in review_requests:
          title_with_links = linkify_jira_keys(rr['title'], slack_format=is_slack)
          mr_url = f"https://gitlab.cee.redhat.com/automation-analytics/automation-analytics-backend/-/merge_requests/{rr['id']}"
          lines.append(f"- {build_link(mr_url, '!' + str(rr['id']))} - {title_with_links}")
      else:
        lines.append("‚úÖ No PRs waiting for your review")
      lines.append("")

      # Jira
      lines.append("## üìã Jira Activity")
      if jira_activity and jira_activity.get('count', 0) > 0:
        lines.append(f"**{jira_activity.get('count', 0)}** issues updated in last {inputs.get('days_back', 1)} day(s)")
        for issue in jira_activity.get('issues', [])[:5]:
          jira_url = f"https://issues.redhat.com/browse/{issue['key']}"
          lines.append(f"- {build_link(jira_url, issue['key'])} - {issue['summary']}")
      else:
        lines.append("- No Jira activity in the last day")
      lines.append("")

      # Merges
      lines.append("## üöÄ Recent Merges")
      if recent_merges:
        for mr in recent_merges:
          title_with_links = linkify_jira_keys(mr['title'], slack_format=is_slack)
          mr_url = f"https://gitlab.cee.redhat.com/automation-analytics/automation-analytics-backend/-/merge_requests/{mr['id']}"
          lines.append(f"- {build_link(mr_url, '!' + str(mr['id']))} - {title_with_links}")
      else:
        lines.append("- No recent merges")
      lines.append("")

      # Ephemeral Environments
      lines.append("## üß™ Ephemeral Environments")
      if ephemeral_namespaces:
        for ns in ephemeral_namespaces:
          expires = ns.get('expires', 'unknown')
          lines.append(f"- **{ns['name']}** - expires {expires}")
      else:
        lines.append("- No active ephemeral environments")
      lines.append("")

      # Yesterday's Work (for standup)
      lines.append("## üìù Yesterday's Work")
      if yesterdays_commits:
        for commit in yesterdays_commits[:5]:
          lines.append(f"- `{commit['sha']}` {commit['message']}")
      else:
        lines.append("- No commits yesterday")
      lines.append("")

      # Alerts
      lines.append("## üö® Alerts")
      if alerts and not alerts.get('error'):
        if alerts.get('production') or alerts.get('stage'):
          for env, alert_list in alerts.items():
            if alert_list and env != 'error':
              lines.append(f"**{env.upper()}:**")
              for a in alert_list:
                lines.append(f"- {a}")
        else:
          lines.append("‚úÖ No active alerts")
      else:
        lines.append("- Alert check not configured")
      lines.append("")

      # Knowledge & Vector Stats
      lines.append("## üß† Knowledge & Search")

      # Knowledge stats
      if 'knowledge_stats' in dir() and knowledge_stats and knowledge_stats.get('has_knowledge'):
        conf = knowledge_stats.get('confidence', 0)
        conf_bar = "‚ñà" * (conf // 10) + "‚ñë" * (10 - conf // 10)
        lines.append(f"**Knowledge Confidence:** {conf_bar} {conf}%")
        if knowledge_stats.get('gotchas_count', 0) > 0:
          lines.append(f"- {knowledge_stats['gotchas_count']} gotchas documented")
      else:
        lines.append("- No project knowledge yet. Run `skill_run('bootstrap_knowledge')` to initialize.")

      # Vector stats
      if 'vector_stats' in dir() and vector_stats and vector_stats.get('indexed'):
        lines.append(f"**Vector Index:** {vector_stats.get('files', 0)} files, {vector_stats.get('chunks', 0)} chunks")
        if vector_stats.get('age_hours') is not None:
          age = vector_stats['age_hours']
          if age < 1:
            age_str = "< 1 hour ago"
          elif age < 24:
            age_str = f"{age:.0f} hours ago"
          else:
            age_str = f"{age/24:.1f} days ago"
          lines.append(f"- Last updated: {age_str}")
        if vector_stats.get('searches', 0) > 0:
          lines.append(f"- {vector_stats['searches']} searches performed")
        if vector_stats.get('watcher_running'):
          lines.append("- üü¢ File watcher running (auto-updates)")
        else:
          lines.append("- üî¥ File watcher not running")
      else:
        lines.append("- No vector index. Run `skill_run('bootstrap_knowledge')` to create.")
      lines.append("")

      # Network error warning
      network_error = ctx.get('network_error', False)
      if network_error:
        lines.append("## ‚ö†Ô∏è Network Issues Detected")
        lines.append("")
        lines.append("GitLab tools failed with DNS errors. This usually means:")
        lines.append("1. **VPN not connected** - Connect to Red Hat VPN")
        lines.append("2. **DNS resolution issue** - Internal GitLab isn't reachable")
        lines.append("")
        lines.append("**To fix:** Run `vpn_connect()` or connect manually.")
        lines.append("")

      # Actions
      lines.append("---")
      lines.append("")
      lines.append("## üéØ Suggested Actions")
      actions = []

      # VPN action first if network error
      if network_error:
        actions.append("- üîå **Connect to VPN** - Run `vpn_connect()` to access GitLab")

      if pr_feedback:
        actions.append(f"- Respond to feedback on {len(pr_feedback)} PR(s)")
      if failed_pipelines:
        actions.append(f"- Fix {len(failed_pipelines)} failed pipeline(s)")
      if review_requests:
        actions.append(f"- Review {len(review_requests)} PR(s) assigned to you")

      # Email actions
      needs_attention = email_summary.get('needs_attention', []) if email_summary else []
      if needs_attention:
        actions.append(f"- Review {len(needs_attention)} email(s) needing attention")

      # Suggest auto-archive if there's cleanup potential
      newsletters = email_summary.get('newsletters', 0) if email_summary else 0
      notifications = email_summary.get('notifications', 0) if email_summary else 0
      cc_only = email_summary.get('cc_only', 0) if email_summary else 0
      skippable = newsletters + notifications + cc_only
      if skippable >= 5 and not email_summary.get('archived'):
        actions.append(f"- üí° Run `/coffee` with `auto_archive_email: true` to clean up {skippable} emails")

      if ephemeral_namespaces:
        expiring = [ns for ns in ephemeral_namespaces if 'h' in str(ns.get('expires', '')) and int(re.search(r'(\d+)h', str(ns.get('expires', '0h'))).group(1) if re.search(r'(\d+)h', str(ns.get('expires', ''))) else 99) < 2]
        if expiring:
          actions.append(f"- ‚è∞ {len(expiring)} ephemeral env(s) expiring soon!")

      if actions:
        for a in actions:
          lines.append(a)
      else:
        lines.append("- You're all caught up! ‚òï")

      result = '\n'.join(lines)
    output: briefing

  # ==================== MEMORY UPDATES ====================

  - name: log_coffee_session
    description: "Log morning briefing to session"
    tool: memory_session_log
    args:
      action: "Morning coffee briefing"
      details: "PRs: {{ my_prs | length if my_prs else 0 }}, Reviews: {{ review_requests | length if review_requests else 0 }}, Failed: {{ failed_pipelines | length if failed_pipelines else 0 }}"
    on_error: continue

  - name: update_work_state
    description: "Sync memory with actual PR/issue state"
    compute: |
      from datetime import datetime

      # Update current work with fresh data
      current_work = memory.read_memory("state/current_work") or {}

      # Sync open MRs from GitLab
      mr_ids = [pr.get('id') for pr in (my_prs or [])]
      current_work["open_mrs"] = [
          {"id": pr.get('id'), "title": pr.get('title', '')[:50], "status": pr.get('status', 'open')}
          for pr in (my_prs or [])[:10]
      ]

      # Track feedback waiting
      current_work["feedback_waiting"] = [
          {"mr_id": fb.get('mr_id'), "from": fb.get('author')}
          for fb in (pr_feedback or [])
      ]

      # Track review assignments
      current_work["reviews_assigned"] = [
          {"id": rr.get('id'), "title": rr.get('title', '')[:40]}
          for rr in (review_requests or [])[:5]
      ]

      current_work["last_coffee"] = datetime.now().isoformat()

      memory.write_memory("state/current_work", current_work)
      result = "work state synced"
    output: work_sync_result
    on_error: continue

  - name: learn_pipeline_failures
    description: "Learn from pipeline failures for future reference"
    condition: "failed_pipelines and len(failed_pipelines) > 0"
    compute: |
      from datetime import datetime

      # Load patterns
      patterns = memory.read_memory("learned/patterns") or {}
      if "pipeline_failures" not in patterns:
          patterns["pipeline_failures"] = []

      # Track recurring failures
      for fp in (failed_pipelines or [])[:5]:
          jobs = fp.get("failed_jobs", [])
          for job in jobs:
              existing = [p for p in patterns["pipeline_failures"] if p.get("job") == job]
              if existing:
                  existing[0]["count"] = existing[0].get("count", 1) + 1
                  existing[0]["last_seen"] = datetime.now().isoformat()
              else:
                  patterns["pipeline_failures"].append({
                      "job": job,
                      "count": 1,
                      "first_seen": datetime.now().isoformat(),
                      "last_seen": datetime.now().isoformat(),
                  })

      # Keep only top 20 by count
      patterns["pipeline_failures"] = sorted(
          patterns["pipeline_failures"],
          key=lambda x: x.get("count", 0),
          reverse=True
      )[:20]

      memory.write_memory("learned/patterns", patterns)
      result = "pipeline failure patterns updated"
    output: pattern_learn_result
    on_error: continue

  - name: save_shared_context
    description: "Save context for other skills to use"
    compute: |
      from datetime import datetime

      # Share context that other skills can use
      shared = memory.read_memory("state/shared_context") or {}

      shared["last_coffee"] = {
          "timestamp": datetime.now().isoformat(),
          "pr_count": len(my_prs) if my_prs else 0,
          "feedback_waiting": len(pr_feedback) if pr_feedback else 0,
          "failed_pipelines": len(failed_pipelines) if failed_pipelines else 0,
          "review_assignments": len(review_requests) if review_requests else 0,
          "active_ephemeral": len(ephemeral_namespaces) if ephemeral_namespaces else 0,
          "alerts": {
              "stage": len(alerts.get("stage", [])) if alerts else 0,
              "production": len(alerts.get("production", [])) if alerts else 0,
          }
      }

      memory.write_memory("state/shared_context", shared)
      result = "shared context saved"
    output: shared_context_result
    on_error: continue

  - name: learn_network_failure
    description: "Learn from network/VPN failures if detected"
    condition: "ctx.get('network_error', False)"
    tool: learn_tool_fix
    args:
      tool_name: "gitlab_mr_list"
      error_pattern: "no such host"
      root_cause: "VPN not connected - internal GitLab not reachable"
      fix_description: "Run vpn_connect() to connect to Red Hat VPN"
    output: network_fix_learned
    on_error: continue

  - name: learn_jira_failure
    description: "Learn from Jira failures if detected"
    condition: "'HTTP error 400' in str(jira_activity_raw) if jira_activity_raw else False"
    tool: learn_tool_fix
    args:
      tool_name: "jira_search"
      error_pattern: "HTTP error 400"
      root_cause: "JQL query syntax error or invalid field"
      fix_description: "Check JQL syntax - labels field may need quotes around values with special chars"
    output: jira_fix_learned
    on_error: continue

outputs:
  - name: summary
    value: "{{ briefing }}"

  - name: context
    value:
      calendar_count: "{{ calendar_events | length if calendar_events else 0 }}"
      unread_emails: "{{ email_summary.unread_count if email_summary else 0 }}"
      my_prs: "{{ my_prs | length if my_prs else 0 }}"
      feedback_waiting: "{{ pr_feedback | length if pr_feedback else 0 }}"
      failed_pipelines: "{{ failed_pipelines | length if failed_pipelines else 0 }}"
      review_requests: "{{ review_requests | length if review_requests else 0 }}"
      ephemeral_envs: "{{ ephemeral_namespaces | length if ephemeral_namespaces else 0 }}"
      yesterdays_commits: "{{ yesterdays_commits | length if yesterdays_commits else 0 }}"
      jira_updates: "{{ jira_activity.count if jira_activity else 0 }}"
      recent_merges: "{{ recent_merges | length if recent_merges else 0 }}"
