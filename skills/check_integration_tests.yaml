# Skill: Check Integration Tests
# Review Konflux integration test status

name: check_integration_tests
description: |
  Check Konflux integration test status and results.

  This skill:
  - Lists integration test runs
  - Gets test results
  - Shows snapshots

  Uses: konflux_list_integration_tests, konflux_get_test_results, konflux_list_snapshots,
        konflux_get_snapshot
version: "1.0"

inputs:
  - name: namespace
    type: string
    required: false
    default: "aap-aa-tenant"
    description: "Konflux namespace"

  - name: limit
    type: integer
    required: false
    default: 10
    description: "Number of results to show"

steps:
  # ==================== KNOWLEDGE INTEGRATION ====================

  - name: check_test_known_issues
    description: "Check for known Konflux/test issues"
    compute: |
      # Check known issues for test operations
      konflux_issues = memory.check_known_issues("konflux", "") or {}
      test_issues = memory.check_known_issues("integration_test", "") or {}
      tekton_issues = memory.check_known_issues("tekton", "") or {}

      all_issues = []
      for issues in [konflux_issues, test_issues, tekton_issues]:
          if issues and issues.get("matches"):
              all_issues.extend(issues.get("matches", [])[:2])

      result = {
          "has_known_issues": len(all_issues) > 0,
          "issues": all_issues[:5],
      }
    output: test_known_issues
    on_error: continue

  - name: get_testing_gotchas
    description: "Get testing-related gotchas from knowledge"
    tool: knowledge_query
    args:
      project: "automation-analytics-backend"
      persona: "developer"
      section: "gotchas"
    output: testing_gotchas_raw
    on_error: continue

  - name: parse_testing_gotchas
    description: "Parse testing-related gotchas"
    compute: |
      gotchas_result = testing_gotchas_raw if 'testing_gotchas_raw' in dir() and testing_gotchas_raw else {}

      testing_gotchas = []
      if isinstance(gotchas_result, dict) and gotchas_result.get('found'):
          content = gotchas_result.get('content', [])
          if isinstance(content, list):
              # Filter for testing-related gotchas
              for g in content:
                  g_str = str(g).lower()
                  if any(kw in g_str for kw in ['test', 'integration', 'konflux', 'snapshot', 'ci']):
                      testing_gotchas.append(g)

      result = {
          'gotchas': testing_gotchas[:5],
          'has_gotchas': len(testing_gotchas) > 0,
      }
    output: integration_test_gotchas
    on_error: continue

  # ==================== LIST TESTS ====================

  - name: list_tests
    description: "List integration test runs"
    tool: konflux_list_integration_tests
    args:
      namespace: "{{ inputs.namespace }}"
    output: tests_raw
    on_error: auto_heal  # Konflux cluster - may need kube_login

  - name: parse_tests
    description: "Parse test list"
    compute: |
      tests_text = str(tests_raw) if 'tests_raw' in dir() and tests_raw else ""

      import re
      tests = []

      for line in tests_text.split("\n"):
        line = line.strip()
        if line and not line.startswith("NAME"):
          parts = line.split()
          if parts:
            name = parts[0]
            status = "unknown"
            if "pass" in line.lower() or "success" in line.lower():
              status = "passed"
            elif "fail" in line.lower():
              status = "failed"
            elif "running" in line.lower():
              status = "running"

            tests.append({"name": name, "status": status, "raw": line[:80]})

      result = {
        "tests": tests[:inputs.limit],
        "count": len(tests),
        "passed": len([t for t in tests if t["status"] == "passed"]),
        "failed": len([t for t in tests if t["status"] == "failed"]),
        "running": len([t for t in tests if t["status"] == "running"]),
      }
    output: tests_info

  - name: get_first_failed_results
    description: "Get results of first failed test"
    condition: "tests_info.failed > 0"
    tool: konflux_get_test_results
    args:
      name: "{{ tests_info.tests[0].name if tests_info.tests else '' }}"
      namespace: "{{ inputs.namespace }}"
    output: failed_results_raw
    on_error: auto_heal  # Konflux cluster - may need kube_login

  - name: list_snapshots
    description: "List recent snapshots"
    tool: konflux_list_snapshots
    args:
      namespace: "{{ inputs.namespace }}"
    output: snapshots_raw
    on_error: auto_heal  # Konflux cluster - may need kube_login

  - name: parse_snapshots
    description: "Parse snapshot list"
    compute: |
      snapshots_text = str(snapshots_raw) if 'snapshots_raw' in dir() and snapshots_raw else ""

      snapshots = []
      for line in snapshots_text.split("\n"):
        line = line.strip()
        if line and not line.startswith("NAME"):
          parts = line.split()
          if parts:
            snapshots.append(parts[0])

      result = {
        "snapshots": snapshots[:10],
        "count": len(snapshots),
      }
    output: snapshots_info

  # ==================== LEARNING FROM FAILURES ====================

  - name: detect_integration_test_failures
    description: "Detect failure patterns from integration test operations"
    compute: |
      errors_detected = []

      # Check Konflux/k8s failures
      tests_text = str(tests_raw) if 'tests_raw' in dir() and tests_raw else ""
      snapshots_text = str(snapshots_raw) if 'snapshots_raw' in dir() and snapshots_raw else ""
      combined = tests_text + snapshots_text

      if "unauthorized" in combined.lower() or "forbidden" in combined.lower():
          errors_detected.append({
              "tool": "konflux_list_integration_tests",
              "pattern": "unauthorized",
              "cause": "Kubernetes auth expired for Konflux cluster",
              "fix": "Run kube_login(cluster='konflux') to refresh credentials"
          })
      if "no route to host" in combined.lower():
          errors_detected.append({
              "tool": "konflux_list_integration_tests",
              "pattern": "no route to host",
              "cause": "VPN not connected - cannot reach Konflux cluster",
              "fix": "Run vpn_connect() to connect to Red Hat VPN"
          })

      result = errors_detected
    output: integration_test_errors_detected
    on_error: continue

  - name: learn_integration_test_auth_failure
    description: "Learn from Konflux auth failures"
    condition: "integration_test_errors_detected and any(e.get('pattern') == 'unauthorized' for e in integration_test_errors_detected)"
    tool: learn_tool_fix
    args:
      tool_name: "konflux_list_integration_tests"
      error_pattern: "unauthorized"
      root_cause: "Kubernetes auth expired for Konflux cluster"
      fix_description: "Run kube_login(cluster='konflux') to refresh credentials"
    output: integration_test_auth_fix_learned
    on_error: continue

  - name: log_session
    description: "Log skill execution to session"
    tool: memory_session_log
    args:
      action: "Checked integration tests"
      details: "namespace={{ inputs.namespace }}, tests={{ tests_info.total if tests_info is defined else 'unknown' }}"
    on_error: continue

  - name: track_test_checks
    description: "Track integration test checks for patterns"
    compute: |
      from datetime import datetime

      # Load patterns
      patterns = memory.read_memory("learned/patterns") or {}
      if "integration_test_checks" not in patterns:
          patterns["integration_test_checks"] = []

      # Record this check
      check_record = {
          "namespace": inputs.namespace,
          "total_tests": tests_info.count if tests_info else 0,
          "passed": tests_info.passed if tests_info else 0,
          "failed": tests_info.failed if tests_info else 0,
          "running": tests_info.running if tests_info else 0,
          "snapshots": snapshots_info.count if snapshots_info else 0,
          "timestamp": datetime.now().isoformat(),
      }

      patterns["integration_test_checks"].append(check_record)

      # Keep last 100 checks
      patterns["integration_test_checks"] = patterns["integration_test_checks"][-100:]

      memory.write_memory("learned/patterns", patterns)
      result = "test check tracked"
    output: test_tracking_result
    on_error: continue

  - name: track_failed_tests
    description: "Track failed tests for flakiness detection"
    condition: "tests_info and tests_info.failed > 0"
    compute: |
      from datetime import datetime

      # Load patterns
      patterns = memory.read_memory("learned/patterns") or {}
      if "failed_integration_tests" not in patterns:
          patterns["failed_integration_tests"] = []

      # Track each failed test
      for test in (tests_info.tests or []):
          if test.get("status") == "failed":
              existing = [t for t in patterns["failed_integration_tests"] if t.get("name") == test.get("name")]

              if existing:
                  existing[0]["fail_count"] = existing[0].get("fail_count", 1) + 1
                  existing[0]["last_failed"] = datetime.now().isoformat()
              else:
                  patterns["failed_integration_tests"].append({
                      "name": test.get("name"),
                      "namespace": inputs.namespace,
                      "fail_count": 1,
                      "first_failed": datetime.now().isoformat(),
                      "last_failed": datetime.now().isoformat(),
                  })

      # Keep top 50 by fail count
      patterns["failed_integration_tests"] = sorted(
          patterns["failed_integration_tests"],
          key=lambda x: x.get("fail_count", 0),
          reverse=True
      )[:50]

      memory.write_memory("learned/patterns", patterns)
      result = "failed tests tracked"
    output: failed_test_tracking_result
    on_error: continue

outputs:
  - name: report
    value: |
      ## ğŸ§ª Integration Tests: {{ inputs.namespace }}

      ---

      ### Summary

      | Status | Count |
      |--------|-------|
      | âœ… Passed | {{ tests_info.passed }} |
      | âŒ Failed | {{ tests_info.failed }} |
      | ğŸ”„ Running | {{ tests_info.running }} |
      | **Total** | {{ tests_info.count }} |

      ---

      ### Recent Test Runs

      {% for test in tests_info.tests %}
      - {{ "âœ…" if test.status == "passed" else ("âŒ" if test.status == "failed" else "ğŸ”„") }} `{{ test.name }}` - {{ test.status }}
      {% endfor %}

      {% if tests_info.failed > 0 %}
      ---

      ### âŒ Failed Test Details

      **Test:** `{{ tests_info.tests[0].name if tests_info.tests else 'unknown' }}`

      ```
      {{ failed_results_raw[:1000] if failed_results_raw else 'Could not fetch results' }}
      ```

      **Get full results:**
      ```
      konflux_get_test_results(name='{{ tests_info.tests[0].name if tests_info.tests else "" }}', namespace='{{ inputs.namespace }}')
      ```
      {% endif %}

      ---

      ### ğŸ“¸ Recent Snapshots ({{ snapshots_info.count }})

      {% for snapshot in snapshots_info.snapshots[:5] %}
      - `{{ snapshot }}`
      {% endfor %}

      **Get snapshot details:**
      ```
      konflux_get_snapshot(name='<snapshot-name>', namespace='{{ inputs.namespace }}')
      ```

      {% if integration_test_gotchas and integration_test_gotchas.has_gotchas %}
      ---

      ### âš ï¸ Testing Gotchas

      {% for gotcha in integration_test_gotchas.gotchas[:3] %}
      - {{ gotcha }}
      {% endfor %}
      {% endif %}

      {% if test_known_issues and test_known_issues.has_known_issues %}
      ---

      ### ğŸ’¡ Known Issues

      {% for issue in test_known_issues.issues[:3] %}
      - {{ issue.pattern if issue.pattern else issue }}
      {% endfor %}
      {% endif %}
