# Memory & Auto-Remediation: Improvement Roadmap

> Identified gaps, potential enhancements, and recommended improvements

## üî¥ Critical Issues (Fix Now)

### ‚úÖ 1. Race Conditions in Memory Writes - IMPLEMENTED

**Status:** ‚úÖ Completed (2026-01-09)

**Implementation:** Added fcntl file locking to 3 critical functions in `scripts/common/memory.py`:
- `append_to_list()` - Atomic append/update with exclusive lock
- `remove_from_list()` - Atomic remove with exclusive lock
- `update_field()` - Atomic field update with exclusive lock

**How it works:**
```python
import fcntl

def append_to_list(key, list_path, item, match_key=None):
    path = get_memory_path(key)

    with open(path, 'r+' if path.exists() else 'w+') as f:
        # Acquire exclusive lock (blocks until available)
        fcntl.flock(f.fileno(), fcntl.LOCK_EX)

        try:
            # Atomic read-modify-write
            data = yaml.safe_load(f.read()) or {}
            # ... modify data ...
            f.seek(0)
            f.truncate()
            yaml.dump(data, f)
        finally:
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
```

**Impact:** Prevents data loss in concurrent skill executions. Multiple skills can now safely modify memory files simultaneously without overwriting each other's changes.

---

### ‚úÖ 2. Missing Memory Backup Strategy - IMPLEMENTED

**Status:** ‚úÖ Completed (2026-01-09)

**Implementation:** Added `backup_before_init` step to `skills/memory_init.yaml`.

**What was added:**
- Timestamped backup creation (YYYYMMDD_HHMMSS format)
- Backs up 8 critical memory files:
  - state/current_work.yaml
  - state/environments.yaml
  - learned/patterns.yaml
  - learned/tool_fixes.yaml
  - learned/tool_failures.yaml
  - learned/runbooks.yaml
  - learned/service_quirks.yaml
  - learned/teammate_preferences.yaml
- Keeps last 10 backups automatically (deletes older ones)
- Shows backup location and file count in output
- Backup happens BEFORE any files are wiped (critical!)

**Location:** `memory/backups/YYYYMMDD_HHMMSS/`

**Impact:** Prevents data loss from accidental memory_init. User can restore from `memory/backups/` if needed.

**Note:** Pre-commit hook for automatic backups not implemented yet (P2 priority).

---

### ‚úÖ 3. tool_failures.yaml Growing Unbounded - IMPLEMENTED

**Status:** ‚úÖ Completed (2026-01-09)

**Implementation:** Added rolling stats window with time-based breakdowns in `server/auto_heal_decorator.py`.

**What was added:**
- Capped global stats at 1000 (was unbounded)
- Added daily stats (keeps last 30 days only)
- Added weekly stats (keeps last 12 weeks only)
- Auto-cleanup of old daily/weekly stats

**New stats structure:**
```yaml
stats:
  total_failures: 1000  # Capped at 1000 (was unbounded)
  auto_fixed: 850       # Capped at 1000 (was unbounded)
  daily:
    "2026-01-09": {total: 15, auto_fixed: 12}
    "2026-01-08": {total: 18, auto_fixed: 14}
    # ... keeps last 30 days only
  weekly:
    "2026-W02": {total: 120, auto_fixed: 95}
    "2026-W01": {total: 115, auto_fixed: 90}
    # ... keeps last 12 weeks only
```

**Impact:** Prevents unbounded growth of stats. File size now stable instead of growing 36,500 entries/year.

---

## üü° High Priority (Plan for Next Sprint)

### ‚úÖ 4. Memory Query Interface - IMPLEMENTED

**Status:** ‚úÖ Completed (2026-01-09)

**Implementation:** Added `memory_query()` MCP tool to `tool_modules/aa_workflow/src/memory_tools.py`.

**What was added:**
- New MCP tool: `memory_query(key, query)`
- Uses JSONPath expressions (via jsonpath-ng library)
- Graceful degradation if jsonpath-ng not installed
- Comprehensive error handling with helpful messages
- 6 example queries in docstring

**Usage examples:**
```python
# Get first active issue
memory_query("state/current_work", "$.active_issues[0]")

# Filter issues by status
memory_query("state/current_work", "$.active_issues[?(@.status=='In Progress')]")

# Extract all issue keys
memory_query("state/current_work", "$.active_issues[*].key")

# Get nested environment status
memory_query("state/environments", "$.environments.stage.status")

# Pattern matching with regex
memory_query("learned/patterns", "$.error_patterns[?(@.pattern =~ /auth.*/i)]")
```

**Implementation details:**
```python
@registry.tool()
async def memory_query(key: str, query: str) -> list[TextContent]:
    """Query memory using JSONPath expressions."""
    try:
        from jsonpath_ng import parse
    except ImportError:
        return [TextContent(type="text",
            text="‚ùå jsonpath_ng not installed. Use memory_read() instead.")]

    # Load memory file
    memory_file = MEMORY_DIR / f"{key}.yaml"
    with open(memory_file) as f:
        data = yaml.safe_load(f) or {}

    # Execute JSONPath query
    expr = parse(query)
    matches = [match.value for match in expr.find(data)]

    # Return formatted results
    return [TextContent(type="text",
        text=f"## Query Results\n**Matches:** {len(matches)}\n\n{yaml.dump(matches)}")]
```

**Impact:** Reduces memory reads for large files, enables precise data extraction, improves query performance.

**Note:** Requires `pip install jsonpath-ng`. Tool gracefully degrades with helpful error message if not installed.

---

### 5. Memory Analytics Dashboard

**Problem:** No visibility into memory health/usage.

**Proposed Tool:**

```python
@registry.tool()
async def memory_stats() -> list[TextContent]:
    """
    Get memory system statistics.

    Returns:
        - File sizes and growth trends
        - Most-read/written files
        - Auto-heal success rates
        - Pattern match frequency
        - Memory fragmentation analysis
    """
    stats = {
        "files": {},
        "auto_heal": {},
        "patterns": {},
        "sessions": {},
    }

    # File sizes
    for file in MEMORY_DIR.rglob("*.yaml"):
        relative = file.relative_to(MEMORY_DIR)
        stats["files"][str(relative)] = {
            "size_kb": file.stat().st_size / 1024,
            "modified": datetime.fromtimestamp(file.stat().st_mtime).isoformat(),
        }

    # Auto-heal stats
    failures = read_memory("learned/tool_failures")
    stats["auto_heal"] = {
        "total": failures.get("stats", {}).get("total_failures", 0),
        "auto_fixed": failures.get("stats", {}).get("auto_fixed", 0),
        "success_rate": ...,
        "recent_failures": failures.get("failures", [])[-10:],
    }

    # Pattern usage
    patterns = read_memory("learned/patterns")
    stats["patterns"] = {
        "total": sum(len(patterns.get(cat, [])) for cat in [
            "auth_patterns", "error_patterns", "bonfire_patterns",
            "pipeline_patterns", "jira_cli_patterns"
        ]),
        "by_category": {...},
    }

    # Session activity
    today = datetime.now().strftime("%Y-%m-%d")
    session = read_memory(f"sessions/{today}")
    stats["sessions"] = {
        "today_actions": len(session.get("actions", [])),
        "total_session_files": len(list((MEMORY_DIR / "sessions").glob("*.yaml"))),
    }

    return [TextContent(type="text", text=yaml.dump(stats))]
```

**Skill:**
```yaml
# skills/memory_health.yaml
name: memory_health
description: Check memory system health

steps:
  - name: get_stats
    tool: memory_stats
    output: stats

  - name: check_large_files
    compute: |
      large = [f for f, info in stats["files"].items()
               if info["size_kb"] > 50]
      if large:
        result = f"‚ö†Ô∏è Large files: {large}"
      else:
        result = "‚úÖ All files normal size"
    output: size_check

  - name: check_success_rate
    compute: |
      rate = stats["auto_heal"]["success_rate"]
      if rate < 0.7:
        result = f"‚ö†Ô∏è Success rate low: {rate:.0%}"
      else:
        result = f"‚úÖ Success rate: {rate:.0%}"
    output: rate_check
```

---

### 6. Pattern Effectiveness Tracking

**Problem:** No way to know which patterns are useful.

**Current State:**
```yaml
# learned/patterns.yaml
auth_patterns:
  - pattern: "token expired"
    fix: "Refresh credentials"
    commands: ["kube_login(cluster='e')"]
    # ‚Üê No usage tracking!
```

**Proposed:**
```yaml
auth_patterns:
  - pattern: "token expired"
    fix: "Refresh credentials"
    commands: ["kube_login(cluster='e')"]
    # NEW:
    usage_stats:
      times_matched: 47
      times_fixed: 45
      success_rate: 0.96
      last_matched: "2026-01-09T14:23:15"
      avg_fix_time_ms: 1250
```

**Implementation:**

```python
# tool_modules/aa_workflow/src/skill_engine.py

def _check_error_patterns(self, error: str) -> str | None:
    patterns_file = SKILLS_DIR.parent / "memory" / "learned" / "patterns.yaml"

    # ... existing matching logic ...

    for pattern in error_patterns:
        if pattern["pattern"].lower() in error_lower:
            # NEW: Track usage
            if "usage_stats" not in pattern:
                pattern["usage_stats"] = {
                    "times_matched": 0,
                    "times_fixed": 0,
                    "success_rate": 0.0,
                }

            pattern["usage_stats"]["times_matched"] += 1
            pattern["usage_stats"]["last_matched"] = datetime.now().isoformat()

            # Write back (async to not block)
            asyncio.create_task(self._update_pattern_stats(patterns_file, pattern))

            return pattern.get("fix", "")
```

---

## üü¢ Medium Priority (Nice to Have)

### 7. Memory Compression/Archival

**Problem:** Session logs accumulate forever.

**Current:**
```
memory/sessions/
  2026-01-09.yaml  (3 KB)
  2026-01-08.yaml  (3 KB)
  2026-01-07.yaml  (3 KB)
  ...
  2025-06-15.yaml  (3 KB)  ‚Üê 200 days old, rarely accessed
```

**Proposed:**

```python
# skills/memory_cleanup.yaml - Add archival step

- name: archive_old_sessions
  compute: |
    from datetime import datetime, timedelta
    import gzip

    sessions_dir = Path.home() / "src/.../memory/sessions"
    archive_dir = sessions_dir / "archive"
    archive_dir.mkdir(exist_ok=True)

    cutoff = datetime.now() - timedelta(days=90)

    archived = 0
    for session_file in sessions_dir.glob("*.yaml"):
        date_str = session_file.stem  # YYYY-MM-DD
        try:
            file_date = datetime.strptime(date_str, "%Y-%m-%d")
            if file_date < cutoff:
                # Compress and move
                with open(session_file, 'rb') as f_in:
                    with gzip.open(archive_dir / f"{session_file.name}.gz", 'wb') as f_out:
                        f_out.writelines(f_in)
                session_file.unlink()
                archived += 1
        except ValueError:
            pass

    result = f"Archived {archived} old session logs"
  output: archive_result
```

---

### 8. Pattern Auto-Discovery

**Problem:** Patterns must be manually added via `learn_pattern` skill.

**Proposed:** Auto-detect common error patterns.

```python
# scripts/pattern_miner.py

def mine_patterns_from_failures():
    """Analyze tool_failures.yaml to discover new patterns."""
    failures = read_memory("learned/tool_failures")
    patterns = read_memory("learned/patterns")

    # Group failures by error text similarity
    from difflib import SequenceMatcher

    error_groups = []
    for failure in failures.get("failures", []):
        error = failure["error_snippet"]

        # Find similar errors
        matched = False
        for group in error_groups:
            similarity = SequenceMatcher(None, error, group["representative"]).ratio()
            if similarity > 0.8:
                group["count"] += 1
                group["errors"].append(error)
                matched = True
                break

        if not matched:
            error_groups.append({
                "representative": error,
                "count": 1,
                "errors": [error],
            })

    # Suggest patterns for frequent errors
    suggestions = []
    for group in error_groups:
        if group["count"] >= 5:  # Seen 5+ times
            # Check if already in patterns
            already_learned = any(
                group["representative"].lower() in str(patterns).lower()
            )

            if not already_learned:
                suggestions.append({
                    "pattern": group["representative"][:50],
                    "frequency": group["count"],
                    "examples": group["errors"][:3],
                })

    return suggestions
```

**Skill:**
```yaml
# skills/suggest_patterns.yaml
- name: mine_patterns
  compute: |
    from scripts.pattern_miner import mine_patterns_from_failures
    suggestions = mine_patterns_from_failures()
    result = suggestions
  output: pattern_suggestions

outputs:
  - name: summary
    value: |
      ## üîç Suggested Patterns

      {% for suggestion in pattern_suggestions %}
      ### Pattern: {{ suggestion.pattern }}
      **Frequency:** {{ suggestion.frequency }} occurrences

      **Examples:**
      {% for example in suggestion.examples %}
      - {{ example }}
      {% endfor %}

      **Action:** Run `learn_pattern` to add this
      {% endfor %}
```

---

### 9. Memory Schema Validation

**Problem:** No validation of memory file structure.

**Risk:**
```yaml
# current_work.yaml - Typo!
active_issuse:  # ‚Üê Should be "active_issues"
  - key: AAP-123
```

Skills silently fail to find data.

**Solution:**

```python
# scripts/common/memory_schemas.py

from pydantic import BaseModel, Field
from typing import List, Optional

class ActiveIssue(BaseModel):
    key: str
    summary: str
    status: str
    branch: str
    repo: str
    started: str

class CurrentWork(BaseModel):
    active_issue: Optional[str] = ""
    active_issues: List[ActiveIssue] = []
    open_mrs: List[dict] = []
    follow_ups: List[dict] = []
    last_updated: str

class ErrorPattern(BaseModel):
    pattern: str
    meaning: str
    fix: str
    commands: List[str] = []
    usage_stats: Optional[dict] = None

class Patterns(BaseModel):
    auth_patterns: List[ErrorPattern] = []
    error_patterns: List[ErrorPattern] = []
    bonfire_patterns: List[ErrorPattern] = []
    # ...

# In memory.py
def validate_memory(key: str, data: dict) -> bool:
    """Validate memory data against schema."""
    schemas = {
        "state/current_work": CurrentWork,
        "learned/patterns": Patterns,
    }

    schema = schemas.get(key)
    if schema:
        try:
            schema(**data)
            return True
        except Exception as e:
            logger.warning(f"Memory validation failed for {key}: {e}")
            return False
    return True

def write_memory(key: str, data: dict) -> bool:
    if not validate_memory(key, data):
        return False
    # ... existing write logic ...
```

---

### 10. Cross-Skill Memory Sharing

**Problem:** Skills can't easily share context.

**Example:**
```python
# Skill A discovers important info
skill_run("investigate_alert", '{"environment": "stage"}')
# ‚Üí Finds: "High CPU on pod tower-analytics-api-123"

# Skill B needs this context
skill_run("debug_prod", '{"namespace": "stage"}')
# ‚Üí Has to re-discover the same info
```

**Proposed:**

```yaml
# New memory file: state/shared_context.yaml
current_investigation:
  started_by: "investigate_alert"
  started_at: "2026-01-09T14:30:00"
  context:
    environment: "stage"
    namespace: "tower-analytics-prod"
    issue: "High CPU on pod tower-analytics-api-123"
    pod_name: "tower-analytics-api-123"
    related_alerts: ["HighCPU", "SlowRequests"]

expires_at: "2026-01-09T15:30:00"  # 1 hour TTL
```

**Usage:**

```yaml
# investigate_alert.yaml - Save context
- name: save_investigation_context
  tool: memory_write
  args:
    key: "state/shared_context"
    content: |
      current_investigation:
        started_by: "investigate_alert"
        context:
          environment: "{{ inputs.environment }}"
          pod_name: "{{ problematic_pod }}"
          issue: "{{ issue_summary }}"
        expires_at: "{{ expiry_time }}"

# debug_prod.yaml - Load context
- name: load_shared_context
  tool: memory_read
  args:
    key: "state/shared_context"
  output: shared_ctx
  on_error: continue

- name: use_context
  condition: "{{ shared_ctx and shared_ctx.current_investigation }}"
  compute: |
    # Auto-populate from previous investigation
    pod = shared_ctx["current_investigation"]["context"]["pod_name"]
    result = f"Continuing investigation of {pod}"
```

---

## üîµ Low Priority (Future Enhancements)

### 11. Memory Metrics Export

Export memory stats to Prometheus for monitoring.

```python
# server/memory_metrics.py
from prometheus_client import Counter, Gauge, Histogram

memory_reads = Counter('memory_reads_total', 'Total memory reads', ['file'])
memory_writes = Counter('memory_writes_total', 'Total memory writes', ['file'])
auto_heal_attempts = Counter('auto_heal_attempts_total', 'Auto-heal attempts', ['tool', 'type'])
auto_heal_success = Counter('auto_heal_success_total', 'Successful auto-heals', ['tool'])
pattern_matches = Counter('pattern_matches_total', 'Pattern matches', ['pattern'])
memory_file_size = Gauge('memory_file_size_bytes', 'Memory file size', ['file'])
```

---

### 12. Memory Replication

Replicate memory across multiple machines for teams.

```python
# Sync learned patterns across team
git add memory/learned/*.yaml
git commit -m "sync: Update learned patterns"
git push
```

---

### 13. Memory A/B Testing

Test different auto-heal strategies.

```yaml
# learned/patterns_experimental.yaml
auth_patterns:
  - pattern: "token expired"
    fix: "Try refresh token before full reauth"  # New strategy
    commands: ["refresh_token()", "kube_login()"]  # Fallback
```

---

## üìã Implementation Priority

| Priority | Item | Effort | Impact | Risk |
|----------|------|--------|--------|------|
| üî¥ **P0** | Race condition fix | Medium | High | High |
| üî¥ **P0** | Backup strategy | Low | High | High |
| üî¥ **P0** | Stats growth limit | Low | Medium | Medium |
| üü° **P1** | Memory query interface | Medium | High | Low |
| üü° **P1** | Analytics dashboard | Medium | Medium | Low |
| üü° **P1** | Pattern effectiveness | Medium | High | Low |
| üü¢ **P2** | Session archival | Low | Low | Low |
| üü¢ **P2** | Pattern auto-discovery | High | Medium | Low |
| üü¢ **P2** | Schema validation | Medium | Medium | Low |
| üü¢ **P2** | Cross-skill context | Medium | Medium | Low |
| üîµ **P3** | Metrics export | Medium | Low | Low |
| üîµ **P3** | Memory replication | High | Low | Medium |
| üîµ **P3** | A/B testing | High | Low | Low |

---

## üéØ Quick Wins (Do This Week)

1. **Add backup before memory_init** (30 min)
2. **Implement file locking** (2 hours)
3. **Add stats rotation** (1 hour)
4. **Create memory_stats tool** (2 hours)

**Total:** ~5.5 hours for major improvements.

---

## üìñ Related Documentation

- [Memory Complete Reference](./MEMORY-COMPLETE-REFERENCE.md)
- [Memory & Auto-Remediation](./memory-and-auto-remediation.md)
- [Memory Integration Deep Dive](./memory-integration-deep-dive.md)
